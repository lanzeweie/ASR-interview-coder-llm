import threading
import asyncio
import json
import os
import argparse
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Body
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse

# Conditional imports for optional features
try:
    from main import RealTimeASR_SV
    ASR_AVAILABLE = True
except ImportError:
    ASR_AVAILABLE = False
    print("Warning: ASR module not available. Use --no-asr to suppress this warning.")

from llm_client import LLMClient

from chat_manager import ChatManager

# Intelligent Agent imports
try:
    from intelligent_agent import agent_manager
    from trigger_manager import trigger_manager
    AGENT_AVAILABLE = True
except ImportError:
    AGENT_AVAILABLE = False
    print("Warning: Intelligent Agent module not available.")

# Parse command line arguments
parser = argparse.ArgumentParser(description='AST Real-time ASR and LLM Chat Server')
parser.add_argument('--no-asr', '--no', action='store_true', help='Skip ASR model initialization (skip audio and ASR)')
parser.add_argument('--host', type=str, default='0.0.0.0', help='Host to bind (default: 0.0.0.0)')
parser.add_argument('--port', type=int, default=8000, help='Port to bind (default: 8000)')
args = parser.parse_args()

app = FastAPI()

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# --- Config Management ---
CONFIG_FILE = "api_config.json"

def load_config():
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"configs": [], "current_config": ""}

def save_config(config):
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4)

# Initialize LLM Client
config_data = load_config()
current_config_name = config_data.get("current_config")
current_config = next((c for c in config_data.get("configs", []) if c["name"] == current_config_name), None)

if current_config:
    llm_client = LLMClient(
        api_key=current_config.get("api_key"),
        base_url=current_config.get("base_url"),
        model=current_config.get("model")
    )
else:
    # Initialize with empty values if no config found
    llm_client = LLMClient(api_key="", base_url="", model="")

# Initialize Chat Manager
chat_manager = ChatManager()

# --- Connection Manager for ASR ---
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                print(f"Error sending message: {e}")

manager = ConnectionManager()

# ASR Instance
asr_system = None

def asr_callback(message):
    """Callback function to be called by ASR system when a message is ready"""
    print(f"Callback received: {message}")
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
             asyncio.run_coroutine_threadsafe(manager.broadcast(message), loop)
        else:
             pass
    except RuntimeError:
        pass

# We need a reference to the main event loop to schedule tasks from the ASR thread
main_event_loop = None

# --- æ™ºèƒ½åˆ†æå›è°ƒå¤„ç† ---
async def agent_analysis_callback(result, messages, speaker_name):
    """æ™ºèƒ½åˆ†æå®Œæˆå›è°ƒ"""
    try:
        is_needed = result.get('is', False)

        if is_needed:
            print(f"[æ™ºèƒ½åˆ†æ] æ£€æµ‹åˆ°éœ€è¦å¤šæ¨¡å‹å»ºè®®ï¼Œä¸»äººå…¬: {speaker_name}")

            # è·å–å½“å‰èŠå¤© ID
            current_chat_id = chat_manager.get_current_chat_id()

            # å¦‚æœæ²¡æœ‰å½“å‰èŠå¤©ï¼Œåˆ›å»ºä¸€ä¸ª
            if not current_chat_id:
                new_chat = chat_manager.create_chat(f"æ™ºèƒ½åˆ†æ - {speaker_name}")
                current_chat_id = new_chat['id']

            # å‡†å¤‡æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘çš„ 10 æ¡æ¶ˆæ¯ï¼‰
            recent_messages = messages[-10:] if len(messages) > 10 else messages

            # æ·»åŠ ç³»ç»Ÿæç¤º
            formatted_messages = [
                {"role": "system", "content": f"ä½ æ˜¯AIåŠ©æ‰‹ï¼Œå¸®åŠ©{speaker_name}åˆ†æä»¥ä¸‹å¯¹è¯ã€‚{speaker_name}æ˜¯ä¸»äººå…¬ã€‚"}
            ]

            # æ·»åŠ å¯¹è¯å†å²
            for msg in recent_messages:
                role = 'user' if msg.get('speaker') else 'assistant'
                formatted_messages.append({
                    "role": role,
                    "content": msg.get('content', '')
                })

            # æ¨é€åˆ°æ‰€æœ‰ LLM WebSocket å®¢æˆ·ç«¯
            await llm_manager.broadcast({
                "type": "agent_triggered",
                "reason": result.get('reason', 'æ™ºèƒ½åˆ†æå»ºè®®'),
                "speaker": speaker_name,
                "messages": formatted_messages,
                "chat_id": current_chat_id,
                "is_multi_llm": True
            })

            print(f"[æ™ºèƒ½åˆ†æ] å¤šæ¨¡å‹å…±è¯å·²è§¦å‘")

    except Exception as e:
        print(f"[æ™ºèƒ½åˆ†æ] å›è°ƒå¤„ç†å¤±è´¥: {e}")

# --- LLM è¿æ¥ç®¡ç†å™¨ ---
class LLMConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                print(f"LLM å¹¿æ’­å¤±è´¥: {e}")

llm_manager = LLMConnectionManager()

# --- å¤šæ¨¡å‹è¯·æ±‚å¤„ç†å‡½æ•° ---
async def handle_multi_llm_request(websocket: WebSocket, messages: list, chat_id: str):
    """å¤„ç†å¤šæ¨¡å‹å…±è¯è¯·æ±‚"""
    config_data = load_config()
    active_names = config_data.get("multi_llm_active_names", [])
    configs = config_data.get("configs", [])

    active_configs = [c for c in configs if c["name"] in active_names]

    if not active_configs:
        await websocket.send_json({"type": "error", "content": "æœªé€‰æ‹©ä»»ä½•æ¨¡å‹åŠ å…¥é›†ç¾¤ (è¯·åœ¨è®¾ç½®ä¸­å‹¾é€‰)"})
        return

    # å‘é€è§¦å‘é€šçŸ¥
    await websocket.send_json({
        "type": "agent_notification",
        "content": f"ğŸ¤– æ™ºèƒ½åˆ†æå·²å¯åŠ¨ï¼Œå°†åŒæ—¶è°ƒç”¨ {len(active_configs)} ä¸ªæ¨¡å‹ä¸ºæ‚¨æä¾›å»ºè®®"
    })

    # Prepare tasks
    async def stream_one(conf):
        name = conf["name"]
        try:
            client = LLMClient(conf["api_key"], conf["base_url"], conf["model"])

            # Handle separate system prompt
            current_messages = [m.copy() for m in messages]
            config_prompt = conf.get("system_prompt", "")

            if config_prompt:
                # Replace or insert system prompt
                sys_idx = next((i for i, m in enumerate(current_messages) if m["role"] == "system"), -1)
                if sys_idx != -1:
                    current_messages[sys_idx]["content"] = config_prompt
                else:
                    current_messages.insert(0, {"role": "system", "content": config_prompt})

            full_resp = ""
            async for chunk in client.chat_stream(current_messages):
                await websocket.send_json({
                    "type": "chunk",
                    "model": name,
                    "content": chunk
                })
                full_resp += chunk

            await websocket.send_json({"type": "done_one", "model": name})
            return name, full_resp
        except Exception as e:
            err_msg = f"Error: {str(e)}"
            await websocket.send_json({"type": "error", "content": f"[{name}] {err_msg}"})
            return name, f"[Error] {err_msg}"

    # Run all concurrently
    tasks = [stream_one(c) for c in active_configs]
    results = await asyncio.gather(*tasks)

    await websocket.send_json({"type": "done_all"})

    # Save to history
    if chat_id:
        # Append all responses
        for name, text in results:
            messages.append({"role": "assistant", "content": f"**{name}**:\n{text}"})
        chat_manager.update_chat_messages(chat_id, messages)

@app.on_event("startup")
async def startup_event():
    global asr_system, main_event_loop
    main_event_loop = asyncio.get_running_loop()

    # Initialize ASR system only if not skipped
    if not args.no_asr and ASR_AVAILABLE:
        print("[åˆå§‹åŒ–] å¯åŠ¨ ASR ç³»ç»Ÿ...")
        asr_system_initialized = False

        def thread_safe_callback(message):
            # Send to WebSocket clients
            if main_event_loop and main_event_loop.is_running():
                asyncio.run_coroutine_threadsafe(manager.broadcast(message), main_event_loop)

            # Send to trigger manager
            if AGENT_AVAILABLE:
                try:
                    trigger_manager.add_message(message)
                except Exception as e:
                    print(f"[è§¦å‘æœºåˆ¶] å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")

        try:
            asr_system = RealTimeASR_SV(on_message_callback=thread_safe_callback)
            # Run ASR in a separate thread so it doesn't block FastAPI
            thread = threading.Thread(target=asr_system.run, daemon=True)
            thread.start()
            asr_system_initialized = True
            print("[æˆåŠŸ] ASR ç³»ç»Ÿå·²åœ¨åå°çº¿ç¨‹å¯åŠ¨")
        except Exception as e:
            print(f"[é”™è¯¯] ASR ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            print("[æç¤º] ä½¿ç”¨ --no-asr å‚æ•°è·³è¿‡ ASR åˆå§‹åŒ–")
    else:
        if args.no_asr:
            print("[é…ç½®] å·²è·³è¿‡ ASR ç³»ç»Ÿåˆå§‹åŒ– (--no-asr)")
        else:
            print("[é…ç½®] ASR ç³»ç»Ÿä¸å¯ç”¨")

    # Initialize Intelligent Agent
    if AGENT_AVAILABLE:
        try:
            # Load agent from config
            config_data = load_config()
            agent_config = config_data.get("agent_config", {})
            agent_model_name = agent_config.get("model_name")

            if agent_model_name:
                # Find agent model config
                model_config = next(
                    (c for c in config_data.get("configs", []) if c["name"] == agent_model_name),
                    None
                )

                if model_config:
                    # Add model_type to config
                    model_config['model_type'] = 'api'
                    agent_manager.load_agent(agent_config, model_config)
                    print(f"[æˆåŠŸ] æ™ºèƒ½ Agent å·²åŠ è½½: {agent_model_name}")
                else:
                    print(f"[è­¦å‘Š] æœªæ‰¾åˆ° Agent æ¨¡å‹é…ç½®: {agent_model_name}")
            else:
                print("[é…ç½®] æœªé…ç½®æ™ºèƒ½ Agent æ¨¡å‹")

            # æ³¨å†Œæ™ºèƒ½åˆ†æå›è°ƒ
            trigger_manager.add_callback(agent_analysis_callback)
            print("[æˆåŠŸ] æ™ºèƒ½åˆ†æå›è°ƒå·²æ³¨å†Œ")

        except Exception as e:
            print(f"[é”™è¯¯] æ™ºèƒ½ Agent åˆå§‹åŒ–å¤±è´¥: {e}")
    else:
        print("[é…ç½®] æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")

@app.get("/")
async def get():
    with open("static/index.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            # Keep the connection alive
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket)

# --- LLM Endpoints ---

@app.get("/api/config")
async def get_config():
    return load_config()

@app.post("/api/config")
async def update_config(data: dict = Body(...)):
    """
    Update configuration. 
    Expected data: { "configs": [...], "current_config": "Name" }
    """
    save_config(data)
    
    # Reload LLM Client if current config changed
    new_current_name = data.get("current_config")
    new_config = next((c for c in data.get("configs", []) if c["name"] == new_current_name), None)
    
    if new_config:
        llm_client.update_config(
            api_key=new_config.get("api_key"),
            base_url=new_config.get("base_url"),
            model=new_config.get("model")
        )
    
    return {"status": "success", "message": "Configuration updated"}

@app.post("/api/test_connection")
async def test_connection_endpoint(data: dict = Body(...)):
    """
    Test connection with provided config.
    """
    api_key = data.get("api_key")
    base_url = data.get("base_url")
    model = data.get("model")
    
    if not all([api_key, base_url, model]):
        return {"success": False, "message": "Missing required fields"}
        
    client = LLMClient(api_key=api_key, base_url=base_url, model=model)
    success, message = await client.test_connection()
    return {"success": success, "message": message}

# --- Chat Management Endpoints ---

@app.get("/api/chats")
async def get_chats():
    return {
        "current_chat_id": chat_manager.get_current_chat_id(),
        "chats": chat_manager.get_all_chats()
    }

@app.post("/api/chats")
async def create_chat(data: dict = Body(...)):
    title = data.get("title", "New Chat")
    new_chat = chat_manager.create_chat(title)
    return new_chat

@app.get("/api/chats/{chat_id}")
async def get_chat(chat_id: str):
    chat = chat_manager.get_chat(chat_id)
    if not chat:
        raise HTTPException(status_code=404, detail="Chat not found")
    return chat

@app.delete("/api/chats/{chat_id}")
async def delete_chat(chat_id: str):
    success = chat_manager.delete_chat(chat_id)
    if not success:
        raise HTTPException(status_code=404, detail="Chat not found")
    return {"status": "success"}

@app.post("/api/chats/{chat_id}/clear")
async def clear_chat(chat_id: str):
    success = chat_manager.clear_chat_messages(chat_id)
    if not success:
        raise HTTPException(status_code=404, detail="Chat not found")
    return {"status": "success"}

# --- Intelligent Agent Endpoints ---

@app.get("/api/agent/status")
async def get_agent_status():
    """è·å–æ™ºèƒ½ Agent çŠ¶æ€"""
    if not AGENT_AVAILABLE:
        return {"available": False, "message": "æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨"}

    config_data = load_config()
    agent_config = config_data.get("agent_config", {})

    return {
        "available": True,
        "enabled": agent_manager.enabled,
        "auto_trigger": agent_manager.auto_trigger,
        "status": trigger_manager.get_status(),
        "config": agent_config
    }

@app.post("/api/agent/enable")
async def enable_agent(data: dict = Body(...)):
    """å¯ç”¨/ç¦ç”¨æ™ºèƒ½ Agent"""
    if not AGENT_AVAILABLE:
        raise HTTPException(status_code=503, detail="æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")

    enabled = data.get("enabled", True)
    auto_trigger = data.get("auto_trigger", True)

    agent_manager.enabled = enabled
    agent_manager.auto_trigger = auto_trigger
    trigger_manager.set_enabled(enabled)

    # Update config
    config_data = load_config()
    agent_config = config_data.get("agent_config", {})
    agent_config["enabled"] = enabled
    agent_config["auto_trigger"] = auto_trigger
    config_data["agent_config"] = agent_config
    save_config(config_data)

    return {
        "status": "success",
        "enabled": enabled,
        "auto_trigger": auto_trigger
    }

@app.post("/api/agent/config")
async def update_agent_config(data: dict = Body(...)):
    """æ›´æ–°æ™ºèƒ½ Agent é…ç½®"""
    if not AGENT_AVAILABLE:
        raise HTTPException(status_code=503, detail="æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")

    min_characters = data.get("min_characters", 10)
    silence_threshold = data.get("silence_threshold", 2)
    model_name = data.get("model_name")

    # Update thresholds
    trigger_manager.set_thresholds(min_characters, silence_threshold)

    # Update config file
    config_data = load_config()
    agent_config = config_data.get("agent_config", {})
    agent_config.update({
        "min_characters": min_characters,
        "silence_threshold": silence_threshold,
        "model_name": model_name
    })
    config_data["agent_config"] = agent_config
    save_config(config_data)

    # Reload agent if model changed
    if model_name and AGENT_AVAILABLE:
        model_config = next(
            (c for c in config_data.get("configs", []) if c["name"] == model_name),
            None
        )
        if model_config:
            model_config['model_type'] = 'api'
            agent_manager.load_agent(agent_config, model_config)

    return {"status": "success", "config": agent_config}

@app.post("/api/agent/analyze")
async def manual_analyze(data: dict = Body(...)):
    """æ‰‹åŠ¨è§¦å‘æ™ºèƒ½åˆ†æ"""
    if not AGENT_AVAILABLE:
        raise HTTPException(status_code=503, detail="æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")

    messages = data.get("messages", [])
    speaker_name = data.get("speaker_name", "ç”¨æˆ·")

    result = await agent_manager.analyze_conversation(messages, speaker_name)
    return result

@app.post("/api/agent/trigger")
async def trigger_multi_llm(data: dict = Body(...)):
    """æ‰‹åŠ¨è§¦å‘å¤šæ¨¡å‹å…±è¯"""
    messages = data.get("messages", [])
    chat_id = data.get("chat_id")

    # This will be handled by the WebSocket endpoint
    return {
        "status": "triggered",
        "message": "å¤šæ¨¡å‹å…±è¯å·²è§¦å‘",
        "messages": messages,
        "chat_id": chat_id
    }

@app.websocket("/ws/llm")
async def llm_websocket(websocket: WebSocket):
    await llm_manager.connect(websocket)

    # Reload config on connection to ensure we have the latest
    current_data = load_config()
    curr_name = current_data.get("current_config")
    curr_conf = next((c for c in current_data.get("configs", []) if c["name"] == curr_name), None)

    if curr_conf:
        llm_client.update_config(
            api_key=curr_conf.get("api_key"),
            base_url=curr_conf.get("base_url"),
            model=curr_conf.get("model")
        )

    try:
        while True:
            data = await websocket.receive_json()

            # å¤„ç†æ™ºèƒ½åˆ†æè§¦å‘æ¶ˆæ¯
            if data.get("type") == "agent_triggered":
                print(f"[æ™ºèƒ½åˆ†æ] WebSocket æ”¶åˆ°è§¦å‘æ¶ˆæ¯")
                messages = data.get("messages", [])
                chat_id = data.get("chat_id")
                is_multi_llm = True

                # ç›´æ¥å¤„ç†å¤šæ¨¡å‹æ¨¡å¼
                await handle_multi_llm_request(websocket, messages, chat_id)
                continue

            # data format: { "messages": [...], "chat_id": "...", "is_multi_llm": bool }
            messages = data.get("messages", [])
            chat_id = data.get("chat_id")
            is_multi_llm = data.get("is_multi_llm", False)

            # Add system prompt if not present or just ensure it's there
            if not messages or messages[0].get("role") != "system":
                 messages.insert(0, {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAiåŠ©æ‰‹å¸®åŠ©ç”¨æˆ·ï¼Œå¹¶ä¸”åˆ†æèŠå¤©è®°å½•"})

            if is_multi_llm:
                # --- Multi-LLM Mode ---
                config_data = load_config()
                active_names = config_data.get("multi_llm_active_names", [])
                configs = config_data.get("configs", [])
                
                active_configs = [c for c in configs if c["name"] in active_names]
                
                if not active_configs:
                     await websocket.send_json({"type": "error", "content": "æœªé€‰æ‹©ä»»ä½•æ¨¡å‹åŠ å…¥é›†ç¾¤ (è¯·åœ¨è®¾ç½®ä¸­å‹¾é€‰)"})
                     continue
                
                # Prepare tasks
                async def stream_one(conf):
                    name = conf["name"]
                    try:
                        client = LLMClient(conf["api_key"], conf["base_url"], conf["model"])
                        
                        # Handle separate system prompt
                        current_messages = [m.copy() for m in messages] # Deep copyish
                        config_prompt = conf.get("system_prompt", "")
                        
                        if config_prompt:
                            # Replace or insert system prompt
                            sys_idx = next((i for i, m in enumerate(current_messages) if m["role"] == "system"), -1)
                            if sys_idx != -1:
                                current_messages[sys_idx]["content"] = config_prompt
                            else:
                                current_messages.insert(0, {"role": "system", "content": config_prompt})
                        
                        full_resp = ""
                        async for chunk in client.chat_stream(current_messages):
                            await websocket.send_json({
                                "type": "chunk",
                                "model": name,
                                "content": chunk
                            })
                            full_resp += chunk
                        
                        await websocket.send_json({"type": "done_one", "model": name})
                        return name, full_resp
                    except Exception as e:
                        err_msg = f"Error: {str(e)}"
                        await websocket.send_json({"type": "error", "content": f"[{name}] {err_msg}"})
                        return name, f"[Error] {err_msg}"

                # Run all concurrently
                tasks = [stream_one(c) for c in active_configs]
                results = await asyncio.gather(*tasks)
                
                await websocket.send_json({"type": "done_all"})
                
                # Save to history
                if chat_id:
                    # Append all responses
                    for name, text in results:
                        messages.append({"role": "assistant", "content": f"**{name}**:\n{text}"})
                    chat_manager.update_chat_messages(chat_id, messages)

            else:
                # --- Single LLM Mode (Original Logic) ---
                response_text = ""
                try:
                    # Check if client is ready
                    if not llm_client.client:
                         await websocket.send_json({"type": "error", "content": "LLM Client not initialized. Please check settings."})
                         continue
    
                    async for chunk in llm_client.chat_stream(messages):
                        await websocket.send_json({"type": "chunk", "content": chunk})
                        response_text += chunk
    
                    await websocket.send_json({"type": "done", "full_text": response_text})
    
                    # Save to chat history if chat_id is provided
                    if chat_id:
                        messages.append({"role": "assistant", "content": response_text})
                        chat_manager.update_chat_messages(chat_id, messages)
    
                except Exception as e:
                    print(f"LLM Stream Error: {e}")
                    await websocket.send_json({"type": "error", "content": f"Stream Error: {str(e)}"})

    except WebSocketDisconnect:
        print("LLM WebSocket disconnected")
        llm_manager.disconnect(websocket)
    except Exception as e:
        print(f"LLM WebSocket Fatal Error: {e}")
        traceback.print_exc()
        llm_manager.disconnect(websocket)

if __name__ == "__main__":
    import uvicorn

    # Print startup banner
    print("=" * 60)
    print("ğŸš€ AST å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬ä¸å¤§æ¨¡å‹åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    print(f"[é…ç½®] ASR ç³»ç»Ÿ: {'âœ… å¯ç”¨' if not args.no_asr else 'âŒ ç¦ç”¨ (--no-asr)'}")
    print(f"[é…ç½®] LLM å®¢æˆ·ç«¯: âœ… å¯ç”¨")
    print(f"[é…ç½®] æœåŠ¡åœ°å€: http://{args.host}:{args.port}")
    print("=" * 60)
    print("")

    uvicorn.run(app, host=args.host, port=args.port)
