import argparse
import asyncio
import base64
import json
import os
import re
import threading
import time
import wave

from fastapi import (Body, FastAPI, File, HTTPException, UploadFile, WebSocket,
                     WebSocketDisconnect)
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles

# Conditional imports for optional features
try:
    from main import RealTimeASR_SV
    ASR_AVAILABLE = True
except ImportError:
    ASR_AVAILABLE = False
    print("è­¦å‘Š: ASR æ¨¡å—ä¸å¯ç”¨ã€‚ä½¿ç”¨ --no-asr å–æ¶ˆæ­¤è­¦å‘Šã€‚")

from chat_manager import ChatManager
from llm_client import LLMClient
from chat_manager import ChatManager
from llm_client import LLMClient
from resume_manager import ResumeManager
from job_manager import JobManager

# Intelligent Agent imports
try:
    from intelligent_agent import agent_manager, format_intent_analysis
    from trigger_manager import trigger_manager
    AGENT_AVAILABLE = True
except ImportError:
    AGENT_AVAILABLE = False
    print("è­¦å‘Š: æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨ã€‚")

# Parse command line arguments
parser = argparse.ArgumentParser(description='AST Real-time ASR and LLM Chat Server')
parser.add_argument('--no', action='store_true', help='Skip ALL model initialization (disable ASR, voiceprint, and local agent models)')
parser.add_argument('--no-asr', '--no-voice', action='store_true', help='[DEPRECATED] Use --no instead. Skip ASR and voiceprint model initialization')
parser.add_argument('--host', type=str, default='0.0.0.0', help='Host to bind (default: 0.0.0.0)')
parser.add_argument('--port', type=int, default=8000, help='Port to bind (default: 8000)')
args = parser.parse_args()

# Handle deprecated argument
if args.no_asr:
    print("[âš ï¸  è­¦å‘Š] --no-asr å‚æ•°å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ --no æ›¿ä»£")
    args.no = True

app = FastAPI()

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# --- Config Management ---
CONFIG_FILE = "api_config.json"
AGENT_ROLE_FILE = "data/agent.json"
UI_STATE_FILE = "data/ui_state.json"

LEGACY_IDENTITY_MAP = {
    "æ€è€ƒ": "tech_assistant",
    "å¿«é€Ÿ": "concise_assistant",
    "å¼•å¯¼": "guide",
    "æŠ€æœ¯è¾…åŠ©è€…": "tech_assistant",
    "ç²¾ç®€è¾…åŠ©è€…": "concise_assistant",
    "èµ„æ·±æ±‚èŒç€": "guide"
}


def normalize_identity_identifier(value: str | None) -> str:
    if not value:
        return ""
    identifier = value.strip()
    if not identifier:
        return ""
    identifier = re.sub(r"\s+", "_", identifier)
    # ä¼˜å…ˆåŒ¹é…ä¸­æ–‡/åˆ«å
    mapped = LEGACY_IDENTITY_MAP.get(identifier)
    if mapped:
        return mapped

    identifier = identifier.lower()
    mapped = LEGACY_IDENTITY_MAP.get(identifier)
    if mapped:
        return mapped

    if identifier.endswith("_tag"):
        identifier = identifier[:-4]
    return identifier


def sanitize_role_definition(role: dict | None) -> dict | None:
    """Normalize role definitions to a single canonical ID field."""
    if not isinstance(role, dict):
        return None

    normalized_id = normalize_identity_identifier(role.get("id") or role.get("tag_key"))
    if not normalized_id:
        return None

    name = (role.get("name") or "").strip() or normalized_id
    prompt = (role.get("prompt") or "").strip()
    enabled = bool(role.get("enabled", True))

    return {
        "id": normalized_id,
        "name": name,
        "prompt": prompt,
        "enabled": enabled
    }


def build_identity_lookup(roles: list[dict]) -> dict[str, dict]:
    """Create a fast lookup table for identity definitions."""
    lookup: dict[str, dict] = {}
    for role in roles:
        normalized_id = normalize_identity_identifier(role.get("id"))
        if not normalized_id:
            continue
        copy = dict(role)
        copy["id"] = normalized_id
        lookup[normalized_id] = copy
    return lookup


def select_identity_role(tags: list[str], lookup: dict[str, dict]) -> tuple[str | None, dict | None, list[str]]:
    """
    Find the first enabled identity (with prompt) that matches the provided tags.
    Returns (active_tag, role_dict, disabled_role_names).
    """
    disabled_names: list[str] = []
    for tag in tags:
        normalized_tag = normalize_identity_identifier(tag)
        if not normalized_tag:
            continue
        role = lookup.get(normalized_tag)
        if not role:
            continue
        if not role.get("enabled", True):
            disabled_names.append(role.get("name") or normalized_tag)
            continue
        prompt_text = (role.get("prompt") or "").strip()
        if not prompt_text:
            continue
        role_copy = dict(role)
        role_copy["prompt"] = prompt_text
        return normalized_tag, role_copy, disabled_names
    return None, None, disabled_names


def load_think_tank_roles() -> list[dict]:
    if not os.path.exists(AGENT_ROLE_FILE):
        return []
    try:
        with open(AGENT_ROLE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as exc:
        print(f"[æ™ºå›Šå›¢] åŠ è½½èº«ä»½å¤±è´¥: {exc}")
        return []

    roles: list[dict] = []
    needs_resave = False
    for raw_role in data.get("think_tank_roles", []):
        sanitized = sanitize_role_definition(raw_role)
        if not sanitized:
            continue
        roles.append(sanitized)
        if (
            raw_role.get("tag_key") is not None
            or normalize_identity_identifier(raw_role.get("id")) != sanitized["id"]
            or (raw_role.get("name") or "").strip() != sanitized["name"]
            or (raw_role.get("prompt") or "").strip() != sanitized["prompt"]
        ):
            needs_resave = True

    if needs_resave:
        save_think_tank_roles(roles)

    return roles


def save_think_tank_roles(roles: list[dict]):
    os.makedirs(os.path.dirname(AGENT_ROLE_FILE), exist_ok=True)
    sanitized_roles = []
    for role in roles:
        sanitized = sanitize_role_definition(role)
        if sanitized:
            sanitized_roles.append(sanitized)
    payload = {"think_tank_roles": sanitized_roles}
    with open(AGENT_ROLE_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def normalize_config_tags(config: dict) -> dict:
    tags = config.get("tags") or []
    normalized_tags = []
    for tag in tags:
        normalized = normalize_identity_identifier(tag)
        if normalized:
            normalized_tags.append(normalized)
    config["tags"] = normalized_tags
    return config

def load_config():
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            configs = data.get("configs", [])
            data["configs"] = [normalize_config_tags(dict(config)) for config in configs]
            return data
    return {"configs": [], "current_config": ""}

def save_config(config):
    configs = config.get("configs", [])
    config["configs"] = [normalize_config_tags(dict(conf)) for conf in configs]
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4, ensure_ascii=False)

def load_ui_state():
    if os.path.exists(UI_STATE_FILE):
        try:
            with open(UI_STATE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading UI state: {e}")
    return {}

def save_ui_state(state):
    os.makedirs(os.path.dirname(UI_STATE_FILE), exist_ok=True)
    try:
        with open(UI_STATE_FILE, "w", encoding="utf-8") as f:
            json.dump(state, f, indent=2)
    except Exception as e:
        print(f"Error saving UI state: {e}")

# Initialize LLM Client
config_data = load_config()
current_config_name = config_data.get("current_config")
current_config = next((c for c in config_data.get("configs", []) if c["name"] == current_config_name), None)

if current_config:
    llm_client = LLMClient(
        api_key=current_config.get("api_key"),
        base_url=current_config.get("base_url"),
        model=current_config.get("model")
    )
else:
    # Initialize with empty values if no config found
    llm_client = LLMClient(api_key="", base_url="", model="")

# Initialize Chat Manager
chat_manager = ChatManager()

# Initialize Resume Manager
resume_manager = ResumeManager(llm_client=llm_client)
# Initialize Resume Manager
resume_manager = ResumeManager(llm_client=llm_client)
resume_personalization_enabled = False

# Initialize Job Manager
job_manager = JobManager(llm_client=llm_client)
CACHED_JOB_CONTEXT = None

def update_job_context_cache():
    """Update the global cached job context."""
    global CACHED_JOB_CONTEXT
    content = None
    if os.path.exists(job_manager.job_analysis_path):
        try:
            with open(job_manager.job_analysis_path, "r", encoding="utf-8") as f:
                content = f.read()
        except:
            pass
    CACHED_JOB_CONTEXT = content
    print(f"[JobManager] Context cache updated. Size: {len(content) if content else 0}")

# Load initial job context
update_job_context_cache()

# Load initial resume config
_initial_config = load_config()
if "resume_config" in _initial_config:
    resume_manager.update_config(_initial_config["resume_config"])

# --- Connection Manager for ASR ---
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                print(f"Error sending message: {e}")

manager = ConnectionManager()

# ASR Instance
asr_system = None

def asr_callback(message):
    """Callback function to be called by ASR system when a message is ready"""
    print(f"Callback received: {message}")
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
             asyncio.run_coroutine_threadsafe(manager.broadcast(message), loop)
        else:
             pass
    except RuntimeError:
        pass

# We need a reference to the main event loop to schedule tasks from the ASR thread
main_event_loop = None

# --- æ™ºèƒ½åˆ†æå›è°ƒå¤„ç† ---
async def agent_analysis_callback(result, messages, speaker_name):
    """æ™ºèƒ½åˆ†æå®Œæˆå›è°ƒ"""
    try:
        phase1_result = result.get('phase1', {})
        is_needed = phase1_result.get('is', False)
        analysis_id = result.get('analysis_id')
        reason = phase1_result.get('reason', '')
        summary = result.get('analysis_summary')
        count = result.get('analysis_count')
        preview = result.get('analysis_preview')
        model_name = phase1_result.get('model_name')

        summary_label = summary or f"[{speaker_name}]"
        status_text = f"{summary_label} Â· åˆ†æå®Œæˆ"
        if is_needed:
            status_text = f"{summary_label} Â· åŠ©æ‰‹ä»‹å…¥"
        elif reason:
            status_text = f"{summary_label} Â· {reason}"

        # Prepare intent info for broadcast
        phase2_result = result.get('phase2')
        intent_data = None
        if phase2_result and phase2_result.get('success'):
            raw_xml = phase2_result.get('summary_xml', '')
            import re
            summary_match = re.search(r'<summary>(.*?)</summary>', raw_xml, re.DOTALL)
            summary_text = summary_match.group(1).strip() if summary_match else raw_xml
            
            intent_data = {
                'model': phase2_result.get('model_name', 'Unknown'),
                'summary': summary_text
            }

        # åˆ†æå®Œæˆï¼Œå‘é€ç»“æŸæ¶ˆæ¯åˆ°ASRé¢æ¿
        await manager.broadcast({
            "time": time.strftime("%H:%M:%S"),
            "speaker": "æ™ºèƒ½åˆ†æ",
            "text": status_text,
            "analysis_status": "completed",
            "analysis_need_ai": is_needed,
            "analysis_id": analysis_id,
            "analysis_reason": reason,
            "analysis_summary": summary,
            "analysis_count": count,
            "analysis_preview": preview,
            "analysis_model": model_name,
            "intent_info": intent_data
        })

        if is_needed:
            print(f"[æ™ºèƒ½åˆ†æ] âœ… æ£€æµ‹åˆ°éœ€è¦AIå¸®åŠ©åˆ†æï¼Œä¸»äººå…¬: {speaker_name}")

            try:
                # è·å–å½“å‰èŠå¤© ID
                current_chat_id = chat_manager.get_current_chat_id()
                print(f"[æ™ºèƒ½åˆ†æ] å½“å‰èŠå¤©ID: {current_chat_id}")

                # å¦‚æœæ²¡æœ‰å½“å‰èŠå¤©ï¼Œåˆ›å»ºä¸€ä¸ª
                if not current_chat_id:
                    new_chat = chat_manager.create_chat(f"æ™ºèƒ½åˆ†æ - {speaker_name}")
                    current_chat_id = new_chat['id']
                    print(f"[æ™ºèƒ½åˆ†æ] âœ… åˆ›å»ºæ–°èŠå¤©: {current_chat_id}")

                # å‡†å¤‡æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘çš„ 10 æ¡æ¶ˆæ¯ï¼‰
                recent_messages = messages[-10:] if len(messages) > 10 else messages
                print(f"[æ™ºèƒ½åˆ†æ] å‡†å¤‡å‘é€ {len(recent_messages)} æ¡æ¶ˆæ¯ç»™AI")

                # è·å–åˆ†å‘é…ç½®
                distribution_result = result.get('distribution', {})
                distribution_mode = distribution_result.get('mode', 'single')
                targets = distribution_result.get('targets', [])
                intent_result = distribution_result.get('intent')

                # æ„é€ å‘é€ç»™ä¸‹ä¸€é˜¶æ®µAIçš„æ¶ˆæ¯
                system_prompt = f"ä½ æ˜¯AIåŠ©æ‰‹ï¼Œå¸®åŠ©{speaker_name}æä¾›æŠ€æœ¯æ”¯æŒã€‚"
                if intent_result and intent_result.get("summary_xml"):
                    intent_summary_xml = format_intent_analysis(intent_result)
                    
                    # æå–æ‘˜è¦ç”¨äºæ˜¾ç¤ºï¼Œé¿å…åœ¨UIæ˜¾ç¤ºåŸå§‹XML
                    import re
                    match = re.search(r'<summary>(.*?)</summary>', intent_summary_xml, re.DOTALL)
                    summary_text = match.group(1).strip() if match else intent_summary_xml
                    
                    # æ„é€ äººç±»å¯è¯»çš„æç¤º
                    display_content = f"ã€æ„å›¾è¯†åˆ«åˆ†æã€‘\n{summary_text}"
                    
                    formatted_messages = [
                        {"role": "system", "content": system_prompt + " è¯·æ ¹æ®æ„å›¾è¯†åˆ«åˆ†æç»“æœç›´æ¥ç»™å‡ºå»ºè®®ã€‚"},
                        {"role": "user", "content": display_content}
                    ]
                    print("[æ™ºèƒ½åˆ†æ] ä½¿ç”¨æ„å›¾è¯†åˆ«ç»“æœä½œä¸ºå”¯ä¸€ä¸Šä¸‹æ–‡å‘é€ç»™ä¸‹ä¸€é˜¶æ®µAI")
                else:
                    formatted_messages = [
                        {"role": "system", "content": f"ä½ æ˜¯AIåŠ©æ‰‹ï¼Œå¸®åŠ©{speaker_name}åˆ†æä»¥ä¸‹å¯¹è¯ã€‚{speaker_name}æ˜¯ä¸»äººå…¬ã€‚"}
                    ]
                    for msg in recent_messages:
                        role = 'user' if msg.get('speaker') else 'assistant'
                        content = msg.get('content', '')
                        formatted_messages.append({
                            "role": role,
                            "content": content
                        })
                    print(f"[æ™ºèƒ½åˆ†æ] ä½¿ç”¨å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡å‘é€ï¼Œå…± {len(formatted_messages)} æ¡æ¶ˆæ¯")

                print(f"[æ™ºèƒ½åˆ†æ] æ¶ˆæ¯å†…å®¹é¢„è§ˆ:")
                for i, msg in enumerate(formatted_messages):
                    preview = msg['content'][:50]
                    suffix = '...' if len(msg['content']) > 50 else ''
                    print(f"  [{i}] {msg['role']}: {preview}{suffix}")

                # æ ¹æ®åˆ†å‘æ¨¡å¼å†³å®šå¤„ç†æ–¹å¼
                is_multi_llm = (distribution_mode == 'think_tank')

                # æ‰“å°å‘é€å‰çš„è°ƒè¯•ä¿¡æ¯
                print(f"[æ™ºèƒ½åˆ†æ] ğŸ“¤ å‡†å¤‡å‘é€æ¶ˆæ¯åˆ°AI:")
                print(f"  - åˆ†å‘æ¨¡å¼: {'æ™ºå›Šå›¢' if is_multi_llm else 'å•æ¨¡å‹'}")
                print(f"  - èŠå¤©ID: {current_chat_id}")
                print(f"  - æ¶ˆæ¯æ•°é‡: {len(formatted_messages)}")

                # å¦‚æœæœ‰æ™ºå›Šå›¢ç›®æ ‡ï¼Œä½¿ç”¨æ™ºå›Šå›¢æ¨¡å¼
                if distribution_mode == 'halt':
                    print(f"[æ™ºèƒ½åˆ†æ] ğŸ›‘ åˆ†ææµç¨‹å·²ç»ˆæ­¢ (åŸå› : {distribution_result.get('reason', 'Unknown')})")
                elif is_multi_llm and targets:
                    broadcast_message = {
                        "type": "agent_triggered",
                        "reason": phase1_result.get('reason', 'æ£€æµ‹åˆ°éœ€è¦AIå¸®åŠ©åˆ†æï¼Œå·²å¯åŠ¨æ™ºå›Šå›¢'),
                        "speaker": speaker_name,
                        "messages": formatted_messages,
                        "chat_id": current_chat_id,
                        "is_multi_llm": True,
                        "intent_recognition": intent_result is not None,
                        "intent_data": intent_result
                    }
                    print(f"[æ™ºèƒ½åˆ†æ] ğŸ“¡ å‘é€æ™ºå›Šå›¢è§¦å‘æ¶ˆæ¯...")
                    await llm_manager.broadcast(broadcast_message)
                    print(f"[æ™ºèƒ½åˆ†æ] âœ… ğŸ¤– æ™ºå›Šå›¢å·²è§¦å‘ï¼Œåˆ†å‘åˆ°{len(targets)}ä¸ªç›®æ ‡")
                else:
                    # ä½¿ç”¨å•æ¨¡å‹æ¨¡å¼
                    broadcast_message = {
                        "type": "agent_triggered",
                        "reason": phase1_result.get('reason', 'æ£€æµ‹åˆ°éœ€è¦AIå¸®åŠ©åˆ†æ'),
                        "speaker": speaker_name,
                        "messages": formatted_messages,
                        "chat_id": current_chat_id,
                        "is_multi_llm": False,
                        "intent_recognition": intent_result is not None,
                        "intent_data": intent_result
                    }
                    print(f"[æ™ºèƒ½åˆ†æ] ğŸ“¡ å‘é€å•æ¨¡å‹è§¦å‘æ¶ˆæ¯...")
                    await llm_manager.broadcast(broadcast_message)
                    print(f"[æ™ºèƒ½åˆ†æ] âœ… ğŸ¤– å•æ¨¡å‹æ¨¡å¼å·²è§¦å‘ï¼Œç­‰å¾…AIå›å¤...")
            except Exception as broadcast_error:
                print(f"[æ™ºèƒ½åˆ†æ] âŒ å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {broadcast_error}")
                import traceback
                traceback.print_exc()
        else:
            print(f"[æ™ºèƒ½åˆ†æ] âŒ æ£€æµ‹åˆ°æ— éœ€AIå¸®åŠ©ï¼Œä¸å‘é€æ¶ˆæ¯")

    except Exception as e:
        print(f"[æ™ºèƒ½åˆ†æ] âŒ å›è°ƒå¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

# --- LLM è¿æ¥ç®¡ç†å™¨ ---
class LLMConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        print(f"[LLMè¿æ¥] æ–°è¿æ¥åŠ å…¥ï¼Œå½“å‰æ´»è·ƒè¿æ¥æ•°: {len(self.active_connections)}")

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
            print(f"[LLMè¿æ¥] è¿æ¥æ–­å¼€ï¼Œå½“å‰æ´»è·ƒè¿æ¥æ•°: {len(self.active_connections)}")
        else:
            print(f"[LLMè¿æ¥] å°è¯•æ–­å¼€ä¸å­˜åœ¨çš„è¿æ¥")

    async def broadcast(self, message: dict):
        print(f"[LLMå¹¿æ’­] å¼€å§‹å¹¿æ’­åˆ° {len(self.active_connections)} ä¸ªè¿æ¥")
        print(f"[LLMå¹¿æ’­] æ¶ˆæ¯ç±»å‹: {message.get('type', 'unknown')}")
        print(f"[LLMå¹¿æ’­] æ¶ˆæ¯å†…å®¹: {str(message)[:100]}{'...' if len(str(message)) > 100 else ''}")

        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
                print(f"[LLMå¹¿æ’­] âœ… æˆåŠŸå‘é€åˆ°è¿æ¥")
            except Exception as e:
                print(f"[LLMå¹¿æ’­] âŒ å¹¿æ’­å¤±è´¥: {e}")
                disconnected.append(connection)

        # ç§»é™¤æ–­å¼€çš„è¿æ¥
        for conn in disconnected:
            if conn in self.active_connections:
                self.active_connections.remove(conn)

        print(f"[LLMå¹¿æ’­] å¹¿æ’­å®Œæˆï¼Œå‰©ä½™ {len(self.active_connections)} ä¸ªæ´»è·ƒè¿æ¥")

llm_manager = LLMConnectionManager()


# --- æ™ºå›Šå›¢è¯·æ±‚å¤„ç†å‡½æ•° ---
async def handle_multi_llm_request(websocket: WebSocket, messages: list, chat_id: str):
    """å¤„ç†æ™ºå›Šå›¢è¯·æ±‚"""
    config_data = load_config()
    active_names = config_data.get("multi_llm_active_names", [])
    configs = config_data.get("configs", [])
    cached_roles = load_think_tank_roles()
    role_lookup = build_identity_lookup(cached_roles)

    active_configs = [c for c in configs if c["name"] in active_names]

    if not active_configs:
        await websocket.send_json({"type": "error", "content": "æœªé€‰æ‹©ä»»ä½•æ¨¡å‹åŠ å…¥é›†ç¾¤ (è¯·åœ¨è®¾ç½®ä¸­å‹¾é€‰)"})
        return

    # Check if job analysis exists locally
    if not os.path.exists(job_manager.job_analysis_path):
        await websocket.send_json({"type": "error", "content": "è¯·å…ˆè®¾ç½®ç›®æ ‡å²—ä½ï¼Œå®Œæˆå²—ä½åˆ†æã€‚åŠ©æ‰‹å¯¹è¯æ¡†å³ä¸Šè§’â†’è®¾ç½®ç›®æ ‡å²—ä½"})
        return

    # Prepare tasks
    async def stream_one(conf):
        name = conf["name"]
        try:
            client = LLMClient(conf["api_key"], conf["base_url"], conf["model"])

            # Handle separate system prompt
            current_messages = [m.copy() for m in messages]
            config_prompt = conf.get("system_prompt", "").strip()

            # Check identity tags and resolve active role
            raw_tags = conf.get("tags", [])
            normalized_tags = [normalize_identity_identifier(tag) for tag in raw_tags if tag]
            active_tag, active_role, disabled_candidates = select_identity_role(normalized_tags, role_lookup)

            identity_applied = False
            if active_role:
                tag_prompt = active_role["prompt"]
                sys_idx = next((i for i, m in enumerate(current_messages) if m["role"] == "system"), -1)
                if sys_idx != -1:
                    current_messages[sys_idx]["content"] = tag_prompt
                else:
                    current_messages.insert(0, {"role": "system", "content": tag_prompt})
                identity_applied = True
                print(f"[æ™ºå›Šå›¢] åº”ç”¨èº«ä»½æ ‡ç­¾ Prompt: {active_role['name']} â†’ æ¨¡å‹ {name}")
            elif config_prompt:
                sys_idx = next((i for i, m in enumerate(current_messages) if m["role"] == "system"), -1)
                if sys_idx != -1:
                    current_messages[sys_idx]["content"] = config_prompt
                else:
                    current_messages.insert(0, {"role": "system", "content": config_prompt})
            elif normalized_tags:
                if disabled_candidates:
                    print(f"[æ™ºå›Šå›¢] èº«ä»½å·²åœç”¨ï¼Œè·³è¿‡ Prompt: {', '.join(disabled_candidates)}")
                else:
                    print(f"[æ™ºå›Šå›¢] æœªæ‰¾åˆ°å¯ç”¨èº«ä»½ Prompt: {normalized_tags}")

            # Inject Job Analysis Context
            inject_job_analysis_to_messages(current_messages)

            # [è°ƒè¯•] æ˜¾ç¤ºå®é™…å‘é€ç»™æ¨¡å‹çš„å®Œæ•´ prompt
            print(f"\n{'='*80}")
            print(f"[è°ƒè¯•] [æ™ºå›Šå›¢] æ­£åœ¨å‘é€è¯·æ±‚åˆ°æ¨¡å‹: {conf.get('model', 'Unknown')} (Stream=True)")
            print(f"{'='*80}")
            print(f"[è°ƒè¯•] [æ™ºå›Šå›¢] æ¨¡å‹åç§°: {name}")
            print(f"[è°ƒè¯•] [æ™ºå›Šå›¢] ä½¿ç”¨ System Prompt: {config_prompt if (config_prompt and not identity_applied) else 'å¦'}")
            if normalized_tags:
                if identity_applied and active_role:
                    print(f"[è°ƒè¯•] [æ™ºå›Šå›¢] èº«ä»½æ ‡ç­¾: {normalized_tags} â†’ æ¿€æ´»: {active_role['name']} ({active_tag})")
                elif disabled_candidates:
                    print(f"[è°ƒè¯•] [æ™ºå›Šå›¢] èº«ä»½æ ‡ç­¾: {normalized_tags} (åœç”¨: {', '.join(disabled_candidates)})")
                else:
                    print(f"[è°ƒè¯•] [æ™ºå›Šå›¢] èº«ä»½æ ‡ç­¾: {normalized_tags} (æœªæ‰¾åˆ°å¯ç”¨èº«ä»½)")
            print(f"[è°ƒè¯•] [æ™ºå›Šå›¢] æ¶ˆæ¯æ€»æ•°: {len(current_messages)}")
            print(f"{'-'*80}")
            print("[è°ƒè¯•] [æ™ºå›Šå›¢] å®Œæ•´ Prompt å†…å®¹:")
            print(f"{'-'*80}")
            for i, msg in enumerate(current_messages):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                print(f"\n[æ¶ˆæ¯ {i+1}] è§’è‰²: {role}")
                print(f"[æ¶ˆæ¯ {i+1}] å†…å®¹: {content[:200]}{'...' if len(content) > 200 else ''}")
            print(f"\n{'='*80}\n")

            full_resp = ""
            async for chunk in client.chat_stream(current_messages):
                await websocket.send_json({
                    "type": "chunk",
                    "model": name,
                    "content": chunk
                })
                full_resp += chunk

            await websocket.send_json({"type": "done_one", "model": name})
            return name, full_resp
        except Exception as e:
            err_msg = f"Error: {str(e)}"
            await websocket.send_json({"type": "error", "content": f"[{name}] {err_msg}"})
            return name, f"[Error] {err_msg}"

    # Run all concurrently
    tasks = [stream_one(c) for c in active_configs]
    results = await asyncio.gather(*tasks)

    await websocket.send_json({"type": "done_all"})

    # Save to history
    if chat_id:
        # Append all responses
        for name, text in results:
            messages.append({"role": "assistant", "content": f"**{name}**:\n{text}"})
        chat_manager.update_chat_messages(chat_id, messages)

@app.on_event("startup")
async def startup_event():
    global asr_system, main_event_loop
    main_event_loop = asyncio.get_running_loop()

    # Initialize ASR system only if not skipped
    if not args.no and ASR_AVAILABLE:
        print("[åˆå§‹åŒ–] å¯åŠ¨ ASR ç³»ç»Ÿ...")
        asr_system_initialized = False

        def thread_safe_callback(message):
            # Send to WebSocket clients
            if main_event_loop and main_event_loop.is_running():
                asyncio.run_coroutine_threadsafe(manager.broadcast(message), main_event_loop)

            # Send to trigger manager
            if AGENT_AVAILABLE:
                try:
                    trigger_manager.add_message(message)
                except Exception as e:
                    print(f"[è§¦å‘æœºåˆ¶] å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")

        try:
            asr_system = RealTimeASR_SV(on_message_callback=thread_safe_callback)
            # Run ASR in a separate thread so it doesn't block FastAPI
            thread = threading.Thread(target=asr_system.run, daemon=True)
            thread.start()
            asr_system_initialized = True
            print("[æˆåŠŸ] ASR ç³»ç»Ÿå·²åœ¨åå°çº¿ç¨‹å¯åŠ¨")
        except Exception as e:
            print(f"[é”™è¯¯] ASR ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            print("[æç¤º] ä½¿ç”¨ --no å‚æ•°è·³è¿‡æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–")
    else:
        if args.no:
            print("[é…ç½®] å·²è·³è¿‡æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ– (--no)")
        else:
            print("[é…ç½®] ASR ç³»ç»Ÿä¸å¯ç”¨")

    # Initialize Intelligent Agent
    if AGENT_AVAILABLE and not args.no:
        try:
            # Load agent from config
            config_data = load_config()
            agent_config = config_data.get("agent_config", {})
            agent_model_name = agent_config.get("model_name")

            if agent_model_name:
                # æ£€æŸ¥æ˜¯å¦æ˜¾å¼æŒ‡å®šäº†æ¨¡å‹ç±»å‹
                model_type = agent_config.get('model_type', None)

                if model_type == 'local':
                    # æ˜¾å¼æŒ‡å®šä¸ºæœ¬åœ°æ¨¡å‹
                    print(f"[é…ç½®] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {agent_model_name}")
                    model_config = {
                        'model_type': 'local',
                        'model': agent_model_name
                    }
                    agent_manager.load_agent(agent_config, model_config)
                    print(f"[æˆåŠŸ] æ™ºèƒ½ Agent å·²åŠ è½½ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰: {agent_model_name}")
                else:
                    # æœªæ˜¾å¼æŒ‡å®šæˆ–æŒ‡å®šä¸ºAPIï¼Œå…ˆå°è¯•ä»configsä¸­æŸ¥æ‰¾
                    model_config = next(
                        (c for c in config_data.get("configs", []) if c["name"] == agent_model_name),
                        None
                    )

                    if model_config:
                        # åœ¨APIé…ç½®ä¸­æ‰¾åˆ°äº†ï¼Œä½¿ç”¨APIæ¨¡å¼
                        model_config['model_type'] = 'api'
                        agent_manager.load_agent(agent_config, model_config)
                        print(f"[æˆåŠŸ] æ™ºèƒ½ Agent å·²åŠ è½½ï¼ˆAPIæ¨¡å‹ï¼‰: {agent_model_name}")
                    else:
                        # APIé…ç½®ä¸­æ²¡æ‰¾åˆ°ï¼Œä½œä¸ºæœ¬åœ°æ¨¡å‹å¤„ç†
                        print(f"[é…ç½®] åœ¨APIé…ç½®ä¸­æœªæ‰¾åˆ° '{agent_model_name}'ï¼Œä½œä¸ºæœ¬åœ°æ¨¡å‹åŠ è½½")
                        model_config = {
                            'model_type': 'local',
                            'model': agent_model_name
                        }
                        agent_manager.load_agent(agent_config, model_config)
                        print(f"[æˆåŠŸ] æ™ºèƒ½ Agent å·²åŠ è½½ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰: {agent_model_name}")
            else:
                print("[é…ç½®] æœªé…ç½®æ™ºèƒ½ Agent æ¨¡å‹")

            # æ³¨å†Œæ™ºèƒ½åˆ†æå›è°ƒ
            trigger_manager.add_callback(agent_analysis_callback)
            print("[æˆåŠŸ] æ™ºèƒ½åˆ†æå›è°ƒå·²æ³¨å†Œ")

            # è®¾ç½®trigger managerçš„event loopå¼•ç”¨
            trigger_manager.set_event_loop(main_event_loop)
            print("[æˆåŠŸ] Trigger Manager event loopå·²è®¾ç½®")

            # è®¾ç½®å¹¿æ’­å›è°ƒï¼Œç”¨äºå‘é€WebSocketæ¶ˆæ¯
            async def broadcast_to_asr(message):
                """å‘ASRé¢æ¿å¹¿æ’­æ¶ˆæ¯"""
                await manager.broadcast(message)
            trigger_manager.set_broadcast_callback(broadcast_to_asr)
            print("[æˆåŠŸ] æ™ºèƒ½åˆ†æå¹¿æ’­å›è°ƒå·²è®¾ç½®")

            # åŠ è½½è§¦å‘é˜ˆå€¼å’Œæ¶ˆæ¯ä¸Šé™
            min_characters = agent_config.get("min_characters", 10)
            silence_threshold = agent_config.get("silence_threshold", 2)
            max_messages = agent_config.get("max_messages", 50)
            trigger_manager.set_thresholds(min_characters, silence_threshold)
            trigger_manager.set_max_history(max_messages)
            print(f"[æˆåŠŸ] è§¦å‘å‚æ•°å·²åŠ è½½: {min_characters}å­—, {silence_threshold}ç§’, {max_messages}æ¡æ¶ˆæ¯")

            # åŠ è½½ä¸»äººå…¬é…ç½®
            protagonist = config_data.get("protagonist", "")
            if protagonist:
                trigger_manager.set_protagonist(protagonist)
                print(f"[æˆåŠŸ] ä¸»äººå…¬å·²åŠ è½½: {protagonist}")

        except Exception as e:
            print(f"[é”™è¯¯] æ™ºèƒ½ Agent åˆå§‹åŒ–å¤±è´¥: {e}")
    else:
        if args.no:
            print("[é…ç½®] å·²è·³è¿‡æ™ºèƒ½åˆ†æåˆå§‹åŒ– (--no)")
        elif not AGENT_AVAILABLE:
            print("[é…ç½®] æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")

@app.get("/")
async def get():
    with open("static/index.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)

    # ç«‹å³å‘é€ ASR ç³»ç»ŸçŠ¶æ€ç»™å‰ç«¯
    await websocket.send_json({
        "time": "00:00:00",
        "speaker": "ç³»ç»Ÿ",
        "text": "ASR ç³»ç»Ÿæœªåˆå§‹åŒ–" if not asr_system else "ASR ç³»ç»Ÿå·²å°±ç»ª",
        "asr_status": {
            "initialized": asr_system is not None,
            "message": "è¯·ä½¿ç”¨æ­£å¸¸æ¨¡å¼å¯åŠ¨æœåŠ¡å™¨ä»¥å¯ç”¨å®æ—¶è¯­éŸ³è½¬å†™åŠŸèƒ½" if not asr_system else "å®æ—¶è¯­éŸ³è½¬å†™åŠŸèƒ½å·²å¯ç”¨"
        }
    })

    try:
        while True:
            # Keep the connection alive
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket)

# --- LLM Endpoints ---

@app.get("/api/ui_state")
async def get_ui_state():
    """è·å–å‰ç«¯ UI çŠ¶æ€"""
    return load_ui_state()

@app.post("/api/ui_state")
async def update_ui_state(data: dict = Body(...)):
    """æ›´æ–°å‰ç«¯ UI çŠ¶æ€ (å¢é‡æ›´æ–°)"""
    current_state = load_ui_state()
    # æ·±åº¦åˆå¹¶æˆ–æ›¿æ¢é¡¶å±‚é”®
    current_state.update(data)
    save_ui_state(current_state)
    return {"status": "success", "state": current_state}

@app.get("/api/identities")
async def get_identities():
    """è·å–å¯ç”¨èº«ä»½"""
    return load_think_tank_roles()


def validate_identity_payload(role_id: str, name: str, prompt: str):
    if not role_id:
        raise HTTPException(status_code=400, detail="è¯·æä¾›å”¯ä¸€çš„èº«ä»½ID")
    if not re.match(r"^[a-z0-9_-]+$", role_id):
        raise HTTPException(status_code=400, detail="ID ä»…èƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿æˆ–è¿å­—ç¬¦")
    if not name:
        raise HTTPException(status_code=400, detail="è¯·è¾“å…¥èº«ä»½åç§°")
    if not prompt:
        raise HTTPException(status_code=400, detail="è¯·è¾“å…¥èº«ä»½æç¤ºè¯")


@app.post("/api/identities")
async def create_identity(data: dict = Body(...)):
    raw_id = data.get("id", "")
    normalized_id = normalize_identity_identifier(raw_id)
    name = (data.get("name") or "").strip()
    prompt = (data.get("prompt") or "").strip()
    enabled = bool(data.get("enabled", True))

    validate_identity_payload(normalized_id, name, prompt)

    roles = load_think_tank_roles()
    if any(normalize_identity_identifier(role.get("id")) == normalized_id for role in roles):
        raise HTTPException(status_code=400, detail="è¯¥èº«ä»½IDå·²å­˜åœ¨")

    new_role = {
        "id": normalized_id,
        "name": name,
        "prompt": prompt,
        "enabled": enabled
    }
    roles.append(new_role)
    save_think_tank_roles(roles)
    return {"status": "success", "role": new_role}


@app.put("/api/identities/{role_id}")
async def update_identity(role_id: str, data: dict = Body(...)):
    normalized_id = normalize_identity_identifier(role_id)
    roles = load_think_tank_roles()
    target = next((role for role in roles if normalize_identity_identifier(role.get("id")) == normalized_id), None)
    if not target:
        raise HTTPException(status_code=404, detail="èº«ä»½ä¸å­˜åœ¨")

    if "name" in data:
        name = (data.get("name") or "").strip()
        if not name:
            raise HTTPException(status_code=400, detail="èº«ä»½åç§°ä¸èƒ½ä¸ºç©º")
        target["name"] = name

    if "prompt" in data:
        prompt = (data.get("prompt") or "").strip()
        if not prompt:
            raise HTTPException(status_code=400, detail="æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        target["prompt"] = prompt

    if "enabled" in data:
        target["enabled"] = bool(data.get("enabled"))

    save_think_tank_roles(roles)
    return {"status": "success", "role": target}


@app.delete("/api/identities/{role_id}")
async def delete_identity(role_id: str):
    normalized_id = normalize_identity_identifier(role_id)
    roles = load_think_tank_roles()
    index = next(
        (idx for idx, role in enumerate(roles) if normalize_identity_identifier(role.get("id")) == normalized_id),
        None
    )
    if index is None:
        raise HTTPException(status_code=404, detail="èº«ä»½ä¸å­˜åœ¨")

    removed = roles.pop(index)
    save_think_tank_roles(roles)
    return {"status": "success", "removed": removed}

@app.get("/api/config")
async def get_config():
    return load_config()

@app.post("/api/config")
async def update_config(data: dict = Body(...)):
    """
    Update configuration. 
    Expected data: { "configs": [...], "current_config": "Name" }
    """
    save_config(data)
    
    # Reload LLM Client if current config changed
    new_current_name = data.get("current_config")
    new_config = next((c for c in data.get("configs", []) if c["name"] == new_current_name), None)
    
    if new_config:
        llm_client.update_config(
            api_key=new_config.get("api_key"),
            base_url=new_config.get("base_url"),
            model=new_config.get("model")
        )
        # Update ResumeManager's LLM client as well
        resume_manager.set_llm_client(llm_client)
        # Update JobManager's LLM client too
        job_manager.set_llm_client(llm_client)
    
    return {"status": "success", "message": "é…ç½®å·²æ›´æ–°"}

@app.post("/api/test_connection")
async def test_connection_endpoint(data: dict = Body(...)):
    """
    Test connection with provided config.
    """
    api_key = data.get("api_key")
    base_url = data.get("base_url")
    model = data.get("model")
    
    if not all([api_key, base_url, model]):
        return {"success": False, "message": "ç¼ºå°‘å¿…éœ€å­—æ®µ"}
        
    client = LLMClient(api_key=api_key, base_url=base_url, model=model)
    success, message = await client.test_connection()
    return {"success": success, "message": message}

# --- Chat Management Endpoints ---

@app.get("/api/chats")
async def get_chats():
    return {
        "current_chat_id": chat_manager.get_current_chat_id(),
        "chats": chat_manager.get_all_chats()
    }

@app.post("/api/chats")
async def create_chat(data: dict = Body(...)):
    title = data.get("title", "æ–°èŠå¤©")
    new_chat = chat_manager.create_chat(title)
    return new_chat

@app.get("/api/chats/{chat_id}")
async def get_chat(chat_id: str):
    chat = chat_manager.get_chat(chat_id)
    if not chat:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°èŠå¤©")
    return chat

@app.delete("/api/chats/{chat_id}")
async def delete_chat(chat_id: str):
    success = chat_manager.delete_chat(chat_id)
    if not success:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°èŠå¤©")
    return {"status": "success"}

@app.post("/api/chats/{chat_id}/clear")
async def clear_chat(chat_id: str):
    success = chat_manager.clear_chat_messages(chat_id)
    if not success:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°èŠå¤©")
    return {"status": "success"}

# --- Intelligent Agent Endpoints ---

@app.get("/api/agent/status")
async def get_agent_status():
    """è·å–æ™ºèƒ½ Agent çŠ¶æ€"""
    if not AGENT_AVAILABLE:
        return {"available": False, "message": "æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨"}

    config_data = load_config()
    agent_config = config_data.get("agent_config", {})

    return {
        "available": True,
        "enabled": agent_manager.enabled,
        "auto_trigger": agent_manager.auto_trigger,
        "status": trigger_manager.get_status(),
        "config": agent_config,
        "model_local": config_data.get("model_local", ["Qwen3-0.6B"])
    }

@app.get("/api/agent/roles")
async def get_agent_roles():
    """è·å–æ™ºå›Šå›¢è§’è‰²é…ç½®"""
    try:
        with open("data/agent.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {"think_tank_roles": []}
    except json.JSONDecodeError:
        return {"think_tank_roles": []}

@app.post("/api/agent/enable")
async def enable_agent(data: dict = Body(...)):
    """å¯ç”¨/ç¦ç”¨æ™ºèƒ½ Agent"""
    if not AGENT_AVAILABLE:
        raise HTTPException(status_code=503, detail="æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")

    enabled = data.get("enabled", True)
    auto_trigger = data.get("auto_trigger", True)

    agent_manager.enabled = enabled
    agent_manager.auto_trigger = auto_trigger
    trigger_manager.set_enabled(enabled)

    # Update config
    config_data = load_config()
    agent_config = config_data.get("agent_config", {})
    agent_config["enabled"] = enabled
    agent_config["auto_trigger"] = auto_trigger
    config_data["agent_config"] = agent_config
    save_config(config_data)

    return {
        "status": "success",
        "enabled": enabled,
        "auto_trigger": auto_trigger
    }

@app.post("/api/agent/config")
async def update_agent_config(data: dict = Body(...)):
    """æ›´æ–°æ™ºèƒ½ Agent é…ç½®"""
    if not AGENT_AVAILABLE:
        raise HTTPException(status_code=503, detail="æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")
    print(f"DEBUG: Received update_agent_config data: {data}")
    
    # Update config file
    config_data = load_config()
    agent_config = config_data.get("agent_config", {})
    
    # Only update fields that are present in data
    if "min_characters" in data:
        agent_config["min_characters"] = data["min_characters"]
        
    if "silence_threshold" in data:
        agent_config["silence_threshold"] = data["silence_threshold"]
        
    if "max_messages" in data:
        agent_config["max_messages"] = data["max_messages"]
        
    if "model_name" in data:
        agent_config["model_name"] = data["model_name"]
        
    if "model_type" in data:
        agent_config["model_type"] = data["model_type"]
        
    if "intent_recognition_enabled" in data:
        agent_config["intent_recognition_enabled"] = data["intent_recognition_enabled"]
        
    if "intent_model_name" in data:
        agent_config["intent_model_name"] = data["intent_model_name"]
        
    if "intent_model_type" in data:
        agent_config["intent_model_type"] = data["intent_model_type"]

    # Update trigger manager thresholds if changed
    min_chars = agent_config.get("min_characters", 10)
    silence_thresh = agent_config.get("silence_threshold", 2)
    max_msgs = agent_config.get("max_messages", 50)
    
    trigger_manager.set_thresholds(min_chars, silence_thresh)
    trigger_manager.set_max_history(max_msgs)

    config_data["agent_config"] = agent_config
    save_config(config_data)

    # Reload agent if model changed
    model_name = agent_config.get("model_name")
    if model_name and AGENT_AVAILABLE:
        model_config = next(
            (c for c in config_data.get("configs", []) if c["name"] == model_name),
            None
        )
        if model_config:
            model_config['model_type'] = 'api'
            agent_manager.load_agent(agent_config, model_config)

    return {"status": "success", "config": agent_config}

@app.post("/api/agent/analyze")
async def manual_analyze(data: dict = Body(...)):
    """æ‰‹åŠ¨è§¦å‘æ™ºèƒ½åˆ†ææˆ–æ„å›¾è¯†åˆ«"""
    if not AGENT_AVAILABLE:
        raise HTTPException(status_code=503, detail="æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")

    messages = data.get("messages", [])
    speaker_name = data.get("speaker_name", "ç”¨æˆ·")
    request_type = data.get("request_type", "agent_analysis")  # åŒºåˆ†æ˜¯æ™ºèƒ½åˆ†æè¿˜æ˜¯æ„å›¾è¯†åˆ«
    modules_param = data.get("modules")
    print(f"[AgentAPI] æ”¶åˆ° /api/agent/analyze è¯·æ±‚ -> type={request_type}, speaker={speaker_name}, messages={len(messages)}")

    def load_intent_agent(intent_cfg: dict):
        config_data = load_config()
        model_type = intent_cfg.get("model_type", "local")
        model_name = intent_cfg.get("model_name", "Qwen3-0.6B")
        print(f"[AgentAPI] æ„å›¾è¯†åˆ«æ¨¡å‹é…ç½®: type={model_type}, name={model_name}")

        model_config = None
        if model_type == "api":
            model_config = next(
                (c for c in config_data.get("configs", []) if c["name"] == model_name),
                None
            )
            if model_config:
                model_config['model_type'] = 'api'
                print(f"[æ„å›¾è¯†åˆ«] å·²åŠ è½½APIæ¨¡å‹: {model_name}")
            else:
                print(f"[æ„å›¾è¯†åˆ«] æœªæ‰¾åˆ°APIæ¨¡å‹ '{model_name}'ï¼Œé™çº§åˆ°æœ¬åœ°æ¨¡å¼")
                model_type = "local"
                model_name = "Qwen3-0.6B"
                model_config = None

        agent_manager.configure_intent_agent(
            {
                "model_type": model_type,
                "model_name": model_name
            },
            model_config
        )
        if model_type == "local":
            print(f"[æ„å›¾è¯†åˆ«] å·²åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")

    def normalize_modules(value):
        if not value:
            return None
        if isinstance(value, str):
            candidates = [value]
        else:
            candidates = list(value)
        normalized = {'analysis': False, 'intent': False, 'think_tank': False}
        for item in candidates:
            name = str(item).strip().lower()
            if name in ("analysis", "smart", "smart_analysis"):
                normalized['analysis'] = True
            elif name in ("intent", "intent_recognition"):
                normalized['intent'] = True
            elif name in ("think_tank", "thinktank", "distribution"):
                normalized['think_tank'] = True
        return normalized if any(normalized.values()) else None

    modules_request = normalize_modules(modules_param)
    if not modules_request and request_type == "intent_recognition":
        modules_request = {'analysis': False, 'intent': True, 'think_tank': False}

    if modules_request:
        print(f"[AgentAPI] modules_request={modules_request}, request_type={request_type}")

        if modules_request['intent']:
            intent_config = data.get("intent_recognition_config", {})
            load_intent_agent(intent_config)

        print(
            "[AgentAPI] è¿è¡Œæ¨¡å— -> "
            f"analysis={modules_request['analysis']} | "
            f"intent={modules_request['intent']} | "
            f"think_tank={modules_request['think_tank']}"
        )

        result = await agent_manager.run_pipeline(
            messages,
            speaker_name,
            use_analysis=modules_request['analysis'],
            use_intent=modules_request['intent'],
            use_think_tank=modules_request['think_tank'],
            bypass_enabled=True,
            force_modules=True
        )

        intent_success = bool(result.get("phase2", {}).get("success")) if modules_request['intent'] else None
        print(
            "[AgentAPI] pipelineå®Œæˆ -> "
            f"phase1_reason={result.get('phase1', {}).get('reason')} | "
            f"intent_success={intent_success}"
        )

        if modules_request['intent'] and not modules_request['analysis']:
            phase2_result = result.get("phase2", {})
            success = bool(phase2_result and phase2_result.get("success"))
            reason = "æ„å›¾è¯†åˆ«å®Œæˆ" if success else phase2_result.get("error", "æ„å›¾è¯†åˆ«å¤±è´¥")
            result["phase1"] = {
                "is": False,
                "reason": reason,
                "confidence": 0.0,
                "intent_only": True,
                "intent_success": success
            }

        return result

    # é»˜è®¤æ‰§è¡Œé˜¶æ®µ1åˆ†æ
    result = await agent_manager.analyze_conversation(messages, speaker_name)
    summary_flag = result.get("is")
    summary_reason = result.get("reason", "")
    print(f"[AgentAPI] åˆ†æå®Œæˆ -> need_ai={summary_flag}, reason={summary_reason}")
    return result

@app.get("/api/protagonist")
async def get_protagonist():
    """è·å–å½“å‰ä¸»äººå…¬é…ç½®"""
    config_data = load_config()
    protagonist = config_data.get("protagonist", "")
    return {"protagonist": protagonist}

@app.post("/api/protagonist")
async def set_protagonist_endpoint(data: dict = Body(...)):
    """è®¾ç½®ä¸»äººå…¬"""
    if not AGENT_AVAILABLE:
        raise HTTPException(status_code=503, detail="æ™ºèƒ½ Agent æ¨¡å—ä¸å¯ç”¨")
    
    protagonist = data.get("protagonist", "").strip()
    
    # æ›´æ–°trigger manager
    trigger_manager.set_protagonist(protagonist)
    
    # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
    config_data = load_config()
    config_data["protagonist"] = protagonist
    save_config(config_data)
    
    return {"status": "success", "protagonist": protagonist}

@app.post("/api/agent/trigger")
async def trigger_multi_llm(data: dict = Body(...)):
    """æ‰‹åŠ¨è§¦å‘æ™ºå›Šå›¢"""
    messages = data.get("messages", [])
    chat_id = data.get("chat_id")

    # This will be handled by the WebSocket endpoint
    return {
        "status": "triggered",
        "message": "æ™ºå›Šå›¢å·²è§¦å‘",
        "messages": messages,
        "chat_id": chat_id
    }

# --- å£°çº¹ç®¡ç† API ---

@app.get("/api/voiceprints")
async def get_voiceprints():
    """è·å–å£°çº¹åº“åˆ—è¡¨"""
    # å³ä½¿ASRç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œä¹Ÿèƒ½æŸ¥çœ‹å£°çº¹åˆ—è¡¨
    voiceprint_dir = asr_system.VOICEPRINT_DIR if asr_system else "voiceprints"

    if not os.path.exists(voiceprint_dir):
        return {"voiceprints": []}

    voiceprints = []
    for filename in os.listdir(voiceprint_dir):
        if filename.lower().endswith('.wav'):
            name = os.path.splitext(filename)[0]
            wav_path = os.path.join(voiceprint_dir, filename)
            npy_path = os.path.join(voiceprint_dir, f"{name}.npy")

            # è·å–æ–‡ä»¶å¤§å°
            wav_size = os.path.getsize(wav_path)
            has_embedding = os.path.exists(npy_path)
            embedding_size = os.path.getsize(npy_path) if has_embedding else 0

            # è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç®€å•ä¼°ç®—ï¼‰
            try:
                import soundfile as sf
                info = sf.info(wav_path)
                duration = round(info.duration, 2)
            except:
                duration = None

            voiceprints.append({
                "name": name,
                "wav_file": filename,
                "wav_size": wav_size,
                "has_embedding": has_embedding,
                "embedding_size": embedding_size,
                "duration": duration,
                "created_time": os.path.getctime(wav_path)
            })

    return {"voiceprints": voiceprints}

@app.post("/api/voiceprints")
async def create_voiceprint(data: dict = Body(...)):
    """å½•åˆ¶å¹¶ä¿å­˜æ–°çš„å£°çº¹"""
    name = data.get("name", "").strip()
    audio_data = data.get("audio_data", "")

    if not name:
        raise HTTPException(status_code=400, detail="è¯·è¾“å…¥è¯´è¯äººå§“å")

    if not audio_data:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘éŸ³é¢‘æ•°æ®")

    # æ£€æŸ¥å§“åæ˜¯å¦å·²å­˜åœ¨
    voiceprint_dir = asr_system.VOICEPRINT_DIR if asr_system else "voiceprints"
    wav_path = os.path.join(voiceprint_dir, f"{name}.wav")
    npy_path = os.path.join(voiceprint_dir, f"{name}.npy")

    if os.path.exists(wav_path):
        raise HTTPException(status_code=400, detail=f"è¯´è¯äºº '{name}' å·²å­˜åœ¨")

    try:
        # è§£ç  base64 éŸ³é¢‘æ•°æ®
        # å‰ç«¯å‘é€çš„æ ¼å¼: "data:audio/wav;base64,<base64_data>"
        if ',' in audio_data:
            header, audio_base64 = audio_data.split(',', 1)
        else:
            audio_base64 = audio_data

        audio_bytes = base64.b64decode(audio_base64)

        # ä¿å­˜ä¸ºä¸´æ—¶ WAV æ–‡ä»¶
        temp_path = os.path.join(voiceprint_dir, f"temp_{name}.wav")
        with open(temp_path, 'wb') as f:
            f.write(audio_bytes)

        # æ£€æŸ¥éŸ³é¢‘æ—¶é•¿
        duration = None
        try:
            import soundfile as sf
            info = sf.info(temp_path)
            duration = info.duration

            if duration < 10:
                os.remove(temp_path)
                raise HTTPException(status_code=400, detail=f"å½•åˆ¶æ—¶é•¿å¤ªçŸ­ ({duration:.1f}ç§’)ï¼Œè‡³å°‘éœ€è¦ 10 ç§’")

            if duration > 40:
                os.remove(temp_path)
                raise HTTPException(status_code=400, detail=f"å½•åˆ¶æ—¶é•¿å¤ªé•¿ ({duration:.1f}ç§’)ï¼Œæœ€å¤š 40 ç§’")
        except Exception as e:
            os.remove(temp_path)
            raise HTTPException(status_code=400, detail=f"éŸ³é¢‘éªŒè¯å¤±è´¥: {str(e)}")

        # å¦‚æœ ASR ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œä½¿ç”¨å®Œæ•´æµç¨‹
        if asr_system:
            # è½¬æ¢å¹¶ä¿å­˜ä¸ºæ ‡å‡†æ ¼å¼
            asr_system.check_and_convert_audio(temp_path)
            # é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶å
            os.rename(temp_path, wav_path)

            # è®¡ç®—å¹¶ä¿å­˜åµŒå…¥
            print(f"æ­£åœ¨ä¸º {name} è®¡ç®—å£°çº¹åµŒå…¥...")
            embedding = asr_system.extract_embedding(wav_path)
            if embedding is not None:
                import numpy as np
                np.save(npy_path, embedding)
                print(f"âœ… å£°çº¹åµŒå…¥å·²ä¿å­˜: {name}")

                # é‡æ–°åŠ è½½å£°çº¹åº“
                asr_system.load_voiceprints()

                return {
                    "status": "success",
                    "message": f"å£°çº¹å·²ä¿å­˜: {name}",
                    "name": name,
                    "duration": duration,
                    "embedding_saved": True
                }
            else:
                os.remove(wav_path)
                if os.path.exists(npy_path):
                    os.remove(npy_path)
                raise HTTPException(status_code=500, detail="å£°çº¹åµŒå…¥è®¡ç®—å¤±è´¥")
        else:
            # ASR ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œåªä¿å­˜ WAV æ–‡ä»¶
            # åç»­ main.py å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨è½¬æ¢
            os.rename(temp_path, wav_path)
            print(f"âš ï¸  ASR ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œå·²ä¿å­˜ WAV æ–‡ä»¶: {name}")
            print(f"ğŸ’¡ æç¤ºï¼šä½¿ç”¨æ­£å¸¸æ¨¡å¼å¯åŠ¨æœåŠ¡å™¨æ—¶ä¼šè‡ªåŠ¨è®¡ç®—å£°çº¹åµŒå…¥")

            return {
                "status": "success",
                "message": f"å£°çº¹å·²ä¿å­˜: {name}ï¼ˆä»…éŸ³é¢‘æ–‡ä»¶ï¼ŒåµŒå…¥å°†åœ¨ä¸‹æ¬¡æ­£å¸¸å¯åŠ¨æ—¶è®¡ç®—ï¼‰",
                "name": name,
                "duration": duration,
                "embedding_saved": False
            }

    except HTTPException:
        raise
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_path = os.path.join(voiceprint_dir, f"temp_{name}.wav")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise HTTPException(status_code=500, detail=f"ä¿å­˜å¤±è´¥: {str(e)}")

@app.delete("/api/voiceprints/{name}")
async def delete_voiceprint(name: str):
    """åˆ é™¤å£°çº¹"""
    # å³ä½¿ASRç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œä¹Ÿèƒ½åˆ é™¤å£°çº¹æ–‡ä»¶
    name = name.strip()
    if not name:
        raise HTTPException(status_code=400, detail="è¯·æä¾›è¯´è¯äººå§“å")

    voiceprint_dir = asr_system.VOICEPRINT_DIR if asr_system else "voiceprints"
    wav_path = os.path.join(voiceprint_dir, f"{name}.wav")
    npy_path = os.path.join(voiceprint_dir, f"{name}.npy")

    deleted_files = []

    # åˆ é™¤ WAV æ–‡ä»¶
    if os.path.exists(wav_path):
        os.remove(wav_path)
        deleted_files.append(f"{name}.wav")

    # åˆ é™¤ NPY æ–‡ä»¶
    if os.path.exists(npy_path):
        os.remove(npy_path)
        deleted_files.append(f"{name}.npy")

    if not deleted_files:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°è¯´è¯äºº '{name}' çš„å£°çº¹")

    # åªæœ‰åœ¨ASRç³»ç»Ÿåˆå§‹åŒ–æ—¶æ‰é‡æ–°åŠ è½½å£°çº¹åº“
    if asr_system:
        asr_system.load_voiceprints()

    return {
        "status": "success",
        "message": f"å·²åˆ é™¤å£°çº¹: {name}",
        "deleted_files": deleted_files
    }

@app.post("/api/voiceprints/rebuild")
async def rebuild_voiceprints():
    """é‡æ–°è®¡ç®—æ‰€æœ‰å£°çº¹åµŒå…¥"""
    if not asr_system:
        return {
            "status": "error",
            "detail": "ASR ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ— æ³•é‡æ–°è®¡ç®—åµŒå…¥ã€‚",
            "message": "è¯·ä½¿ç”¨æ­£å¸¸æ¨¡å¼å¯åŠ¨æœåŠ¡å™¨ï¼ˆä¸ä½¿ç”¨ --no å‚æ•°ï¼‰"
        }

    try:
        asr_system.load_voiceprints()
        return {
            "status": "success",
            "message": "å£°çº¹åµŒå…¥é‡æ–°è®¡ç®—å®Œæˆ",
            "count": len(asr_system.speakers)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‡æ–°è®¡ç®—å¤±è´¥: {str(e)}")

@app.get("/api/voiceprint/audio/{name}")
async def get_voiceprint_audio(name: str):
    """è·å–å£°çº¹éŸ³é¢‘æ–‡ä»¶"""
    # å³ä½¿ASRç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œä¹Ÿèƒ½æä¾›éŸ³é¢‘æ–‡ä»¶
    # è§£ç URLç¼–ç çš„åå­—
    name = name.strip()
    if not name:
        raise HTTPException(status_code=400, detail="è¯·æä¾›è¯´è¯äººå§“å")

    voiceprint_dir = asr_system.VOICEPRINT_DIR if asr_system else "voiceprints"
    wav_path = os.path.join(voiceprint_dir, f"{name}.wav")

    if not os.path.exists(wav_path):
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°å£°çº¹æ–‡ä»¶: {name}")

    # è¿”å›éŸ³é¢‘æ–‡ä»¶
    from fastapi.responses import FileResponse
    return FileResponse(
        wav_path,
        media_type='audio/wav',
        filename=f"{name}.wav"
    )


# --- Resume Management Endpoints ---

@app.post("/api/resume/upload")
async def upload_resume(file: UploadFile = File(...)):
    """Upload and parse resume PDF."""
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="ä»…æ”¯æŒ PDF æ–‡ä»¶")
    
    # Check if already processing
    status = resume_manager.get_status()
    if status["state"] == "processing":
        return JSONResponse(status_code=400, content={"status": "error", "message": "æ­£åœ¨å¤„ç†å¦ä¸€ä¸ªç®€å†ï¼Œè¯·ç¨å€™æˆ–åœæ­¢å½“å‰ä»»åŠ¡"})

    try:
        content = await file.read()
        # Save PDF
        pdf_path = await resume_manager.save_pdf(content, file.filename)
        
        # Start background processing
        current_config_data = load_config()
        task = asyncio.create_task(resume_manager.process_resume_task(pdf_path, config_data=current_config_data))
        resume_manager.current_task = task
        
        return {"status": "success", "message": "ç®€å†å·²ä¸Šä¼ ï¼Œå¼€å§‹åå°åˆ†æ..."}
            
    except Exception as e:
        print(f"Resume upload error: {e}")
        return JSONResponse(status_code=500, content={"status": "error", "message": str(e)})

@app.post("/api/resume/stop")
async def stop_resume_processing():
    """Stop current resume processing."""
    await resume_manager.stop_processing()
    return {"status": "success", "message": "å·²åœæ­¢å¤„ç†"}

@app.get("/api/resume/status")
async def get_resume_status():
    """Get resume status and configuration."""
    status = resume_manager.get_status()
    status["personalization_enabled"] = resume_personalization_enabled
    return status

@app.post("/api/resume/toggle")
async def toggle_resume_personalization(data: dict = Body(...)):
    """Toggle resume personalization."""
    global resume_personalization_enabled
    enabled = data.get("enabled", False)
    resume_personalization_enabled = enabled
    return {"status": "success", "enabled": resume_personalization_enabled}

@app.get("/api/resume/xml")
async def get_resume_xml():
    """Get the parsed resume XML."""
    xml = resume_manager.get_resume_xml()
    if not xml:
        raise HTTPException(status_code=404, detail="Resume not found")
    return {"xml": xml}

@app.get("/api/resume/markdown")
async def get_resume_markdown():
    """Get the parsed resume Markdown."""
    md = resume_manager.get_resume_markdown()
    if not md:
        raise HTTPException(status_code=404, detail="Resume markdown not found")
    return {"markdown": md}

    return {"markdown": md}

# --- Job Analysis Endpoints ---

@app.post("/api/job/generate")
async def generate_job_analysis(data: dict = Body(...)):
    """Generate job analysis background task."""
    title = data.get("title")
    jd = data.get("jd", "")
    
    if not title:
        raise HTTPException(status_code=400, detail="è¯·æä¾›èŒä½æ ‡é¢˜")
        
    # Check if already processing
    status = job_manager.processing_status
    if status["state"] == "processing":
         return JSONResponse(status_code=400, content={"status": "error", "message": "å¦ä¸€ä¸ªåˆ†ææ­£åœ¨è¿›è¡Œä¸­"})

    current_config_data = load_config()
    # Merge transient options from request
    if "thinking_mode" in data:
         if "job_config" not in current_config_data:
             current_config_data["job_config"] = {}
         current_config_data["job_config"]["thinking_mode"] = data["thinking_mode"]

    await job_manager.generate_analysis(title, jd, config_data=current_config_data)
    
    # Trigger cache update (optimistic or wait for completion? 
    # Since it's async, we might not update immediately, but the client will poll status.
    # The cache should be updated after completion. 
    # For simplicity, we can't update cache here. 
    # But we can update it in the status check or separate task.)
    
    return {"status": "success", "message": "å·²å¼€å§‹èŒä½åˆ†æç”Ÿæˆ"}

@app.get("/api/job/status")
async def get_job_status():
    status = job_manager.processing_status.copy()
    info = job_manager.get_job_info()
    has_content = os.path.exists(job_manager.job_analysis_path)
    
    # If completed and cache is empty, update cache
    if status["state"] == "completed" and not CACHED_JOB_CONTEXT:
        update_job_context_cache()
    
    # Also if state is idle but file exists (restart case), ensure cache
    if status["state"] == "idle" and has_content and not CACHED_JOB_CONTEXT:
        update_job_context_cache()

    return {
        "status": status,
        "info": info,
        "has_analysis": has_content
    }

@app.get("/api/job/content")
async def get_job_content():
    content = await job_manager.get_analysis_content()
    if not content:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°åˆ†æç»“æœ")
    return {"content": content}

@app.post("/api/job/clear")
async def clear_job_analysis():
    job_manager.clear_analysis()
    update_job_context_cache()
    return {"status": "success", "message": "èŒä½åˆ†æå·²æ¸…ç©º"}

def inject_job_analysis_to_messages(messages: list[dict]):
    """Inject job analysis context into system prompt if available."""
    if not CACHED_JOB_CONTEXT:
        return

    prompt = f"""
    ä¸ºäº†è®©å›ç­”æ›´å…·é’ˆå¯¹æ€§ï¼Œè¯·å‚è€ƒä»¥ä¸‹ã€ç›®æ ‡å²—ä½ä¿¡æ¯ã€‘è¿›è¡Œé€‚é…ï¼š
    <job_context_global>{CACHED_JOB_CONTEXT}</job_context_global>
    """

    found_system = False
    for msg in messages:
        if msg.get("role") == "system":
            if "<job_context_global>" not in msg["content"]:
                msg["content"] += prompt
            found_system = True
            break
            
    if not found_system:
        messages.insert(0, {"role": "system", "content": prompt})

def inject_resume_to_messages(messages: list[dict]):
    """Inject resume XML into system prompt if enabled."""
    if not resume_personalization_enabled:
        return
    
    xml = resume_manager.get_resume_xml()
    if not xml:
        return

    prompt = f"\n\n<resume_context>\n{xml}\n</resume_context>\nè¯·æ ¹æ®ä»¥ä¸Šç®€å†ä¿¡æ¯ï¼Œä¸ªæ€§åŒ–ä½ çš„å›ç­”ã€‚"
    
    for msg in messages:
        if msg.get("role") == "system":
            if "<resume_context>" not in msg["content"]:
                msg["content"] += prompt
            return
            
    messages.insert(0, {"role": "system", "content": prompt})

@app.websocket("/ws/llm")
async def llm_websocket(websocket: WebSocket):
    await llm_manager.connect(websocket)
    current_data = load_config()
    curr_name = current_data.get("current_config")
    curr_conf = next((c for c in current_data.get("configs", []) if c["name"] == curr_name), None)

    if curr_conf:
        llm_client.update_config(
            api_key=curr_conf.get("api_key"),
            base_url=curr_conf.get("base_url"),
            model=curr_conf.get("model")
        )
        job_manager.set_llm_client(llm_client)
    
    # Store initial config to detect changes
    last_config_signature = None

    try:
        while True:
            data = await websocket.receive_json()

            # Reload config for every request to ensure freshness
            current_data = load_config()
            curr_name = current_data.get("current_config")
            curr_conf = next((c for c in current_data.get("configs", []) if c["name"] == curr_name), None)
            
            # Update LLM Client if config changed
            if curr_conf:
                # Create a signature to check if we really need to update LLM client (avoid overhead if possible, though update_config is cheap)
                # But simple update is fine.
                llm_client.update_config(
                    api_key=curr_conf.get("api_key"),
                    base_url=curr_conf.get("base_url"),
                    model=curr_conf.get("model")
                )
                resume_manager.set_llm_client(llm_client)
                job_manager.set_llm_client(llm_client)

            # å¤„ç†æ™ºèƒ½åˆ†æè§¦å‘æ¶ˆæ¯
            if data.get("type") == "agent_triggered":
                print(f"[æ™ºèƒ½åˆ†æ] âœ… WebSocket æ”¶åˆ°è§¦å‘æ¶ˆæ¯")
                messages = data.get("messages", [])
                chat_id = data.get("chat_id")
                is_multi_llm = data.get("is_multi_llm", False)
                intent_recognition = data.get("intent_recognition", False)

                print(f"[æ™ºèƒ½åˆ†æ] ğŸ“‹ æ¶ˆæ¯è¯¦æƒ…:")
                print(f"  - åˆ†å‘æ¨¡å¼: {'æ™ºå›Šå›¢' if is_multi_llm else 'å•æ¨¡å‹'}")
                print(f"  - æ„å›¾è¯†åˆ«: {'å¼€å¯' if intent_recognition else 'å…³é—­'}")
                print(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
                print(f"  - èŠå¤©ID: {chat_id}")
                print(f"[æ™ºèƒ½åˆ†æ] ğŸ“ æ¶ˆæ¯å†…å®¹é¢„è§ˆ:")
                for i, msg in enumerate(messages):
                    print(f"  [{i}] {msg.get('role', 'unknown')}: {str(msg.get('content', ''))[:50]}{'...' if len(str(msg.get('content', ''))) > 50 else ''}")

                # æ ¹æ®æ¨¡å¼å¤„ç†
                if is_multi_llm:
                    # å¤„ç†æ™ºå›Šå›¢æ¨¡å¼
                    await handle_multi_llm_request(websocket, messages, chat_id)
                else:
                    # å¤„ç†å•æ¨¡å‹æ¨¡å¼
                    await websocket.send_json({
                        "type": "agent_notification",
                        "content": "ğŸ¤– æ™ºèƒ½åˆ†æå·²å¯åŠ¨ï¼Œå°†ä¸ºæ‚¨æä¾›ä¸“ä¸šå»ºè®®"
                    })

                    # ä¿®å¤ï¼šå¤„ç†å½“å‰é…ç½®çš„ System Prompt
                    current_messages = [m.copy() for m in messages]
                    config_prompt = (curr_conf.get("system_prompt", "") if curr_conf else "").strip()

                    # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†èº«ä»½æ ‡ç­¾
                    raw_tags = curr_conf.get("tags", []) if curr_conf else []
                    normalized_tags = [normalize_identity_identifier(tag) for tag in raw_tags if tag]
                    roles = load_think_tank_roles()
                    role_lookup = build_identity_lookup(roles)
                    active_tag, active_role, disabled_candidates = select_identity_role(normalized_tags, role_lookup)
                    identity_applied = False

                    # åº”ç”¨ System Prompt
                    if active_role:
                        tag_prompt = active_role["prompt"]
                        identity_applied = True
                        print(f"[æ™ºèƒ½åˆ†æ] åº”ç”¨èº«ä»½æ ‡ç­¾ Prompt: {active_role['name']}")
                        sys_idx = next((i for i, m in enumerate(current_messages) if m["role"] == "system"), -1)
                        if sys_idx != -1:
                            current_messages[sys_idx]["content"] = tag_prompt
                        else:
                            current_messages.insert(0, {"role": "system", "content": tag_prompt})
                    elif config_prompt:
                        sys_idx = next((i for i, m in enumerate(current_messages) if m["role"] == "system"), -1)
                        if sys_idx != -1:
                            current_messages[sys_idx]["content"] = config_prompt
                        else:
                            current_messages.insert(0, {"role": "system", "content": config_prompt})
                    elif normalized_tags:
                        if disabled_candidates:
                            print(f"[æ™ºèƒ½åˆ†æ] èº«ä»½å·²åœç”¨ï¼Œè·³è¿‡ Prompt: {', '.join(disabled_candidates)}")
                        else:
                            print(f"[æ™ºèƒ½åˆ†æ] æœªæ‰¾åˆ°æ ‡ç­¾ '{normalized_tags[0]}' çš„ Prompt å®šä¹‰")

                    # Check if job analysis exists locally
                    if not os.path.exists(job_manager.job_analysis_path):
                        error_msg = "è¯·å…ˆè®¾ç½®ç›®æ ‡å²—ä½ï¼Œå®Œæˆå²—ä½åˆ†æã€‚åŠ©æ‰‹å¯¹è¯æ¡†å³ä¸Šè§’â†’è®¾ç½®ç›®æ ‡å²—ä½"
                        await websocket.send_json({"type": "done", "full_text": error_msg})
                        if chat_id:
                            messages.append({"role": "assistant", "content": error_msg})
                            chat_manager.update_chat_messages(chat_id, messages)
                        continue

                    inject_job_analysis_to_messages(current_messages)

                    # [è°ƒè¯•] æ˜¾ç¤ºå®é™…å‘é€ç»™æ¨¡å‹çš„å®Œæ•´ prompt
                    print(f"\n{'='*80}")
                    print(f"[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] æ­£åœ¨å‘é€è¯·æ±‚åˆ°æ¨¡å‹: {curr_conf.get('model', 'Unknown')} (Stream=True)")
                    print(f"{'='*80}")
                    print(f"[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] å½“å‰é…ç½®: {curr_conf.get('name', 'Unknown')}")
                    print(f"[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] ä½¿ç”¨ System Prompt: {config_prompt if (config_prompt and not identity_applied) else 'å¦'}")
                    if normalized_tags:
                        if identity_applied and active_role:
                            print(f"[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] èº«ä»½æ ‡ç­¾: {normalized_tags} â†’ æ¿€æ´»: {active_role['name']} ({active_tag})")
                        elif disabled_candidates:
                            print(f"[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] èº«ä»½æ ‡ç­¾: {normalized_tags} (åœç”¨: {', '.join(disabled_candidates)})")
                        else:
                            print(f"[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] èº«ä»½æ ‡ç­¾: {normalized_tags} (æœªæ‰¾åˆ°å¯ç”¨èº«ä»½)")
                    print(f"[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] æ¶ˆæ¯æ€»æ•°: {len(current_messages)}")
                    print(f"{'-'*80}")
                    print("[è°ƒè¯•] [æ™ºèƒ½åˆ†æ] å®Œæ•´ Prompt å†…å®¹:")
                    print(f"{'-'*80}")
                    for i, msg in enumerate(current_messages):
                        role = msg.get('role', 'unknown')
                        content = msg.get('content', '')
                        print(f"\n[æ¶ˆæ¯ {i+1}] è§’è‰²: {role}")
                        print(f"[æ¶ˆæ¯ {i+1}] å†…å®¹: {content}")
                    print(f"\n{'='*80}\n")

                    # ç›´æ¥ä½¿ç”¨å½“å‰é…ç½®çš„æ¨¡å‹
                    response_text = ""
                    try:
                        async for chunk in llm_client.chat_stream(current_messages):
                            await websocket.send_json({"type": "chunk", "content": chunk})
                            response_text += chunk

                        await websocket.send_json({"type": "done", "full_text": response_text})

                        # ä¿å­˜åˆ°èŠå¤©å†å²
                        if chat_id:
                            messages.append({"role": "assistant", "content": response_text})
                            chat_manager.update_chat_messages(chat_id, messages)

                    except Exception as e:
                        print(f"å•æ¨¡å‹æµå¼å“åº”é”™è¯¯: {e}")
                        await websocket.send_json({"type": "error", "content": f"æµå¼å“åº”é”™è¯¯: {str(e)}"})

                continue

            # data format: { "messages": [...], "chat_id": "...", "is_multi_llm": bool }
            messages = data.get("messages", [])
            chat_id = data.get("chat_id")
            is_multi_llm = data.get("is_multi_llm", False)

            # è·å–åŠ¨æ€ system prompt
            config_data = load_config()
            agent_config = config_data.get("agent_config", {})
            intent_enabled = agent_config.get("intent_recognition_enabled", False)
            
            from intelligent_agent import get_sub_agent_system, normalize_identity_identifier
            system_prompt = get_sub_agent_system(
                agent_config_path=AGENT_ROLE_FILE
            )
            
            # Add system prompt if not present or update existing one
            if not messages or messages[0].get("role") != "system":
                 messages.insert(0, {"role": "system", "content": system_prompt})
            else:
                 messages[0]["content"] = system_prompt

            # Inject Resume if enabled
            inject_resume_to_messages(messages)
            
            # Check if job analysis exists locally
            if not os.path.exists(job_manager.job_analysis_path):
                error_msg = "è¯·å…ˆè®¾ç½®ç›®æ ‡å²—ä½ï¼Œå®Œæˆå²—ä½åˆ†æã€‚åŠ©æ‰‹å¯¹è¯æ¡†å³ä¸Šè§’â†’è®¾ç½®ç›®æ ‡å²—ä½"
                await websocket.send_json({"type": "done", "full_text": error_msg})
                if chat_id:
                    messages.append({"role": "assistant", "content": error_msg})
                    chat_manager.update_chat_messages(chat_id, messages)
                continue

            # Inject Job Analysis Context (Always if available)
            inject_job_analysis_to_messages(messages)

            if is_multi_llm:
                await handle_multi_llm_request(websocket, messages, chat_id)
                continue
            else:
                # --- Single LLM Mode (Original Logic) ---
                response_text = ""
                try:
                    # Check if client is ready
                    if not llm_client.client:
                         await websocket.send_json({"type": "error", "content": "LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥è®¾ç½®ã€‚"})
                         continue

                    # ä¿®å¤ï¼šå¤„ç†å½“å‰é…ç½®çš„ System Prompt å’Œ èº«ä»½æ ‡ç­¾
                    current_messages = [m.copy() for m in messages]
                    config_prompt = (curr_conf.get("system_prompt", "") if curr_conf else "").strip()
                    tags = curr_conf.get("tags", []) if curr_conf else []
                    has_tags = bool(tags)
                    
                    target_system_prompt = None
                    
                    if tags:
                        # 1. ä¼˜å…ˆä½¿ç”¨èº«ä»½æ ‡ç­¾ (å³ä½¿æ˜¯æ— æ•ˆæ ‡ç­¾ï¼Œä¹Ÿä¼˜å…ˆäº config_promptï¼Œå›é€€åˆ°é»˜è®¤)
                        # è¿™é‡Œç®€å•å–ç¬¬ä¸€ä¸ªæ ‡ç­¾ä½œä¸ºè§’è‰²ID
                        first_tag = tags[0]
                        normalized_tag = normalize_identity_identifier(first_tag)
                        target_system_prompt = get_sub_agent_system(
                             agent_config_path=AGENT_ROLE_FILE,
                             role_id=normalized_tag
                        )
                        print(f"[AgentAPI] æ£€æµ‹åˆ°èº«ä»½æ ‡ç­¾: {tags} -> ä½¿ç”¨è§’è‰²: {normalized_tag}")
                    elif config_prompt:
                         # 2. å…¶æ¬¡ä½¿ç”¨é…ç½®å®šä¹‰çš„ system prompt
                         target_system_prompt = config_prompt
                         print(f"[AgentAPI] ä½¿ç”¨é…ç½®å®šä¹‰çš„ System Prompt")

                    # åº”ç”¨ç›®æ ‡ System Prompt (å¦‚æœæœ‰)
                    # å¦‚æœ target_system_prompt ä¸º Noneï¼Œåˆ™ä¿æŒ messages ä¸­çš„é»˜è®¤ Prompt (å·²åœ¨ loop å¼€å§‹æ—¶æ’å…¥)
                    if target_system_prompt:
                        sys_idx = next((i for i, m in enumerate(current_messages) if m["role"] == "system"), -1)
                        if sys_idx != -1:
                            current_messages[sys_idx]["content"] = target_system_prompt
                        else:
                            current_messages.insert(0, {"role": "system", "content": target_system_prompt})

                    # Ensure Job Analysis and Resume Context is present (in case it was overwritten)
                    inject_job_analysis_to_messages(current_messages)
                    inject_resume_to_messages(current_messages)

                    # [è°ƒè¯•] æ˜¾ç¤ºå®é™…å‘é€ç»™æ¨¡å‹çš„å®Œæ•´ prompt
                    print(f"\n{'='*80}")
                    print(f"[è°ƒè¯•] æ­£åœ¨å‘é€è¯·æ±‚åˆ°æ¨¡å‹: {curr_conf.get('model', 'Unknown')} (Stream=True)")
                    print(f"{'='*80}")
                    print(f"[è°ƒè¯•] å½“å‰é…ç½®: {curr_conf.get('name', 'Unknown')}")
                    print(f"[è°ƒè¯•] ä½¿ç”¨ System Prompt: {config_prompt if (config_prompt and not has_tags) else 'å¦'}")
                    if has_tags:
                        print(f"[è°ƒè¯•] èº«ä»½æ ‡ç­¾: {tags} (System Prompt è¢«ç¦ç”¨)")
                    print(f"[è°ƒè¯•] æ¶ˆæ¯æ€»æ•°: {len(current_messages)}")
                    print(f"{'-'*80}")
                    print("[è°ƒè¯•] å®Œæ•´ Prompt å†…å®¹:")
                    print(f"{'-'*80}")
                    for i, msg in enumerate(current_messages):
                        role = msg.get('role', 'unknown')
                        content = msg.get('content', '')
                        print(f"\n[æ¶ˆæ¯ {i+1}] è§’è‰²: {role}")
                        print(f"[æ¶ˆæ¯ {i+1}] å†…å®¹: {content}")
                    print(f"\n{'='*80}\n")

                    async for chunk in llm_client.chat_stream(current_messages):
                        await websocket.send_json({"type": "chunk", "content": chunk})
                        response_text += chunk

                    await websocket.send_json({"type": "done", "full_text": response_text})

                    # Save to chat history if chat_id is provided
                    if chat_id:
                        messages.append({"role": "assistant", "content": response_text})
                        chat_manager.update_chat_messages(chat_id, messages)

                except Exception as e:
                    print(f"LLM æµå¼å“åº”é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                    try:
                        await websocket.send_json({"type": "error", "content": f"æµå¼å“åº”é”™è¯¯: {str(e)}"})
                    except Exception as send_error:
                        print(f"å‘é€é”™è¯¯æ¶ˆæ¯å¤±è´¥: {send_error}")

    except WebSocketDisconnect:
        print("LLM WebSocket è¿æ¥å·²æ–­å¼€")
        llm_manager.disconnect(websocket)
    except Exception as e:
        print(f"LLM WebSocket ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        try:
            llm_manager.disconnect(websocket)
        except Exception as disconnect_error:
            print(f"æ–­å¼€è¿æ¥å¤±è´¥: {disconnect_error}")

if __name__ == "__main__":
    import uvicorn

    # Print startup banner
    print("=" * 60)
    print("ğŸš€ AST å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬ä¸å¤§æ¨¡å‹åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    print(f"[é…ç½®] ASR ç³»ç»Ÿ: {'âœ… å¯ç”¨' if not args.no else 'âŒ ç¦ç”¨ (--no)'}")
    print(f"[é…ç½®] LLM å®¢æˆ·ç«¯: âœ… å¯ç”¨")
    print(f"[é…ç½®] æœåŠ¡åœ°å€: http://{args.host}:{args.port}")
    print("=" * 60)
    print("")

    uvicorn.run(app, host=args.host, port=args.port)
def format_intent_analysis(intent_result: dict) -> str:
    """å°†æ„å›¾è¯†åˆ«ç»“æœæ ¼å¼åŒ–ä¸ºç³»ç»Ÿæ¶ˆæ¯"""
    summary_xml = intent_result.get("summary_xml", "")
    if not summary_xml:
        return "ã€æ„å›¾è¯†åˆ«ã€‘æœªç”Ÿæˆç»“æ„åŒ–ç»“æœ"

    def _extract(tag):
        import re
        match = re.search(rf"<{tag}>([\s\S]*?)</{tag}>", summary_xml, re.IGNORECASE)
        return match.group(1).strip() if match else ""

    summary = _extract("summary")
    question = _extract("true_question")
    steps = re.findall(r"<step>([\s\S]*?)</step>", summary_xml, re.IGNORECASE)
    steps = [s.strip() for s in steps if s.strip()]

    parts = ["ã€Leader Agent æ„å›¾åˆ†æã€‘"]
    if summary:
        parts.append(f"æ„å›¾æ€»ç»“ï¼š{summary}")
    if question:
        parts.append(f"çœŸå®é—®é¢˜ï¼š{question}")
    if steps:
        parts.append("ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š")
        parts.extend(f"- {step}" for step in steps)
    return "\n".join(parts)
    return "\n".join(parts)
