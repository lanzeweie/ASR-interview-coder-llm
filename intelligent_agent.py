"""
æ™ºèƒ½åˆ†æ Agent æ¨¡å—

åŸºäºåº•å±‚å°æ¨¡å‹åˆ¤å®šæ˜¯å¦éœ€è¦è®©AIå¸®åŠ©åˆ†æ
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œäº‘ç«¯ API ä¸¤ç§æ–¹å¼
"""

import json
import asyncio
import re
import time
from typing import List, Dict, Optional, Tuple
from llm_client import LLMClient

# å°è¯•å¯¼å…¥ transformers å’Œ torch
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("[æ™ºèƒ½åˆ†æ] æœªå®‰è£… transformers/torchï¼Œæœ¬åœ°æ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")


class IntelligentAgent:
    """æ™ºèƒ½åˆ†æ Agent"""

    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–æ™ºèƒ½ Agent

        Args:
            config: Agent é…ç½®
                - model_name: å°æ¨¡å‹åç§°
                - model_type: 'local' | 'api'
                - api_key: API å¯†é’¥ï¼ˆäº‘ç«¯æ¨¡å¼ï¼‰
                - base_url: API åœ°å€ï¼ˆäº‘ç«¯æ¨¡å¼ï¼‰
                - model: æ¨¡å‹æ ‡è¯†ï¼ˆäº‘ç«¯æ¨¡å¼ï¼‰
                - threshold: å­—æ•°é˜ˆå€¼ï¼Œé»˜è®¤ 10
                - silence_seconds: é™éŸ³ç§’æ•°ï¼Œé»˜è®¤ 2
        """
        self.config = config
        self.threshold = config.get('threshold', 10)
        self.silence_seconds = config.get('silence_seconds', 2)
        self.client = None
        self.local_model = None
        self.local_tokenizer = None
        self.last_message_time = 0  # æœ€åæ¶ˆæ¯æ—¶é—´
        self.current_speaker = None  # å½“å‰è¯´è¯äºº
        self.accumulated_text = ""  # ç´¯ç§¯æ–‡æœ¬
        self.silence_timer = None  # é™éŸ³è®¡æ—¶å™¨
        self.silence_detection_started = False  # æ˜¯å¦å·²å¯åŠ¨é™éŸ³æ£€æµ‹
        self.last_analysis_time = 0  # ä¸Šæ¬¡åˆ†ææ—¶é—´
        self.force_trigger_threshold = self.threshold * 3  # å¼ºåˆ¶è§¦å‘é˜ˆå€¼ï¼ˆ3å€ï¼‰
        self.generation_params = config.get('generation_params', {})

        model_type = config.get('model_type', 'api')

        # åˆå§‹åŒ–å®¢æˆ·ç«¯æˆ–æœ¬åœ°æ¨¡å‹
        if model_type == 'api':
            self.client = LLMClient(
                api_key=config.get('api_key', ''),
                base_url=config.get('base_url', ''),
                model=config.get('model', '')
            )
        elif model_type == 'local':
            if TRANSFORMERS_AVAILABLE:
                model_name = config.get('model_name', 'Qwen/Qwen2-0.5B-Instruct')
                success = self._load_local_model(model_name)
                if not success:
                    print(f"[æ™ºèƒ½åˆ†æ] æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
                    print(f"  1. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
                    print(f"  2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼ˆéœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰")
                    print(f"  3. ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
            else:
                print("[æ™ºèƒ½åˆ†æ] ç¼ºå°‘ä¾èµ–ï¼Œæ— æ³•åŠ è½½æœ¬åœ°æ¨¡å‹")

        print(f"[æ™ºèƒ½åˆ†æ] Agent å·²åˆå§‹åŒ–ï¼Œé˜ˆå€¼: {self.threshold} å­—ï¼Œé™éŸ³: {self.silence_seconds}ç§’")

    def _load_local_model(self, model_name: str):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        print(f"[æ™ºèƒ½åˆ†æ] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
        print(f"[æ™ºèƒ½åˆ†æ] æ£€æŸ¥ä¾èµ–: TRANSFORMERS_AVAILABLE={TRANSFORMERS_AVAILABLE}")
        try:
            print(f"[æ™ºèƒ½åˆ†æ] æ­¥éª¤1: åŠ è½½tokenizer...")
            self.local_tokenizer = AutoTokenizer.from_pretrained(model_name)
            print(f"[æ™ºèƒ½åˆ†æ] âœ… TokenizeråŠ è½½æˆåŠŸ")

            print(f"[æ™ºèƒ½åˆ†æ] æ­¥éª¤2: åŠ è½½æ¨¡å‹...")
            self.local_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16,
                device_map="auto"
            )
            print(f"[æ™ºèƒ½åˆ†æ] âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            self.local_model.eval()
            print(f"[æ™ºèƒ½åˆ†æ] âœ… æœ¬åœ°æ¨¡å‹å®Œå…¨åŠ è½½æˆåŠŸ: {model_name}")
            print(f"[æ™ºèƒ½åˆ†æ] æ¨¡å‹è®¾å¤‡: {self.local_model.device}")
            return True
        except Exception as e:
            print(f"[æ™ºèƒ½åˆ†æ] âš ï¸âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥")
            print(f"[æ™ºèƒ½åˆ†æ] é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"[æ™ºèƒ½åˆ†æ] é”™è¯¯ä¿¡æ¯: {e}")
            self.local_model = None
            self.local_tokenizer = None
            return False

    def build_analysis_prompt(self, messages: List[Dict], speaker_name: str) -> str:
        """
        æ„å»ºåˆ†æ Prompt
        """
        # ä½¿ç”¨ç´§å‡‘æ ¼å¼åŒ–çš„å¯¹è¯å†…å®¹
        dialogue = self.format_messages_compact(messages)

        print(f"[æ™ºèƒ½åˆ†æ] æ„å»ºPromptï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
        print(f"[æ™ºèƒ½åˆ†æ] æ ¼å¼åŒ–å¯¹è¯é•¿åº¦: {len(dialogue)} å­—ç¬¦")
        print(f"[æ™ºèƒ½åˆ†æ] å¯¹è¯å†…å®¹é¢„è§ˆ: {dialogue[:2000]}...")

        prompt = """
        ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶å·¥ç¨‹å¯¹è¯åˆ†æå™¨ã€‚è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹è§„åˆ™åˆ†ææä¾›çš„å¯¹è¯å†…å®¹ï¼Œå¹¶ä»…è¾“å‡ºä¸€ä¸ªæ ‡å‡† JSON å¯¹è±¡ï¼Œä¸å¾—åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€è§£é‡Šã€æ ¼å¼ç¬¦å·æˆ–æ¢è¡Œã€‚
        **è¾“å…¥ï¼š**
        {dialogue}
        **åˆ¤æ–­è§„åˆ™ï¼š**
        1. {speaker_name} æ˜¯å¯¹è¯ä¸­çš„ä¸»äººå…¬ã€‚
        2. ä»…å½“å¯¹è¯ä¸­æ˜ç¡®æ¶‰åŠ **è½¯ä»¶å¼€å‘ç›¸å…³** çš„ä»¥ä¸‹ä»»ä¸€å†…å®¹æ—¶ï¼Œè¿”å› {{"is": true}}ï¼š
        - ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€åº“çš„ä½¿ç”¨é—®é¢˜ï¼ˆå¦‚ Pythonã€Reactã€TensorFlowï¼‰
        - è°ƒè¯•ã€æŠ¥é”™æ’æŸ¥ã€æ€§èƒ½ä¼˜åŒ–
        - ç³»ç»Ÿæ¶æ„ã€API è®¾è®¡ã€æ•°æ®åº“è®¾è®¡
        - å¼€å‘å·¥å…·é“¾ï¼ˆå¦‚ Gitã€Dockerã€CI/CDï¼‰
        - ç®—æ³•ã€æ•°æ®ç»“æ„ã€ä»£ç å®¡æŸ¥
        - è½¯ä»¶å·¥ç¨‹å®è·µï¼ˆå¦‚æµ‹è¯•ã€éƒ¨ç½²ã€DevOpsï¼‰
        3. ä»¥ä¸‹æƒ…å†µ**ä¸€å¾‹è¿”å› {{"is": false}}**ï¼š
        - éè½¯ä»¶ç±»æŠ€æœ¯è¯é¢˜ï¼ˆå¦‚ç”µè·¯è®¾è®¡ã€ç”Ÿç‰©ä¿¡æ¯å­¦ã€é‡åŒ–é‡‘èâ€”â€”å³ä½¿æœ‰ä»£ç ä¹Ÿä¸ç®—ï¼‰
        - æ—¥å¸¸èŠå¤©ã€é—®å€™ã€æƒ…æ„Ÿè¡¨è¾¾
        - æ³›æ³›è€Œè°ˆçš„ç§‘æŠ€è§‚ç‚¹ï¼ˆå¦‚"AI ä¼šå–ä»£ç¨‹åºå‘˜å—ï¼Ÿ"æ— å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼‰
        - ä»…æåŠ"å†™ä»£ç "ä½†æ— å®è´¨æŠ€æœ¯å†…å®¹
        - ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°éç¼–ç¨‹ä»»åŠ¡ï¼ˆå¦‚"å¸®æˆ‘å†™ä¸ª Excel å…¬å¼"ä¸å±äºè½¯ä»¶å¼€å‘ï¼‰

        **è¾“å‡ºè¦æ±‚ï¼š**
        - ä¸¥æ ¼è¾“å‡ºï¼š{{"is": true}} æˆ– {{"is": false}}
        - å¿…é¡»æ˜¯åˆæ³• JSONï¼Œä¸åŒ…è£¹åœ¨ Markdownã€åå¼•å·æˆ–ä»£ç å—ä¸­

        **ç¤ºä¾‹ï¼š**
        {{"is": true}}
        {{"is": false}}
        """.format(dialogue=dialogue, speaker_name=speaker_name)
        return prompt

    def format_messages_compact(self, messages: List[Dict]) -> str:
        """
        å°†æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºç´§å‡‘çš„XMLæ ¼å¼ï¼Œå¤§å¹…å‡å°‘tokenæ¶ˆè€—

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å« roleã€contentã€speakerã€timestamp

        Returns:
            æ ¼å¼åŒ–çš„XMLå­—ç¬¦ä¸²
        """
        xml_lines = ['<conversation>']

        for msg in messages:
            role = msg.get('role', 'u')  # é»˜è®¤ä¸ºuser
            content = msg.get('content', '').strip()
            speaker = msg.get('speaker', '')

            # æå–è¯´è¯äººå§“åï¼ˆå»æ‰ç½®ä¿¡åº¦ï¼‰
            if ' (' in speaker:
                speaker = speaker.split(' (')[0]
            elif '(' in speaker:
                speaker = speaker.split('(')[0]

            # è·å–æ—¶é—´æˆ³ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            timestamp = msg.get('timestamp', 0)
            if isinstance(timestamp, (int, float)) and timestamp > 0:
                # è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒèŠ‚çœå­—ç¬¦
                relative_time = int(timestamp % 3600)  # åªä¿ç•™å°æ—¶å†…çš„ç§’æ•°
                timestamp_str = f' t="{relative_time}"'
            else:
                timestamp_str = ''

            # ç”Ÿæˆç´§å‡‘çš„XMLæ ‡ç­¾
            # r=role, sp=speaker, t=timestampï¼ˆå¯é€‰ï¼‰
            xml_lines.append(
                f'  <msg r="{role[0]}" sp="{speaker}"{timestamp_str}>{content}</msg>'
            )

        xml_lines.append('</conversation>')

        result = '\n'.join(xml_lines)
        print(f"[æ ¼å¼åŒ–] åŸå§‹æ¶ˆæ¯æ•°: {len(messages)}, æ ¼å¼åŒ–åé•¿åº¦: {len(result)} å­—ç¬¦")

        return result

    def validate_response(self, response: str) -> Tuple[bool, Optional[dict]]:
        """ç®€å•éªŒè¯å“åº”æ ¼å¼ï¼ˆæ¨¡å‹è¿”å›ç¨³å®šï¼‰"""
        try:
            # ä½¿ç”¨æ­£åˆ™åŒ¹é… {"is": true} æ ¼å¼
            match = re.search(r'\{\s*"is"\s*:\s*(true|false)\s*\}', response, re.IGNORECASE)
            if match:
                is_true = match.group(1).lower() == 'true'
                print(f"[æ™ºèƒ½åˆ†æ] ç®€å•åˆ¤å®šç»“æœ: {is_true}")
                return True, {'is': is_true}
        except Exception as e:
            print(f"[æ™ºèƒ½åˆ†æ] å“åº”è§£æå‡ºé”™: {e}")
        return False, None

    def _get_local_generation_kwargs(self) -> Dict:
        defaults = {
            "max_new_tokens": 512,
            "do_sample": False,
        }
        params = {**defaults, **self.generation_params}
        if self.local_tokenizer:
            params.setdefault("pad_token_id", self.local_tokenizer.eos_token_id)
            params.setdefault("eos_token_id", self.local_tokenizer.eos_token_id)
        return params

    async def analyze(self, messages: List[Dict], speaker_name: str) -> Dict:
        """
        åˆ†æå¯¹è¯å¹¶åˆ¤å®šæ˜¯å¦éœ€è¦Aiè¾…åŠ©
        """
        try:
            # æ„å»º Prompt
            prompt = self.build_analysis_prompt(messages, speaker_name)

            print(f"[æ™ºèƒ½åˆ†æ] å¼€å§‹åˆ†æï¼Œä¸»äººå…¬: {speaker_name}ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")

            # è°ƒç”¨å°æ¨¡å‹
            model_type = self.config.get('model_type', 'api')
            if model_type == 'api':
                if not self.client:
                    print("[æ™ºèƒ½åˆ†æ] âŒ APIæ¨¡å¼ï¼Œä½†å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                    return {
                        'is': False,
                        'confidence': 0.0,
                        'reason': f'APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ– (model_type={model_type})',
                        'raw_response': ''
                    }
                # API æ¨¡å¼
                print(f"[æ™ºèƒ½åˆ†æ] ğŸ“¡ ä½¿ç”¨APIæ¨¡å¼è°ƒç”¨ï¼Œæ¨¡å‹: {self.config.get('model', 'unknown')}")
                response_text = ""
                async for chunk in self.client.chat_stream([
                    {"role": "user", "content": prompt}
                ]):
                    response_text += chunk

                print(f"[æ™ºèƒ½åˆ†æ] APIæ¨¡å‹å®Œæ•´å“åº”å†…å®¹:")
                print("=" * 80)
                print(response_text)
                print("=" * 80)
                print(f"[æ™ºèƒ½åˆ†æ] å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                is_valid, result = self.validate_response(response_text)

                if is_valid and result:
                    is_needed = result['is']
                    reason = "æ£€æµ‹åˆ°éœ€è¦AIå¸®åŠ©åˆ†æ" if is_needed else "æ™®é€šå¯¹è¯ï¼Œæ— éœ€ AI ä»‹å…¥"
                    print(f"[æ™ºèƒ½åˆ†æ] åˆ¤å®šç»“æœ: {is_needed}")
                    return {
                        'is': is_needed,
                        'confidence': 0.95,
                        'reason': reason,
                        'raw_response': response_text
                    }
                else:
                    print(f"[æ™ºèƒ½åˆ†æ] âŒ APIå“åº”æ— æ•ˆ")
                    return {
                        'is': False,
                        'confidence': 0.0,
                        'reason': 'APIå“åº”æ— æ•ˆ',
                        'raw_response': response_text
                    }

            elif model_type == 'local':
                if not self.local_model:
                    print("[æ™ºèƒ½åˆ†æ] âŒ æœ¬åœ°æ¨¡å¼ï¼Œä½†æ¨¡å‹æœªåŠ è½½")
                    return {
                        'is': False,
                        'confidence': 0.0,
                        'reason': f'æœ¬åœ°æ¨¡å‹æœªåŠ è½½ (model_type={model_type})',
                        'raw_response': ''
                    }
                # æœ¬åœ°æ¨¡å¼
                try:
                    messages = [
                        {
                            "role": "system", 
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ ¼å¼åŒ–è¾“å‡ºå·¥å…·ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯æ¥æ”¶å¯¹è¯åˆ†ææŒ‡ä»¤å¹¶è¾“å‡ºJSONã€‚ä¸¥ç¦è¾“å‡ºä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚"
                        },
                        {
                            "role": "user", 
                            "content": prompt 
                        }
                    ]
                    text = self.local_tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    inputs = self.local_tokenizer([text], return_tensors="pt").to(self.local_model.device)
                    # 4. ç”Ÿæˆå“åº”
                    with torch.no_grad():
                        outputs = self.local_model.generate(
                            inputs.input_ids,
                            attention_mask=inputs.attention_mask, # æ˜¾å¼ä¼ å…¥ maskï¼Œæ¶ˆé™¤è­¦å‘Š
                            **self._get_local_generation_kwargs()
                        )
                    response_text = self.local_tokenizer.decode(
                        outputs[0][inputs.input_ids.shape[1]:],
                        skip_special_tokens=True
                    ).strip()


                    print(f"[æ™ºèƒ½åˆ†æ] æœ¬åœ°æ¨¡å‹å®Œæ•´å“åº”å†…å®¹:")
                    print("=" * 80)
                    print(response_text)
                    print("=" * 80)
                    print(f"[æ™ºèƒ½åˆ†æ] å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                    
                    # å°è¯•æ¸…ç†å¯èƒ½æ®‹ç•™çš„ Markdown æ ‡è®° (0.5B æœ‰æ—¶ä¼šé¡½å›ºåœ°è¾“å‡º ```json)
                    clean_text = response_text.replace("```json", "").replace("```", "").strip()
                    
                    is_valid, result = self.validate_response(clean_text)

                    if is_valid and result:
                        is_needed = result['is']
                        reason = "æ£€æµ‹åˆ°éœ€è¦AIå¸®åŠ©åˆ†æ" if is_needed else "æ™®é€šå¯¹è¯ï¼Œæ— éœ€ AI ä»‹å…¥"
                        print(f"[æ™ºèƒ½åˆ†æ] æœ¬åœ°æ¨¡å‹åˆ¤å®šç»“æœ: {is_needed}")
                        return {
                            'is': is_needed,
                            'confidence': 0.95,
                            'reason': reason,
                            'raw_response': response_text
                        }
                except Exception as e:
                    print(f"[æ™ºèƒ½åˆ†æ] æœ¬åœ°æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    return {
                        'is': False,
                        'confidence': 0.0,
                        'reason': f'æœ¬åœ°æ¨¡å‹æ¨ç†å¤±è´¥: {str(e)}',
                        'raw_response': ''
                    }

            # å¦‚æœæ²¡æœ‰å®¢æˆ·ç«¯æˆ–éªŒè¯å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
            print(f"[æ™ºèƒ½åˆ†æ] âŒ æœªæ»¡è¶³ä»»ä½•æ¡ä»¶:")
            print(f"  - model_type: {model_type}")
            print(f"  - APIæ¡ä»¶: model_type=='api' and self.client: {model_type == 'api' and self.client is not None}")
            print(f"  - æœ¬åœ°æ¡ä»¶: model_type=='local' and self.local_model: {model_type == 'local' and self.local_model is not None}")
            print(f"[æ™ºèƒ½åˆ†æ] è¿”å›é»˜è®¤ç»“æœ")
            return {
                'is': False,
                'confidence': 0.0,
                'reason': f'æ¨¡å‹æœªæ­£ç¡®é…ç½® (model_type={model_type}, client={self.client}, local_model={self.local_model})',
                'raw_response': ''
            }

        except Exception as e:
            print(f"[æ™ºèƒ½åˆ†æ] åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            return {
                'is': False,
                'confidence': 0.0,
                'reason': f'åˆ†æå¤±è´¥: {str(e)}',
                'raw_response': ''
            }

    def process_message(self, message: Dict, conversation_history: List[Dict]) -> Tuple[bool, Optional[str]]:
        """
        æŒ‰ç…§æµç¨‹å›¾å¤„ç†ASRæ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘æ™ºèƒ½åˆ†æ

        Args:
            message: å½“å‰æ¶ˆæ¯ï¼ŒåŒ…å«textå’Œspeakerä¿¡æ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            (æ˜¯å¦åº”è¯¥è§¦å‘, è§¦å‘åŸå› )
        """
        # Step 1: æ£€æŸ¥é•¿åº¦æ˜¯å¦â‰¥3å­—ç¬¦
        text = message.get('text', '').strip()
        if len(text) < 3:
            print(f"[æ™ºèƒ½åˆ†æ] æ¶ˆæ¯é•¿åº¦ä¸è¶³3å­—ç¬¦ï¼Œå¿½ç•¥: {len(text)}å­—ç¬¦")
            return False, None

        # Step 2: æ›´æ–°æœ€åæ¶ˆæ¯æ—¶é—´
        current_time = time.time()
        self.last_message_time = current_time

        # Step 3: æå–è¯´è¯äººä¿¡æ¯
        speaker_info = message.get('speaker', '')
        # ä»speakerä¸­æå–å§“åï¼ˆæ ¼å¼å¦‚"å¼ ä¸‰ (ç½®ä¿¡åº¦:0.85)"ï¼‰
        speaker_name = speaker_info.split(' (')[0] if '(' in speaker_info else speaker_info

        print(f"[æ™ºèƒ½åˆ†æ] å¤„ç†æ¶ˆæ¯: {speaker_name} - {text[:20]}... (é•¿åº¦: {len(text)})")

        # Step 4 & 5: æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€è¯´è¯äººå¹¶å¤„ç†ç´¯ç§¯
        if self.current_speaker is None:
            # é¦–æ¬¡æ¥æ”¶æ¶ˆæ¯
            self.current_speaker = speaker_name
            self.accumulated_text = text
            print(f"[æ™ºèƒ½åˆ†æ] é¦–æ¬¡æ¥æ”¶æ¶ˆæ¯ï¼Œè¯´è¯äºº: {speaker_name}")
        elif self.current_speaker == speaker_name:
            # åŒä¸€è¯´è¯äººï¼Œç´¯ç§¯æ–‡æœ¬
            self.accumulated_text += text
            print(f"[æ™ºèƒ½åˆ†æ] åŒä¸€è¯´è¯äººç´¯ç§¯ï¼Œç´¯ç§¯é•¿åº¦: {len(self.accumulated_text)}")
        else:
            # ä¸åŒè¯´è¯äººï¼Œé‡ç½®ç´¯ç§¯å¹¶æ›´æ–°è¯´è¯äºº
            print(f"[æ™ºèƒ½åˆ†æ] è¯´è¯äººå˜æ›´: {self.current_speaker} -> {speaker_name}")
            print(f"[æ™ºèƒ½åˆ†æ] é‡ç½®ç´¯ç§¯ (åŸé•¿åº¦: {len(self.accumulated_text)})")
            self.current_speaker = speaker_name
            self.accumulated_text = text

        # Step 6: æ£€æŸ¥ç´¯ç§¯å­—ç¬¦æ˜¯å¦â‰¥æœ€å°å€¼ï¼ˆé»˜è®¤10ï¼‰
        if len(self.accumulated_text) < self.threshold:
            print(f"[æ™ºèƒ½åˆ†æ] ç´¯ç§¯å­—ç¬¦ä¸è¶³: {len(self.accumulated_text)}/{self.threshold}ï¼Œç­‰å¾…æ›´å¤šéŸ³é¢‘")
            return False, None

        # Step 7: è¾¾åˆ°é˜ˆå€¼ï¼Œå¯åŠ¨æˆ–æ£€æŸ¥é™éŸ³æ£€æµ‹
        if not self.silence_detection_started:
            # é¦–æ¬¡è¾¾åˆ°é˜ˆå€¼ï¼Œå¯åŠ¨é™éŸ³æ£€æµ‹
            self.silence_detection_started = True
            # é‡ç½®é™éŸ³è®¡æ—¶å™¨
            if self.silence_timer:
                self.silence_timer.cancel()
            self.silence_timer = asyncio.create_task(self._monitor_silence())
            print(f"[æ™ºèƒ½åˆ†æ] å·²å¯åŠ¨é™éŸ³æ£€æµ‹ï¼Œé™éŸ³é˜ˆå€¼: {self.silence_seconds}ç§’")
            return False, None
        else:
            # å·²åœ¨é™éŸ³æ£€æµ‹ä¸­ï¼Œæ£€æŸ¥æ¡ä»¶
            print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³æ£€æµ‹ä¸­...")
            return self._check_trigger_conditions(text), "æ»¡è¶³è§¦å‘æ¡ä»¶"

    async def _monitor_silence(self):
        """ç›‘å¬é™éŸ³çŠ¶æ€ï¼Œè¶…æ—¶åè‡ªåŠ¨è§¦å‘åˆ†æ"""
        try:
            await asyncio.sleep(self.silence_seconds)
            print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³è¶…æ—¶ï¼Œè§¦å‘åˆ†æ")
            # é‡ç½®é™éŸ³æ£€æµ‹çŠ¶æ€
            self.silence_detection_started = False
            self.silence_timer = None
        except asyncio.CancelledError:
            # é™éŸ³æ£€æµ‹è¢«å–æ¶ˆï¼ˆæ”¶åˆ°æ–°æ¶ˆæ¯ï¼‰
            print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³æ£€æµ‹è¢«å–æ¶ˆ")
            pass

    def _check_trigger_conditions(self, current_text: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ»¡è¶³è§¦å‘æ¡ä»¶ï¼ˆæŒ‰ç…§æµç¨‹å›¾çš„é€»è¾‘ï¼‰

        Args:
            current_text: å½“å‰æ–‡æœ¬

        Returns:
            æ˜¯å¦åº”è¯¥è§¦å‘
        """
        current_time = time.time()
        silence_duration = current_time - self.last_message_time

        print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³æ—¶é•¿: {silence_duration:.2f}ç§’")

        # Step 8: æ£€æŸ¥é™éŸ³æ˜¯å¦â‰¥é˜ˆå€¼ï¼ˆ2ç§’ï¼‰
        if silence_duration >= self.silence_seconds:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶1: é™éŸ³ â‰¥ é˜ˆå€¼ ({silence_duration:.2f}s â‰¥ {self.silence_seconds}s)")
            return True

        # Step 9: æ£€æŸ¥æ–‡æœ¬æ˜¯å¦â‰¥3å€é˜ˆå€¼ï¼ˆå¼ºåˆ¶è§¦å‘ï¼‰
        current_length = len(self.accumulated_text)
        if current_length >= self.force_trigger_threshold:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶2: ç´¯ç§¯æ–‡æœ¬ â‰¥ 3å€é˜ˆå€¼ ({current_length} â‰¥ {self.force_trigger_threshold})")
            return True

        # Step 10: æ£€æŸ¥é™éŸ³æ˜¯å¦â‰¥2å€é˜ˆå€¼
        double_threshold = self.silence_seconds * 2
        if silence_duration >= double_threshold:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶3: é™éŸ³ â‰¥ 2å€é˜ˆå€¼ ({silence_duration:.2f}s â‰¥ {double_threshold}s)")
            return True

        print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶ä¸æ»¡è¶³ï¼Œç»§ç»­ç­‰å¾…")
        return False

    def reset_state(self):
        """é‡ç½®AgentçŠ¶æ€"""
        self.last_message_time = 0
        self.current_speaker = None
        self.accumulated_text = ""
        self.silence_detection_started = False
        if self.silence_timer:
            self.silence_timer.cancel()
            self.silence_timer = None
        self.last_analysis_time = 0
        print(f"[æ™ºèƒ½åˆ†æ] çŠ¶æ€å·²é‡ç½®")


class AgentManager:
    """æ™ºèƒ½ Agent ç®¡ç†å™¨"""

    def __init__(self):
        self.agents: Dict[str, IntelligentAgent] = {}
        self.enabled = False
        self.auto_trigger = True
        print("[æ™ºèƒ½åˆ†æ] Agent ç®¡ç†å™¨å·²åˆå§‹åŒ–")

    def load_agent(self, config: dict, model_config: dict) -> bool:
        """
        åŠ è½½æ™ºèƒ½ Agent

        Args:
            config: Agent é…ç½®
            model_config: æ¨¡å‹é…ç½®ï¼ˆAPI é…ç½®ï¼‰

        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            # å¯¹äºæœ¬åœ°æ¨¡å¼ï¼Œmodel_name åº”è¯¥ä» model_config è·å–ï¼ˆæˆ–è€…ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            # ä¼˜å…ˆçº§ï¼šmodel_config.model_name > config.model_name > é»˜è®¤å€¼
            model_name = model_config.get('model_name', config.get('model_name', 'Qwen/Qwen2-0.5B-Instruct'))

            # åˆå¹¶é…ç½®
            agent_config = {
                'model_type': model_config.get('model_type', 'api'),
                'api_key': model_config.get('api_key', ''),
                'base_url': model_config.get('base_url', ''),
                'model': model_config.get('model', ''),
                'model_name': model_name,  # æ·»åŠ  model_name åˆ° agent_config
                'threshold': config.get('min_characters', 10),
                'silence_seconds': config.get('silence_threshold', 2),
                'generation_params': model_config.get('generation_params', {})
            }

            # åˆ›å»º Agent
            self.agents[model_name] = IntelligentAgent(agent_config)
            self.enabled = config.get('enabled', False)
            self.auto_trigger = config.get('auto_trigger', True)

            print(f"[æ™ºèƒ½åˆ†æ] å·²åŠ è½½ Agent: {model_name}, å¯ç”¨çŠ¶æ€: {self.enabled}")
            return True

        except Exception as e:
            print(f"[æ™ºèƒ½åˆ†æ] åŠ è½½ Agent å¤±è´¥: {e}")
            return False

    async def analyze_conversation(self, messages: List[Dict], speaker_name: str, agent_name: str = None) -> Dict:
        """
        åˆ†æå¯¹è¯

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            speaker_name: ä¸»äººå…¬å§“å
            agent_name: Agent åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ†æç»“æœ
        """
        if not self.enabled:
            return {'is': False, 'reason': 'æ™ºèƒ½åˆ†æå·²å…³é—­'}

        # é€‰æ‹© Agent
        agent = None
        if agent_name and agent_name in self.agents:
            agent = self.agents[agent_name]
        elif self.agents:
            agent = list(self.agents.values())[0]

        if not agent:
            return {'is': False, 'reason': 'æœªé…ç½®æ™ºèƒ½ Agent'}

        return await agent.analyze(messages, speaker_name)

    async def run_intelligent_analysis(self, messages: List[Dict], speaker_name: str, intent_recognition: bool = False) -> Dict:
        """
        è¿è¡Œä¸‰é˜¶æ®µæ™ºèƒ½åˆ†ææµç¨‹

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            speaker_name: ä¸»äººå…¬å§“å
            intent_recognition: æ˜¯å¦å¯ç”¨æ„å›¾è¯†åˆ«

        Returns:
            åŒ…å«åˆ†æç»“æœå’Œåˆ†å‘ä¿¡æ¯çš„å­—å…¸
        """
        print(f"[æ™ºèƒ½åˆ†æ] å¼€å§‹ä¸‰é˜¶æ®µåˆ†æï¼Œå¯ç”¨æ„å›¾è¯†åˆ«: {intent_recognition}")

        # é˜¶æ®µ1ï¼šç°æœ‰åˆ†æï¼ˆä¿æŒä¸å˜ï¼‰
        phase1_result = await self.analyze_conversation(messages, speaker_name)
        print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ1å®Œæˆ: {phase1_result}")

        # æ£€æŸ¥é˜¶æ®µ1æ˜¯å¦æˆåŠŸï¼ˆåˆ†æå¤±è´¥æˆ–æ¨¡å‹æœªé…ç½®ï¼‰
        reason = phase1_result.get('reason', '')
        confidence = phase1_result.get('confidence', 0.0)
        if confidence == 0.0 and ('åˆ†æå¤±è´¥' in reason or 'æœªé…ç½®' in reason or 'æ— æ•ˆ' in reason):
            print(f"[æ™ºèƒ½åˆ†æ] âš ï¸ é˜¶æ®µ1å¤±è´¥ï¼Œè·³è¿‡åç»­é˜¶æ®µï¼ŒåŸå› : {reason}")
            return {
                'phase1': phase1_result,
                'phase2': None,
                'distribution': {'mode': 'default', 'targets': []}
            }

        # ä¿®å¤ï¼šå¦‚æœé˜¶æ®µ1åˆ¤æ–­ä¸éœ€è¦AIå¸®åŠ©ï¼Œç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œåç»­é˜¶æ®µ
        if not phase1_result.get('is', False):
            print(f"[æ™ºèƒ½åˆ†æ] âš ï¸ é˜¶æ®µ1åˆ¤æ–­æ— éœ€AIå¸®åŠ©ï¼Œè·³è¿‡é˜¶æ®µ2å’Œé˜¶æ®µ3ï¼ŒåŸå› : {reason}")
            return {
                'phase1': phase1_result,
                'phase2': None,
                'distribution': {'mode': 'default', 'targets': []}
            }

        # é˜¶æ®µ2ï¼šæ„å›¾è¯†åˆ«ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        intent_result = None
        if intent_recognition:
            print("[æ™ºèƒ½åˆ†æ] è¿›å…¥é˜¶æ®µ2ï¼šæ„å›¾è¯†åˆ«")
            intent_result = await self._recognize_intent(messages, speaker_name)
            print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ2å®Œæˆ: æ„å›¾è¯†åˆ«ç»“æœ")

        # é˜¶æ®µ3ï¼šæœ€ç»ˆåˆ†å‘
        distribution_result = self._prepare_distribution(messages, phase1_result, intent_result)
        print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ3å®Œæˆ: å‡†å¤‡åˆ†å‘åˆ° {distribution_result.get('targets', [])}")

        return {
            'phase1': phase1_result,
            'phase2': intent_result,
            'distribution': distribution_result
        }

    async def _recognize_intent(self, messages: List[Dict], speaker_name: str) -> Dict:
        """
        é˜¶æ®µ2ï¼šæ„å›¾è¯†åˆ«å’Œä¸Šä¸‹æ–‡æå–

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            speaker_name: ä¸»äººå…¬å§“å

        Returns:
            æ„å›¾è¯†åˆ«ç»“æœ
        """
        # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„agentè¿›è¡Œæ ¼å¼åŒ–
        agent = list(self.agents.values())[0]
        dialogue = agent.format_messages_compact(messages)

        intent_prompt = (
            "ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ„å›¾è¯†åˆ«Agentï¼Œè¯·é˜…è¯»ä¸‹æ–¹å¯¹è¯å¹¶æç‚¼æ ¸å¿ƒé—®é¢˜ä¸è®¨è®ºå¤§çº²ã€‚\n\n"
            f"{speaker_name} æ˜¯å¯¹è¯ä¸­çš„ä¸»äººå…¬ã€‚\n\n"
            f"{dialogue}\n\n"
            "è¯·ä»…è¾“å‡ºä»¥ä¸‹XMLç»“æ„ï¼š\n"
            "<analysis>\n"
            "  <core>æ ¸å¿ƒé—®é¢˜</core>\n"
            "  <outline>\n"
            "    <item>è¦ç‚¹1</item>\n"
            "    <item>è¦ç‚¹2</item>\n"
            "  </outline>\n"
            "</analysis>\n"
            "è¦æ±‚ï¼š\n"
            "1. æ ¸å¿ƒé—®é¢˜ç²¾ç‚¼ä¸ºä¸€å¥è¯ã€‚\n"
            "2. å¤§çº²åˆ—å‡º2-5ä¸ªè¦ç‚¹ï¼ŒæŒ‰é‡è¦æ€§æ’åºã€‚\n"
            "3. ç¦æ­¢è¾“å‡ºXMLç»“æ„ä¹‹å¤–çš„ä»»ä½•æ–‡æœ¬ã€‚"
        )

        def _extract_xml(text: str) -> str:
            match = re.search(r'<analysis[\s\S]*?</analysis>', text, re.IGNORECASE)
            return match.group(0).strip() if match else text.strip()

        agent_type = agent.config.get('model_type', 'api')

        if agent_type == 'api' and agent.client:
            response_text = ""
            async for chunk in agent.client.chat_stream([
                {"role": "user", "content": intent_prompt}
            ]):
                response_text += chunk
            xml_content = _extract_xml(response_text)
            return {
                'success': True,
                'summary_xml': xml_content,
                'raw_response': response_text
            }

        elif agent_type == 'local' and agent.local_model and agent.local_tokenizer:
            try:
                chat_messages = [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯æ„å›¾è¯†åˆ«Agentï¼Œåªèƒ½è¾“å‡ºä¸¥æ ¼çš„XMLåˆ†æç»“æœã€‚"
                    },
                    {"role": "user", "content": intent_prompt}
                ]
                text = agent.local_tokenizer.apply_chat_template(
                    chat_messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                inputs = agent.local_tokenizer([text], return_tensors="pt").to(agent.local_model.device)
                with torch.no_grad():
                    outputs = agent.local_model.generate(
                        inputs.input_ids,
                        attention_mask=inputs.attention_mask,
                        **agent._get_local_generation_kwargs()
                    )
                response_text = agent.local_tokenizer.decode(
                    outputs[0][inputs.input_ids.shape[1]:],
                    skip_special_tokens=True
                ).strip()
                xml_content = _extract_xml(response_text)
                return {
                    'success': True,
                    'summary_xml': xml_content,
                    'raw_response': response_text
                }
            except Exception as e:
                print(f"[æ™ºèƒ½åˆ†æ] æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
                return {'success': False, 'error': f'æœ¬åœ°æ„å›¾è¯†åˆ«å¤±è´¥: {str(e)}'}

        return {'success': False, 'error': 'æ— å¯ç”¨çš„Agentè¿›è¡Œæ„å›¾è¯†åˆ«'}

    def _prepare_distribution(self, messages: List[Dict], phase1_result: Dict, intent_result: Dict = None) -> Dict:
        """
        é˜¶æ®µ3ï¼šå‡†å¤‡åˆ†å‘åˆ°æ™ºå›Šå›¢è§’è‰²

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            phase1_result: é˜¶æ®µ1çš„åˆ†æç»“æœ
            intent_result: é˜¶æ®µ2çš„æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            åˆ†å‘é…ç½®
        """
        # ä¿®å¤ï¼šé¦–å…ˆæ£€æŸ¥é˜¶æ®µ1æ˜¯å¦åˆ¤æ–­éœ€è¦AIå¸®åŠ©
        if not phase1_result.get('is', False):
            print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ1åˆ¤æ–­æ— éœ€AIå¸®åŠ©ï¼Œä¸å‡†å¤‡åˆ†å‘é…ç½®")
            return {
                'mode': 'default',
                'targets': [],
                'intent': intent_result
            }

        # åŠ è½½æ™ºå›Šå›¢è§’è‰²é…ç½®
        try:
            import json
            with open("data/agent.json", "r", encoding="utf-8") as f:
                agent_data = json.load(f)
                roles = agent_data.get('think_tank_roles', [])
        except Exception as e:
            print(f"[æ™ºèƒ½åˆ†æ] åŠ è½½è§’è‰²é…ç½®å¤±è´¥: {e}")
            roles = []

        # åŠ è½½å½“å‰é…ç½®
        try:
            with open("api_config.json", "r", encoding="utf-8") as f:
                config_data = json.load(f)
                active_names = config_data.get('multi_llm_active_names', [])
                configs = config_data.get('configs', [])
        except Exception as e:
            print(f"[æ™ºèƒ½åˆ†æ] åŠ è½½APIé…ç½®å¤±è´¥: {e}")
            return {'targets': [], 'mode': 'default'}

        # æ ¹æ®è§’è‰²æ ‡ç­¾åŒ¹é…æ¨¡å‹
        role_targets = {}
        for role in roles:
            role_id = role.get('id')
            tag_key = role.get('tag_key')

            # æŸ¥æ‰¾åŒ¹é…è¯¥è§’è‰²æ ‡ç­¾çš„æ¨¡å‹
            matching_configs = [
                c for c in configs
                if c['name'] in active_names
                and c.get('tags', [])
                and tag_key in c['tags']
            ]

            if matching_configs:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ¨¡å‹
                role_targets[role_id] = matching_configs[0]['name']

        # å¦‚æœæœ‰è§’è‰²åŒ¹é…ï¼Œä½¿ç”¨æ™ºå›Šå›¢æ¨¡å¼
        if role_targets:
            return {
                'mode': 'think_tank',
                'targets': role_targets,
                'intent': intent_result
            }
        else:
            # å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å¼ï¼ˆå•æ¨¡å‹ï¼‰
            return {
                'mode': 'default',
                'targets': active_names[:1] if active_names else [],
                'intent': intent_result
            }

    async def should_analyze(self, message: Dict, conversation_history: List[Dict]) -> Tuple[bool, Optional[str]]:
        """
        å¼‚æ­¥æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æï¼ˆæŒ‰ç…§æµç¨‹å›¾é€»è¾‘ï¼‰

        Args:
            message: å½“å‰æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            (æ˜¯å¦éœ€è¦åˆ†æ, è§¦å‘åŸå› )
        """
        if not self.enabled:
            return False, "æ™ºèƒ½åˆ†æå·²å…³é—­"

        if not self.auto_trigger:
            return False, "è‡ªåŠ¨è§¦å‘å·²å…³é—­"

        # æ£€æŸ¥æ˜¯å¦æœ‰ Agent
        if not self.agents:
            return False, "æœªé…ç½®Agent"

        # é€‰æ‹©ç¬¬ä¸€ä¸ª Agent æ£€æŸ¥è§¦å‘æ¡ä»¶
        agent = list(self.agents.values())[0]
        should_trigger, reason = agent.process_message(message, conversation_history)
        return should_trigger, reason


# å…¨å±€ Agent ç®¡ç†å™¨å®ä¾‹
agent_manager = AgentManager()
