"""
æ™ºèƒ½åˆ†æ Agent æ¨¡å—

åŸºäºåº•å±‚å°æ¨¡å‹åˆ¤å®šæ˜¯å¦éœ€è¦è®©AIå¸®åŠ©åˆ†æ
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œäº‘ç«¯ API ä¸¤ç§æ–¹å¼
æŒ‰èŒè´£æ‹†åˆ†ä¸ºã€æ™ºèƒ½åˆ†æã€‘ã€æ„å›¾è¯†åˆ«ã€‘ã€æ™ºå›Šå›¢ã€‘ä¸‰ç±»Agentï¼Œå¯ç»„åˆä¹Ÿå¯ç‹¬ç«‹å¯ç”¨ã€‚
"""

import asyncio
import copy
import json
import re
import time
from html import escape
from typing import Callable, Dict, List, Optional, Tuple

from llm_client import LLMClient
from data.prompt import PromptTemplate

# å°è¯•å¯¼å…¥ transformers å’Œ torch
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("[æ™ºèƒ½Agent] æœªå®‰è£… transformers/torchï¼Œæœ¬åœ°æ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")

LEGACY_IDENTITY_MAP = {
    "æ€è€ƒ": "tech_assistant",
    "å¿«é€Ÿ": "concise_assistant",
    "å¼•å¯¼": "guide",
    "æŠ€æœ¯è¾…åŠ©è€…": "tech_assistant",
    "ç²¾ç®€è¾…åŠ©è€…": "concise_assistant",
    "èµ„æ·±æ±‚èŒç€": "guide"
}


def normalize_identity_identifier(value: Optional[str]) -> str:
    if not value:
        return ""
    identifier = value.strip()
    if not identifier:
        return ""
    identifier = re.sub(r"\s+", "_", identifier)
    mapped = LEGACY_IDENTITY_MAP.get(identifier)
    if mapped:
        return mapped
    identifier = identifier.lower()
    mapped = LEGACY_IDENTITY_MAP.get(identifier)
    if mapped:
        return mapped
    if identifier.endswith("_tag"):
        identifier = identifier[:-4]
    return identifier


def sanitize_role_definition(role: Optional[Dict]) -> Optional[Dict]:
    if not isinstance(role, dict):
        return None

    normalized_id = normalize_identity_identifier(role.get("id") or role.get("tag_key"))
    if not normalized_id:
        return None

    name = (role.get("name") or "").strip() or normalized_id
    prompt = (role.get("prompt") or "").strip()
    enabled = bool(role.get("enabled", True))

    return {
        "id": normalized_id,
        "name": name,
        "prompt": prompt,
        "enabled": enabled
    }


def get_sub_agent_system(agent_config_path: str = "data/agent.json", use_intent: bool = False, use_resume: bool = False) -> str:
    """
    æ ¹æ®å¼€å…³çŠ¶æ€è·å–å¯¹åº”çš„ sub-agent system prompt
    
    Args:
        agent_config_path: agenté…ç½®æ–‡ä»¶è·¯å¾„
        use_intent: æ˜¯å¦å¯ç”¨æ„å›¾è¯†åˆ«
        use_resume: æ˜¯å¦å¯ç”¨ç®€å†ä¸ªæ€§åŒ–
    
    Returns:
        å¯¹åº”åœºæ™¯çš„system promptå­—ç¬¦ä¸²
    """
    try:
        with open(agent_config_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        sub_agents = data.get('sub_agents', {})
        
        # æ ¹æ®åŠŸèƒ½ç»„åˆé€‰æ‹©å¯¹åº”çš„ sub-agent
        if use_intent and use_resume:
            agent_key = 'full_featured'
        elif use_intent:
            agent_key = 'with_intent'
        elif use_resume:
            agent_key = 'with_resume'
        else:
            agent_key = 'direct_chat'
        
        agent_config = sub_agents.get(agent_key, {})
        system_prompt = agent_config.get('system', '')
        
        if system_prompt:
            print(f"[Sub-Agent] åŠ è½½ç³»ç»Ÿæç¤ºè¯: {agent_config.get('name', agent_key)}")
        else:
            print(f"[Sub-Agent] è­¦å‘Š: æœªæ‰¾åˆ° {agent_key} çš„ç³»ç»Ÿæç¤ºè¯ï¼Œä½¿ç”¨é»˜è®¤")
            system_prompt = "ä½ æ˜¯ä¸€åèµ„æ·± Python æŠ€æœ¯ä¸“å®¶ï¼Œæ­£åœ¨å‚åŠ é«˜çº§å·¥ç¨‹å¸ˆé¢è¯•ã€‚"
        
        return system_prompt
    except Exception as exc:
        print(f"[Sub-Agent] åŠ è½½é…ç½®å¤±è´¥: {exc}")
        return "ä½ æ˜¯ä¸€åèµ„æ·± Python æŠ€æœ¯ä¸“å®¶ï¼Œæ­£åœ¨å‚åŠ é«˜çº§å·¥ç¨‹å¸ˆé¢è¯•ã€‚"


def format_messages_compact(messages: List[Dict]) -> str:
    """å°†æ¶ˆæ¯å‹ç¼©ä¸ºXMLæ ¼å¼ï¼Œå‡å°‘tokenæ¶ˆè€—"""
    xml_lines = ['<conversation>']

    for msg in messages:
        role = msg.get('role', 'u')
        content = msg.get('content', '').strip()
        speaker = msg.get('speaker', '')

        if ' (' in speaker:
            speaker = speaker.split(' (')[0]
        elif '(' in speaker:
            speaker = speaker.split('(')[0]

        timestamp = msg.get('timestamp', 0)
        if isinstance(timestamp, (int, float)) and timestamp > 0:
            relative_time = int(timestamp % 3600)
            timestamp_str = f' t="{relative_time}"'
        else:
            timestamp_str = ''

        xml_lines.append(
            f'  <msg r="{role[0]}" sp="{speaker}"{timestamp_str}>{content}</msg>'
        )

    xml_lines.append('</conversation>')
    result = '\n'.join(xml_lines)
    print(f"[æ ¼å¼åŒ–] åŸå§‹æ¶ˆæ¯æ•°: {len(messages)}, æ ¼å¼åŒ–åé•¿åº¦: {len(result)} å­—ç¬¦")
    return result


class BaseLLMAgent:
    """å°è£…æœ¬åœ°/äº‘ç«¯æ¨¡å‹åŠ è½½ä¸æ¨ç†çš„åŸºç¡€Agent"""

    def __init__(self, agent_label: str, config: dict):
        self.agent_label = agent_label
        self.config = config
        self.model_type = config.get('model_type', 'api')
        self.generation_params = config.get('generation_params', {})
        self.client = None
        self.local_model = None
        self.local_tokenizer = None
        self._init_backend()

    def _init_backend(self):
        if self.model_type == 'api':
            self.client = LLMClient(
                api_key=self.config.get('api_key', ''),
                base_url=self.config.get('base_url', ''),
                model=self.config.get('model', '')
            )
            print(f"[{self.agent_label}] APIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
        elif self.model_type == 'local':
            model_name = self.config.get('model_name', 'Qwen/Qwen2-0.5B-Instruct')
            if TRANSFORMERS_AVAILABLE:
                self._load_local_model(model_name)
            else:
                print(f"[{self.agent_label}] ç¼ºå°‘æœ¬åœ°æ¨ç†ä¾èµ–ï¼Œæ— æ³•åŠ è½½ {model_name}")

    def _load_local_model(self, model_name: str) -> bool:
        print(f"[{self.agent_label}] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
        try:
            self.local_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.local_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16,
                device_map="auto"
            )
            self.local_model.eval()
            print(f"[{self.agent_label}] âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            return True
        except Exception as exc:
            print(f"[{self.agent_label}] âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {exc}")
            self.local_model = None
            self.local_tokenizer = None
            return False

    def _get_local_generation_kwargs(self) -> Dict:
        defaults = {
            "max_new_tokens": 512,
            "do_sample": False,
        }
        params = {**defaults, **self.generation_params}
        if self.local_tokenizer:
            params.setdefault("pad_token_id", self.local_tokenizer.eos_token_id)
            params.setdefault("eos_token_id", self.local_tokenizer.eos_token_id)
        return params

    async def _run_chat(self, messages: List[Dict]) -> str:
        if self.model_type == 'api':
            if not self.client:
                raise RuntimeError(f"APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ– ({self.agent_label})")
            response_text = ""
            async for chunk in self.client.chat_stream(messages):
                response_text += chunk
        elif self.model_type == 'local':
            if not (self.local_model and self.local_tokenizer):
                raise RuntimeError(f"æœ¬åœ°æ¨¡å‹æœªåŠ è½½ ({self.agent_label})")
            
            enable_thinking = self.generation_params.get("enable_thinking", False)
            if self.config.get("enable_thinking"):
                enable_thinking = True
                
            text = self.local_tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=enable_thinking
            )
            inputs = self.local_tokenizer([text], return_tensors="pt").to(self.local_model.device)
            with torch.no_grad():
                outputs = self.local_model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **self._get_local_generation_kwargs()
                )
            response_text = self.local_tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
        else:
            raise RuntimeError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")

        print(f"[{self.agent_label}] æ¨¡å‹å®Œæ•´å“åº”å†…å®¹:")
        print("=" * 80)
        print(response_text)
        print("=" * 80)
        print(f"[{self.agent_label}] å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
        return response_text


class SmartAnalysisAgent(BaseLLMAgent):
    """è´Ÿè´£é˜¶æ®µ1æ™ºèƒ½åˆ†æçš„Agent"""

    def __init__(self, config: dict):
        super().__init__("æ™ºèƒ½åˆ†æ", config)
        self.threshold = config.get('threshold', 10)
        self.silence_seconds = config.get('silence_seconds', 2)
        self.last_message_time = 0
        self.current_speaker = None
        self.accumulated_text = ""
        self.silence_timer = None
        self.silence_detection_started = False
        self.last_analysis_time = 0
        self.force_trigger_threshold = self.threshold * 3
        self._pending_trigger_message = None
        print(f"[æ™ºèƒ½åˆ†æ] Agent åˆå§‹åŒ–ï¼Œé˜ˆå€¼:{self.threshold} å­—ï¼Œé™éŸ³:{self.silence_seconds} ç§’")

    def build_analysis_prompt(self, messages: List[Dict], speaker_name: str) -> str:
        dialogue = format_messages_compact(messages)
        print(f"[æ™ºèƒ½åˆ†æ] æ„å»ºPromptï¼Œæ¶ˆæ¯æ•°: {len(messages)}ï¼Œé•¿åº¦: {len(dialogue)}")
        return PromptTemplate.get_analysis_prompt(dialogue, speaker_name)

    @staticmethod
    def validate_response(response: str) -> Tuple[bool, Optional[dict]]:
        try:
            match = re.search(r'\{\s*"is"\s*:\s*(true|false)\s*\}', response, re.IGNORECASE)
            if match:
                is_true = match.group(1).lower() == 'true'
                print(f"[æ™ºèƒ½åˆ†æ] åˆ¤å®šç»“æœ: {is_true}")
                return True, {'is': is_true}
        except Exception as exc:
            print(f"[æ™ºèƒ½åˆ†æ] å“åº”è§£æå‡ºé”™: {exc}")
        return False, None

    async def analyze(self, messages: List[Dict], speaker_name: str) -> Dict:
        model_name = self.config.get('model_name') or self.config.get('model') or 'æœªçŸ¥æ¨¡å‹'
        try:
            prompt = self.build_analysis_prompt(messages, speaker_name)
            print(f"[æ™ºèƒ½åˆ†æ] å¼€å§‹åˆ†æï¼Œä¸»äººå…¬: {speaker_name}")
            if self.model_type == 'local':
                chat_messages = [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ ¼å¼åŒ–è¾“å‡ºå·¥å…·ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯æ¥æ”¶å¯¹è¯åˆ†ææŒ‡ä»¤å¹¶è¾“å‡ºJSONã€‚ä¸¥ç¦è¾“å‡ºä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ]
            else:
                chat_messages = [{"role": "user", "content": prompt}]
            print(chat_messages)
            response_text = await self._run_chat(chat_messages)
            
            # å»é™¤ <think> æ ‡ç­¾å†…å®¹
            response_text = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)
            
            clean_text = response_text.replace("```json", "").replace("```", "").strip()
            is_valid, result = self.validate_response(clean_text)
            if is_valid and result:
                is_needed = result['is']
                reason = "æ£€æµ‹åˆ°éœ€è¦AIå¸®åŠ©åˆ†æ" if is_needed else "æ™®é€šå¯¹è¯ï¼Œæ— éœ€ AI ä»‹å…¥"
                return {
                    'is': is_needed,
                    'reason': reason,
                    'raw_response': response_text,
                    'model_name': model_name
                }
            return {
                'is': False,
                'reason': 'æ¨¡å‹å“åº”æ— æ•ˆ',
                'raw_response': response_text,
                'model_name': model_name
            }
        except RuntimeError as exc:
            return {
                'is': False,
                'reason': f'åˆ†æå¤±è´¥: {str(exc)}',
                'raw_response': '',
                'model_name': model_name
            }
        except Exception as exc:
            print(f"[æ™ºèƒ½åˆ†æ] åˆ†æè¿‡ç¨‹å‡ºé”™: {exc}")
            return {
                'is': False,
                'reason': f'åˆ†æå¤±è´¥: {str(exc)}',
                'raw_response': '',
                'model_name': model_name
            }

    def process_message(self, message: Dict, conversation_history: List[Dict]) -> Tuple[bool, Optional[str]]:
        text = message.get('text', '').strip()
        if len(text) < 3:
            print(f"[æ™ºèƒ½åˆ†æ] æ¶ˆæ¯é•¿åº¦ä¸è¶³3å­—ç¬¦ï¼Œå¿½ç•¥: {len(text)} å­—ç¬¦")
            return False, None

        current_time = time.time()
        self.last_message_time = current_time

        speaker_info = message.get('speaker', '')
        speaker_name = speaker_info.split(' (')[0] if '(' in speaker_info else speaker_info
        print(f"[æ™ºèƒ½åˆ†æ] å¤„ç†æ¶ˆæ¯: {speaker_name} - {text[:20]}... (é•¿åº¦: {len(text)})")

        if self.current_speaker is None:
            self.current_speaker = speaker_name
            self.accumulated_text = text
        elif self.current_speaker == speaker_name:
            self.accumulated_text += text
            print(f"[æ™ºèƒ½åˆ†æ] åŒä¸€è¯´è¯äººç´¯ç§¯ï¼Œé•¿åº¦: {len(self.accumulated_text)}")
        else:
            print(f"[æ™ºèƒ½åˆ†æ] è¯´è¯äººå˜æ›´: {self.current_speaker} -> {speaker_name}")
            self.current_speaker = speaker_name
            self.accumulated_text = text

        if len(self.accumulated_text) < self.threshold:
            print(f"[æ™ºèƒ½åˆ†æ] ç´¯ç§¯å­—ç¬¦ä¸è¶³: {len(self.accumulated_text)}/{self.threshold}")
            return False, None

        if not self.silence_detection_started:
            self.silence_detection_started = True
            if self.silence_timer:
                self.silence_timer.cancel()
            self.silence_timer = asyncio.create_task(self._monitor_silence())
            print(f"[æ™ºèƒ½åˆ†æ] å·²å¯åŠ¨é™éŸ³æ£€æµ‹ï¼Œé˜ˆå€¼: {self.silence_seconds}ç§’")
            return False, None
        else:
            triggered = self._check_trigger_conditions(text)
            if triggered:
                enriched_message = copy.deepcopy(message)
                accumulated = self.accumulated_text.strip()
                if accumulated:
                    enriched_message['text'] = accumulated
                    if 'content' in enriched_message:
                        enriched_message['content'] = accumulated
                self._pending_trigger_message = enriched_message
                self.accumulated_text = ""
                self.current_speaker = None
                self.silence_detection_started = False
                if self.silence_timer:
                    self.silence_timer.cancel()
                    self.silence_timer = None
            return triggered, "æ»¡è¶³è§¦å‘æ¡ä»¶"

    def prepare_analysis_messages(self, messages: List[Dict]) -> List[Dict]:
        if not self._pending_trigger_message:
            return messages
        
        # ç¡®ä¿pendingæ¶ˆæ¯è¢«åŒ…å«
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ¯”è¾ƒå¼•ç”¨ï¼Œå¦‚æœéœ€è¦åœ¨å†…å®¹ä¸Šåˆ¤é‡å¯èƒ½éœ€è¦è°ƒæ•´
        if self._pending_trigger_message not in messages:
            messages = [*messages, self._pending_trigger_message]
            print(f"[æ™ºèƒ½åˆ†æ] å·²è¿½åŠ è§¦å‘æ¶ˆæ¯: {self._pending_trigger_message.get('text', '')[:20]}...")
        
        self._pending_trigger_message = None
        return messages

    async def _monitor_silence(self):
        try:
            await asyncio.sleep(self.silence_seconds)
            print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³è¶…æ—¶ï¼Œè§¦å‘åˆ†æ")
            self.silence_detection_started = False
            self.silence_timer = None
        except asyncio.CancelledError:
            print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³æ£€æµ‹è¢«å–æ¶ˆ")

    def _check_trigger_conditions(self, current_text: str) -> bool:
        current_time = time.time()
        silence_duration = current_time - self.last_message_time
        print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³æ—¶é•¿: {silence_duration:.2f}ç§’")

        if silence_duration >= self.silence_seconds:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶1æ»¡è¶³ï¼šé™éŸ³ â‰¥ {self.silence_seconds}")
            return True

        current_length = len(self.accumulated_text)
        if current_length >= self.force_trigger_threshold:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶2æ»¡è¶³ï¼šç´¯ç§¯ â‰¥ 3å€é˜ˆå€¼ ({current_length})")
            return True

        double_threshold = self.silence_seconds * 2
        if silence_duration >= double_threshold:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶3æ»¡è¶³ï¼šé™éŸ³ â‰¥ {double_threshold}")
            return True

        print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶ä¸è¶³ï¼Œç»§ç»­ç­‰å¾…")
        return False

    def reset_state(self):
        self.last_message_time = 0
        self.current_speaker = None
        self.accumulated_text = ""
        self.silence_detection_started = False
        if self.silence_timer:
            self.silence_timer.cancel()
            self.silence_timer = None
        self.last_analysis_time = 0
        self._pending_trigger_message = None
        print(f"[æ™ºèƒ½åˆ†æ] çŠ¶æ€å·²é‡ç½®")


class IntentRecognitionAgent(BaseLLMAgent):
    """è´Ÿè´£é˜¶æ®µ2æ„å›¾è¯†åˆ«çš„Agent"""

    def __init__(self, config: dict):
        super().__init__("æ„å›¾è¯†åˆ«", config)

    def build_prompt(self, messages: List[Dict], speaker_name: str) -> str:
        dialogue = format_messages_compact(messages)
        return PromptTemplate.get_intent_prompt(dialogue)
    @staticmethod
    def _extract_xml(text: str) -> str:
        match = re.search(r'<leader_analysis[\s\S]*?</leader_analysis>', text, re.IGNORECASE)
        if match:
            return match.group(0).strip()
        import html
        content = text.strip() or "æœªæ£€æµ‹åˆ°æŠ€æœ¯é—®é¢˜"
        escaped = html.escape(content)
        return (
            "<leader_analysis>"
            f"<summary>{escaped}</summary>"
            "<true_question></true_question>"
            "<steps></steps>"
            "</leader_analysis>"
        )

    async def analyze(self, messages: List[Dict], speaker_name: str) -> Dict:
        prompt = self.build_prompt(messages, speaker_name)
        try:
            print(f"\n[DEBUG_INTENT] ğŸš€ æ­£åœ¨æ‰§è¡Œæ„å›¾è¯†åˆ« prompt...")
            print(f"[DEBUG_INTENT] ä¸»äººå…¬: {speaker_name}")
            print(f"[DEBUG_INTENT] ä¸Šä¸‹æ–‡æ¶ˆæ¯æ•°: {len(messages)}")
            print("[DEBUG_INTENT] å®Œæ•´ Prompt å†…å®¹:")
            print("=" * 80)
            print(prompt)
            print("=" * 80)
            if self.model_type == 'local':
                chat_messages = [
                    {"role": "system", "content": "ä½ æ˜¯æ„å›¾è¯†åˆ«Agentï¼Œåªèƒ½è¾“å‡ºä¸¥æ ¼çš„XMLåˆ†æç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ]
            else:
                chat_messages = [{"role": "user", "content": prompt}]

            response_text = await self._run_chat(chat_messages)
            
            # å»é™¤ <think> æ ‡ç­¾å†…å®¹
            response_text = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)
            
            xml_content = self._extract_xml(response_text)
            print("[æ„å›¾è¯†åˆ«] XMLç»“æœ: ")
            print(xml_content)
            return {
                'success': True,
                'summary_xml': xml_content,
                'raw_response': response_text,
                'model_name': (
                    self.config.get('model_name') 
                    or self.config.get('model') 
                    or "Wiki_QA"
                )
            }
        except RuntimeError as exc:
            return {'success': False, 'error': str(exc)}
        except Exception as exc:
            print(f"[æ„å›¾è¯†åˆ«] åˆ†æå¤±è´¥: {exc}")
            return {'success': False, 'error': str(exc)}

def format_intent_analysis(intent_result: Optional[Dict]) -> str:
    if not intent_result:
        return ""
    if intent_result.get("success") and intent_result.get("summary_xml"):
        return intent_result["summary_xml"]
    error = intent_result.get("error")
    if error:
        safe_error = escape(str(error))
        return (
            "<leader_analysis>"
            f"<summary>{safe_error}</summary>"
            "<true_question></true_question>"
            "<steps></steps>"
            "</leader_analysis>"
        )
    return ""


class ThinkTankAgent:
    """è´Ÿè´£é˜¶æ®µ3æ™ºå›Šå›¢åˆ†å‘é€»è¾‘çš„Agent"""

    def __init__(self, agent_config_path: str = "api_config.json", role_config_path: str = "data/agent.json"):
        self.agent_config_path = agent_config_path
        self.role_config_path = role_config_path

    def _safe_load_json(self, path: str) -> dict:
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as exc:
            print(f"[æ™ºå›Šå›¢] åŠ è½½ {path} å¤±è´¥: {exc}")
            return {}

    def get_system_prompt(self, use_intent: bool = False, use_resume: bool = False) -> str:
        """
        è·å–å½“å‰åœºæ™¯å¯¹åº”çš„system prompt
        
        Args:
            use_intent: æ˜¯å¦å¯ç”¨æ„å›¾è¯†åˆ«
            use_resume: æ˜¯å¦å¯ç”¨ç®€å†ä¸ªæ€§åŒ–
        
        Returns:
            å¯¹åº”åœºæ™¯çš„system prompt
        """
        return get_sub_agent_system(
            agent_config_path=self.role_config_path,
            use_intent=use_intent,
            use_resume=use_resume
        )

    def prepare_distribution(
        self,
        messages: List[Dict],
        phase1_result: Optional[Dict],
        intent_result: Optional[Dict] = None,
        force: bool = False,
        use_intent: bool = False,
        use_resume: bool = False
    ) -> Dict:
        phase1_is_positive = bool(phase1_result and phase1_result.get('is'))
        if not force and not phase1_is_positive:
            print(f"[æ™ºå›Šå›¢] é˜¶æ®µ1åˆ¤å®šæ— éœ€AIï¼Œç›´æ¥è¿”å›é»˜è®¤æ¨¡å¼")
            return {
                'mode': 'default',
                'targets': [],
                'intent': intent_result,
                'system_prompt': self.get_system_prompt(use_intent, use_resume)
            }

        role_data = self._safe_load_json(self.role_config_path)
        raw_roles = role_data.get('think_tank_roles', [])
        roles = []
        for role in raw_roles:
            sanitized = sanitize_role_definition(role)
            if sanitized:
                roles.append(sanitized)

        config_data = self._safe_load_json(self.agent_config_path)
        active_names = set(config_data.get('multi_llm_active_names', []))
        configs = config_data.get('configs', [])
        config_tag_map = {
            c['name']: [
                normalize_identity_identifier(tag)
                for tag in c.get('tags', []) if tag
            ]
            for c in configs
        }

        role_targets = {}
        for role in roles:
            if not role.get("enabled", True):
                continue
            role_id = role.get('id')
            matching_configs = [
                name for name, tags in config_tag_map.items()
                if name in active_names and role_id in tags
            ]
            if matching_configs:
                role_targets[role_id] = matching_configs[0]

        # è·å–å½“å‰åœºæ™¯å¯¹åº”çš„system prompt
        system_prompt = self.get_system_prompt(use_intent, use_resume)

        if role_targets:
            print(f"[æ™ºå›Šå›¢] åŒ¹é…åˆ° {len(role_targets)} ä¸ªè§’è‰²ç›®æ ‡")
            return {
                'mode': 'think_tank',
                'targets': role_targets,
                'intent': intent_result,
                'system_prompt': system_prompt
            }

        default_targets = list(active_names)[:1]
        print(f"[æ™ºå›Šå›¢] æœªåŒ¹é…åˆ°è§’è‰²ï¼Œä½¿ç”¨é»˜è®¤ç›®æ ‡: {default_targets}")
        return {
            'mode': 'default',
            'targets': default_targets,
            'intent': intent_result,
            'system_prompt': system_prompt
        }


class AgentManager:
    """ç»„åˆä¸‰ä¸ªAgentå¹¶æä¾›ç»Ÿä¸€æ¥å£"""

    def __init__(self):
        self.agents: Dict[str, SmartAnalysisAgent] = {}
        self.intent_agent: Optional[IntentRecognitionAgent] = None
        self.think_tank_agent = ThinkTankAgent()
        self.enabled = False
        self.auto_trigger = True
        print("[æ™ºèƒ½åˆ†æ] Agent ç®¡ç†å™¨å·²åˆå§‹åŒ–")

    def _build_llm_runtime_config(self, overrides: dict, model_config: Optional[dict], fallback_model_name: str) -> dict:
        runtime = {
            'model_type': overrides.get('model_type') or (model_config.get('model_type') if model_config else 'api'),
            'model_name': overrides.get('model_name') or (model_config.get('model_name') if model_config else fallback_model_name),
            'api_key': overrides.get('api_key', ''),
            'base_url': overrides.get('base_url', ''),
            'model': overrides.get('model', ''),
            'generation_params': overrides.get('generation_params', {})
        }
        
        # å°† enable_thinking æ”¾å…¥ generation_params
        if overrides.get('enable_thinking'):
             runtime['generation_params']['enable_thinking'] = True
        elif model_config and model_config.get('enable_thinking'):
             runtime['generation_params']['enable_thinking'] = True
             
        if runtime['model_type'] == 'api' and model_config:
            runtime['api_key'] = model_config.get('api_key', runtime['api_key'])
            runtime['base_url'] = model_config.get('base_url', runtime['base_url'])
            runtime['model'] = model_config.get('model', runtime['model'])
            runtime['generation_params'] = model_config.get('generation_params', runtime['generation_params'])
        return runtime

    def load_agent(self, config: dict, model_config: dict) -> bool:
        try:
            model_name = model_config.get('model_name', config.get('model_name', 'Qwen/Qwen2-0.5B-Instruct'))
            overrides = {
                'model_type': model_config.get('model_type', config.get('model_type', 'api')),
                'model_name': model_name,
                'enable_thinking': config.get('enable_thinking', False)
            }
            agent_config = self._build_llm_runtime_config(overrides, model_config, model_name)
            agent_config.update({
                'threshold': config.get('min_characters', 10),
                'silence_seconds': config.get('silence_threshold', 2)
            })
            agent = SmartAnalysisAgent(agent_config)
            self.agents[model_name] = agent
            self.enabled = config.get('enabled', False)
            self.auto_trigger = config.get('auto_trigger', True)
            print(f"[æ™ºèƒ½åˆ†æ] å·²åŠ è½½ Agent: {model_name}, å¯ç”¨: {self.enabled}")
            return True
        except Exception as exc:
            print(f"[æ™ºèƒ½åˆ†æ] åŠ è½½ Agent å¤±è´¥: {exc}")
            return False

    def configure_intent_agent(self, config: dict, model_config: Optional[dict]) -> bool:
        try:
            fallback = config.get('model_name', 'Qwen3-0.6B')
            overrides = {
                'model_type': config.get('model_type', 'local'),
                'model_name': fallback,
                'enable_thinking': config.get('intent_enable_thinking', False)
            }
            agent_config = self._build_llm_runtime_config(overrides, model_config, fallback)
            self.intent_agent = IntentRecognitionAgent(agent_config)
            print(f"[æ„å›¾è¯†åˆ«] å·²é…ç½®: {agent_config.get('model_name')}")
            return True
        except Exception as exc:
            print(f"[æ„å›¾è¯†åˆ«] é…ç½®å¤±è´¥: {exc}")
            self.intent_agent = None
            return False

    def _get_primary_agent(self) -> Optional[SmartAnalysisAgent]:
        return next(iter(self.agents.values()), None)

    async def analyze_conversation(
        self,
        messages: List[Dict],
        speaker_name: str,
        agent_name: Optional[str] = None,
        bypass_enabled: bool = False
    ) -> Dict:
        if not self.enabled and not bypass_enabled:
            return {'is': False, 'reason': 'æ™ºèƒ½åˆ†æå·²å…³é—­'}

        agent = None
        if agent_name and agent_name in self.agents:
            agent = self.agents[agent_name]
        elif self.agents:
            agent = self._get_primary_agent()

        if not agent:
            return {'is': False, 'reason': 'æœªé…ç½®æ™ºèƒ½ Agent'}

        prepared_messages = agent.prepare_analysis_messages(messages)
        return await agent.analyze(prepared_messages, speaker_name)

    async def run_pipeline(
        self,
        messages: List[Dict],
        speaker_name: str,
        *,
        use_analysis: bool = True,
        use_intent: bool = False,
        use_resume: bool = False,
        use_think_tank: bool = True,
        bypass_enabled: bool = False,
        force_modules: bool = False,
        status_callback: Optional[Callable[[str, Dict], asyncio.Future]] = None
    ) -> Dict:
        print(
            "[æ™ºèƒ½åˆ†æ] run_pipeline -> "
            f"analysis={use_analysis}, intent={use_intent}, resume={use_resume}, "
            f"think_tank={use_think_tank}, bypass={bypass_enabled}, force={force_modules}"
        )
        if use_analysis:
            phase1_result = await self.analyze_conversation(
                messages,
                speaker_name,
                bypass_enabled=bypass_enabled
            )
        else:
            phase1_result = {
                'is': False,
                'reason': 'æ™ºèƒ½åˆ†ææ¨¡å—æœªå¯ç”¨',
                'confidence': 0.0,
                'raw_response': '',
                'skipped': True
            }

        phase1_success = bool(phase1_result.get('is')) if isinstance(phase1_result, dict) else False
        should_halt = use_analysis and not force_modules and not phase1_success

        intent_result = None
        if use_intent and not should_halt:
            print("[æ„å›¾è¯†åˆ«] æ¨¡å—å¯ç”¨ï¼Œå³å°†è¿è¡Œ IntentRecognitionAgent")
            if status_callback:
                intent_model = (
                    self.intent_agent.config.get('model_name') 
                    or self.intent_agent.config.get('model') 
                    or getattr(self.intent_agent, 'model_name', 'Unknown')
                ) if self.intent_agent else "Unknown"
                if asyncio.iscoroutinefunction(status_callback):
                    await status_callback("intent_started", {"model": intent_model})
                else:
                    status_callback("intent_started", {"model": intent_model})
            
            intent_result = await self.run_intent_recognition(messages, speaker_name)
            
            # æ£€æŸ¥æ„å›¾è¯†åˆ«ç»“æœï¼Œå¦‚æœæœªæ£€æµ‹åˆ°æŠ€æœ¯é—®é¢˜ï¼Œåˆ™ç»ˆæ­¢åç»­æµç¨‹
            if intent_result and intent_result.get('success'):
                summary_xml = intent_result.get('summary_xml', '')
                if 'æœªæ£€æµ‹åˆ°æŠ€æœ¯é—®é¢˜' in summary_xml:
                    print("[æ™ºèƒ½åˆ†æ] æ„å›¾è¯†åˆ«ç»“æœä¸º'æœªæ£€æµ‹åˆ°æŠ€æœ¯é—®é¢˜'ï¼Œç»ˆæ­¢åç»­æµç¨‹")
                    should_halt = True

        distribution_result = None
        if use_think_tank:
            if should_halt:
                distribution_result = {
                    'mode': 'halt', # ä½¿ç”¨ 'halt' æ¨¡å¼æ˜ç¡®è¡¨ç¤ºåœæ­¢
                    'targets': [],
                    'intent': intent_result,
                    'system_prompt': '', # åœæ­¢æ—¶ä¸ä¸éœ€è¦ system prompt
                    'reason': 'Process halted by analysis/intent result'
                }
            else:
                distribution_result = self.think_tank_agent.prepare_distribution(
                    messages,
                    phase1_result,
                    intent_result,
                    force=force_modules or not use_analysis,
                    use_intent=use_intent,
                    use_resume=use_resume
                )
        else:
            distribution_result = {
                'mode': 'skipped',
                'targets': [],
                'intent': intent_result,
                'system_prompt': self.think_tank_agent.get_system_prompt(use_intent, use_resume)
            }

        return {
            'phase1': phase1_result,
            'phase2': intent_result,
            'distribution': distribution_result
        }

    async def run_intelligent_analysis(
        self,
        messages: List[Dict],
        speaker_name: str,
        intent_recognition: bool = False,
        resume_personalization: bool = False,
        status_callback: Optional[Callable[[str, Dict], asyncio.Future]] = None
    ) -> Dict:
        print(f"[æ™ºèƒ½åˆ†æ] å¼€å§‹ä¸‰é˜¶æ®µåˆ†æï¼Œæ„å›¾è¯†åˆ«: {intent_recognition}, ç®€å†ä¸ªæ€§åŒ–: {resume_personalization}")
        result = await self.run_pipeline(
            messages,
            speaker_name,
            use_analysis=True,
            use_intent=intent_recognition,
            use_resume=resume_personalization,
            use_think_tank=True,
            bypass_enabled=True,
            force_modules=False,
            status_callback=status_callback
        )
        phase2_result = result.get('phase2')
        if phase2_result is not None:
            print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ2å®Œæˆ: {phase2_result.get('success', False)}")
        distribution_result = result.get('distribution', {})
        print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ3å®Œæˆ: mode={distribution_result.get('mode')}")
        print(f"[æ™ºèƒ½åˆ†æ] ä½¿ç”¨çš„ç³»ç»Ÿæç¤ºè¯æ¨¡å¼: {distribution_result.get('system_prompt', 'æœªè®¾ç½®')[:50]}...")
        return result

    async def run_intent_recognition(self, messages: List[Dict], speaker_name: str) -> Dict:
        agent = self.intent_agent
        if not agent:
            primary = self._get_primary_agent()
            if primary:
                print("[æ„å›¾è¯†åˆ«] æœªå•ç‹¬é…ç½®ï¼Œå¤ç”¨ä¸»Agentæ¨¡å‹å‚æ•°")
                agent_config = dict(primary.config)
                agent = IntentRecognitionAgent(agent_config)
            else:
                return {'success': False, 'error': 'æ— å¯ç”¨çš„æ„å›¾è¯†åˆ«æ¨¡å‹'}

        return await agent.analyze(messages, speaker_name)

    async def should_analyze(self, message: Dict, conversation_history: List[Dict]) -> Tuple[bool, Optional[str]]:
        if not self.enabled:
            return False, "æ™ºèƒ½åˆ†æå·²å…³é—­"
        if not self.auto_trigger:
            return False, "è‡ªåŠ¨è§¦å‘å·²å…³é—­"
        if not self.agents:
            return False, "æœªé…ç½®Agent"
        agent = self._get_primary_agent()
        return agent.process_message(message, conversation_history)


# å…¨å±€Agentç®¡ç†å™¨å®ä¾‹
agent_manager = AgentManager()
