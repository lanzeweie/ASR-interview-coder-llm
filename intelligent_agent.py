"""
æ™ºèƒ½åˆ†æ Agent æ¨¡å—

åŸºäºåº•å±‚å°æ¨¡å‹åˆ¤å®šæ˜¯å¦éœ€è¦è®©AIå¸®åŠ©åˆ†æ
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œäº‘ç«¯ API ä¸¤ç§æ–¹å¼
æŒ‰èŒè´£æ‹†åˆ†ä¸ºã€æ™ºèƒ½åˆ†æã€‘ã€æ„å›¾è¯†åˆ«ã€‘ã€æ™ºå›Šå›¢ã€‘ä¸‰ç±»Agentï¼Œå¯ç»„åˆä¹Ÿå¯ç‹¬ç«‹å¯ç”¨ã€‚
"""

import asyncio
import copy
import json
import re
import time
from html import escape
from typing import Dict, List, Optional, Tuple

from llm_client import LLMClient

# å°è¯•å¯¼å…¥ transformers å’Œ torch
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("[æ™ºèƒ½Agent] æœªå®‰è£… transformers/torchï¼Œæœ¬åœ°æ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")

LEGACY_IDENTITY_MAP = {
    "æ€è€ƒ": "tech_assistant",
    "å¿«é€Ÿ": "concise_assistant",
    "å¼•å¯¼": "guide",
    "æŠ€æœ¯è¾…åŠ©è€…": "tech_assistant",
    "ç²¾ç®€è¾…åŠ©è€…": "concise_assistant",
    "èµ„æ·±æ±‚èŒç€": "guide"
}


def normalize_identity_identifier(value: Optional[str]) -> str:
    if not value:
        return ""
    identifier = value.strip()
    if not identifier:
        return ""
    identifier = re.sub(r"\s+", "_", identifier)
    mapped = LEGACY_IDENTITY_MAP.get(identifier)
    if mapped:
        return mapped
    identifier = identifier.lower()
    mapped = LEGACY_IDENTITY_MAP.get(identifier)
    if mapped:
        return mapped
    if identifier.endswith("_tag"):
        identifier = identifier[:-4]
    return identifier


def sanitize_role_definition(role: Optional[Dict]) -> Optional[Dict]:
    if not isinstance(role, dict):
        return None

    normalized_id = normalize_identity_identifier(role.get("id") or role.get("tag_key"))
    if not normalized_id:
        return None

    name = (role.get("name") or "").strip() or normalized_id
    prompt = (role.get("prompt") or "").strip()
    enabled = bool(role.get("enabled", True))

    return {
        "id": normalized_id,
        "name": name,
        "prompt": prompt,
        "enabled": enabled
    }


def get_sub_agent_system(agent_config_path: str = "data/agent.json", use_intent: bool = False, use_resume: bool = False) -> str:
    """
    æ ¹æ®å¼€å…³çŠ¶æ€è·å–å¯¹åº”çš„ sub-agent system prompt
    
    Args:
        agent_config_path: agenté…ç½®æ–‡ä»¶è·¯å¾„
        use_intent: æ˜¯å¦å¯ç”¨æ„å›¾è¯†åˆ«
        use_resume: æ˜¯å¦å¯ç”¨ç®€å†ä¸ªæ€§åŒ–
    
    Returns:
        å¯¹åº”åœºæ™¯çš„system promptå­—ç¬¦ä¸²
    """
    try:
        with open(agent_config_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        sub_agents = data.get('sub_agents', {})
        
        # æ ¹æ®åŠŸèƒ½ç»„åˆé€‰æ‹©å¯¹åº”çš„ sub-agent
        if use_intent and use_resume:
            agent_key = 'full_featured'
        elif use_intent:
            agent_key = 'with_intent'
        elif use_resume:
            agent_key = 'with_resume'
        else:
            agent_key = 'direct_chat'
        
        agent_config = sub_agents.get(agent_key, {})
        system_prompt = agent_config.get('system', '')
        
        if system_prompt:
            print(f"[Sub-Agent] åŠ è½½ç³»ç»Ÿæç¤ºè¯: {agent_config.get('name', agent_key)}")
        else:
            print(f"[Sub-Agent] è­¦å‘Š: æœªæ‰¾åˆ° {agent_key} çš„ç³»ç»Ÿæç¤ºè¯ï¼Œä½¿ç”¨é»˜è®¤")
            system_prompt = "ä½ æ˜¯ä¸€åèµ„æ·± Python æŠ€æœ¯ä¸“å®¶ï¼Œæ­£åœ¨å‚åŠ é«˜çº§å·¥ç¨‹å¸ˆé¢è¯•ã€‚"
        
        return system_prompt
    except Exception as exc:
        print(f"[Sub-Agent] åŠ è½½é…ç½®å¤±è´¥: {exc}")
        return "ä½ æ˜¯ä¸€åèµ„æ·± Python æŠ€æœ¯ä¸“å®¶ï¼Œæ­£åœ¨å‚åŠ é«˜çº§å·¥ç¨‹å¸ˆé¢è¯•ã€‚"


def format_messages_compact(messages: List[Dict]) -> str:
    """å°†æ¶ˆæ¯å‹ç¼©ä¸ºXMLæ ¼å¼ï¼Œå‡å°‘tokenæ¶ˆè€—"""
    xml_lines = ['<conversation>']

    for msg in messages:
        role = msg.get('role', 'u')
        content = msg.get('content', '').strip()
        speaker = msg.get('speaker', '')

        if ' (' in speaker:
            speaker = speaker.split(' (')[0]
        elif '(' in speaker:
            speaker = speaker.split('(')[0]

        timestamp = msg.get('timestamp', 0)
        if isinstance(timestamp, (int, float)) and timestamp > 0:
            relative_time = int(timestamp % 3600)
            timestamp_str = f' t="{relative_time}"'
        else:
            timestamp_str = ''

        xml_lines.append(
            f'  <msg r="{role[0]}" sp="{speaker}"{timestamp_str}>{content}</msg>'
        )

    xml_lines.append('</conversation>')
    result = '\n'.join(xml_lines)
    print(f"[æ ¼å¼åŒ–] åŸå§‹æ¶ˆæ¯æ•°: {len(messages)}, æ ¼å¼åŒ–åé•¿åº¦: {len(result)} å­—ç¬¦")
    return result


class BaseLLMAgent:
    """å°è£…æœ¬åœ°/äº‘ç«¯æ¨¡å‹åŠ è½½ä¸æ¨ç†çš„åŸºç¡€Agent"""

    def __init__(self, agent_label: str, config: dict):
        self.agent_label = agent_label
        self.config = config
        self.model_type = config.get('model_type', 'api')
        self.generation_params = config.get('generation_params', {})
        self.client = None
        self.local_model = None
        self.local_tokenizer = None
        self._init_backend()

    def _init_backend(self):
        if self.model_type == 'api':
            self.client = LLMClient(
                api_key=self.config.get('api_key', ''),
                base_url=self.config.get('base_url', ''),
                model=self.config.get('model', '')
            )
            print(f"[{self.agent_label}] APIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
        elif self.model_type == 'local':
            model_name = self.config.get('model_name', 'Qwen/Qwen2-0.5B-Instruct')
            if TRANSFORMERS_AVAILABLE:
                self._load_local_model(model_name)
            else:
                print(f"[{self.agent_label}] ç¼ºå°‘æœ¬åœ°æ¨ç†ä¾èµ–ï¼Œæ— æ³•åŠ è½½ {model_name}")

    def _load_local_model(self, model_name: str) -> bool:
        print(f"[{self.agent_label}] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
        try:
            self.local_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.local_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16,
                device_map="auto"
            )
            self.local_model.eval()
            print(f"[{self.agent_label}] âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            return True
        except Exception as exc:
            print(f"[{self.agent_label}] âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {exc}")
            self.local_model = None
            self.local_tokenizer = None
            return False

    def _get_local_generation_kwargs(self) -> Dict:
        defaults = {
            "max_new_tokens": 512,
            "do_sample": False,
        }
        params = {**defaults, **self.generation_params}
        if self.local_tokenizer:
            params.setdefault("pad_token_id", self.local_tokenizer.eos_token_id)
            params.setdefault("eos_token_id", self.local_tokenizer.eos_token_id)
        return params

    async def _run_chat(self, messages: List[Dict]) -> str:
        if self.model_type == 'api':
            if not self.client:
                raise RuntimeError(f"APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ– ({self.agent_label})")
            response_text = ""
            async for chunk in self.client.chat_stream(messages):
                response_text += chunk
        elif self.model_type == 'local':
            if not (self.local_model and self.local_tokenizer):
                raise RuntimeError(f"æœ¬åœ°æ¨¡å‹æœªåŠ è½½ ({self.agent_label})")
            
            enable_thinking = self.generation_params.get("enable_thinking", False)
            if self.config.get("enable_thinking"):
                enable_thinking = True
                
            text = self.local_tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=enable_thinking
            )
            inputs = self.local_tokenizer([text], return_tensors="pt").to(self.local_model.device)
            with torch.no_grad():
                outputs = self.local_model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **self._get_local_generation_kwargs()
                )
            response_text = self.local_tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
        else:
            raise RuntimeError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")

        print(f"[{self.agent_label}] æ¨¡å‹å®Œæ•´å“åº”å†…å®¹:")
        print("=" * 80)
        print(response_text)
        print("=" * 80)
        print(f"[{self.agent_label}] å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
        return response_text


class SmartAnalysisAgent(BaseLLMAgent):
    """è´Ÿè´£é˜¶æ®µ1æ™ºèƒ½åˆ†æçš„Agent"""

    def __init__(self, config: dict):
        super().__init__("æ™ºèƒ½åˆ†æ", config)
        self.threshold = config.get('threshold', 10)
        self.silence_seconds = config.get('silence_seconds', 2)
        self.last_message_time = 0
        self.current_speaker = None
        self.accumulated_text = ""
        self.silence_timer = None
        self.silence_detection_started = False
        self.last_analysis_time = 0
        self.force_trigger_threshold = self.threshold * 3
        self._pending_trigger_message = None
        print(f"[æ™ºèƒ½åˆ†æ] Agent åˆå§‹åŒ–ï¼Œé˜ˆå€¼:{self.threshold} å­—ï¼Œé™éŸ³:{self.silence_seconds} ç§’")

    def build_analysis_prompt(self, messages: List[Dict], speaker_name: str) -> str:
        dialogue = format_messages_compact(messages)
        print(f"[æ™ºèƒ½åˆ†æ] æ„å»ºPromptï¼Œæ¶ˆæ¯æ•°: {len(messages)}ï¼Œé•¿åº¦: {len(dialogue)}")
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶å·¥ç¨‹å¯¹è¯åˆ†æå™¨ï¼Œä¸“é—¨ç”¨äºåˆ¤æ–­è¯­éŸ³è½¬æ–‡å­—åçš„å¯¹è¯ç‰‡æ®µæ˜¯å¦æ˜ç¡®æ¶‰åŠç›¸å…³æŠ€æœ¯å†…å®¹ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™ï¼Œå¹¶ä»…è¾“å‡ºä¸€ä¸ªæ ‡å‡† JSON å¯¹è±¡ï¼Œä¸å¾—åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€è§£é‡Šã€æ ¼å¼ç¬¦å·æˆ–æ¢è¡Œã€‚

        è¾“å…¥ï¼š
        {dialogue}

        å¤„ç†åŸåˆ™ï¼š

        ä¸»äººå…¬ä¸º {speaker_name}ã€‚ä¸å­˜åˆ™åœ¨æ— è§†
        è¯­éŸ³è½¬æ–‡å­—å¯èƒ½å­˜åœ¨æœ¯è¯­æˆªæ–­ã€é”™åˆ«å­—æˆ–ä¸å®Œæ•´è¡¨è¾¾ï¼ˆå¦‚â€œDockâ€ã€â€œè°ƒæ¥å£è¶…æ—¶â€ã€â€œReact çš„ useEâ€ï¼‰ï¼Œä½ å¯åŸºäºä¸Šä¸‹æ–‡åˆç†æ¨æµ‹å…¶æŒ‡ä»£çš„å¸¸è§è½¯ä»¶å·¥ç¨‹æœ¯è¯­ï¼ˆå¦‚ Dockerã€APIã€useEffectï¼‰ã€‚
        ä½†å¿…é¡»é‡‡å–æ¿€è¿›ç­–ç•¥ï¼šåªè¦æœ‰è¯æ®è¡¨æ˜å¯¹è¯æ˜ç¡®æ¶‰åŠå’¨è¯¢/è¯¢é—®/é—®ç­”æŠ€æœ¯ä¸»é¢˜æ—¶ï¼Œæ‰è¿”å› {{"is": true}}ï¼š
        ä¾‹å¦‚ï¼š
        â€¢ ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶æˆ–åº“çš„å…·ä½“ä½¿ç”¨é—®é¢˜ï¼ˆå¦‚ Python è£…åŒ…å¤±è´¥ã€React çŠ¶æ€ç®¡ç†ã€TensorFlow æ¨¡å‹åŠ è½½ï¼‰
        â€¢ è°ƒè¯•ã€é”™è¯¯æ’æŸ¥ã€æ€§èƒ½ç“¶é¢ˆåˆ†æ
        â€¢ ç³»ç»Ÿæ¶æ„è®¾è®¡ã€API è§„èŒƒã€æ•°æ®åº“ schema æˆ–æŸ¥è¯¢ä¼˜åŒ–
        â€¢ å¼€å‘å·¥å…·é“¾æ“ä½œï¼ˆå¦‚ Git åˆ†æ”¯å†²çªã€Docker é•œåƒæ„å»ºã€CI/CD æµæ°´çº¿é…ç½®ï¼‰
        â€¢ ç®—æ³•å®ç°ã€æ•°æ®ç»“æ„é€‰æ‹©ã€ä»£ç å¯è¯»æ€§æˆ–å®¡æŸ¥åé¦ˆ
        â€¢ è½¯ä»¶å·¥ç¨‹å®è·µï¼ˆå¦‚å•å…ƒæµ‹è¯•è¦†ç›–ã€éƒ¨ç½²å›æ»šã€æ—¥å¿—ç›‘æ§ï¼‰
        ä»¥ä¸‹æƒ…å†µä¸€å¾‹è¿”å› {{"is": false}}

        è¯é¢˜å±äºéè½¯ä»¶é¢†åŸŸï¼ˆå¦‚ç¡¬ä»¶ã€ç”Ÿç‰©ã€é‡‘èï¼‰ï¼Œå³ä½¿å«ä»£ç ç‰‡æ®µ
        ä»…æ³›æ³›æåŠâ€œå†™ä»£ç â€â€œæå¼€å‘â€è€Œæ— å…·ä½“æŠ€æœ¯ç»†èŠ‚
        è¡¨è¾¾æ¨¡ç³Šã€ç¼ºä¹å¯è¯†åˆ«æŠ€æœ¯å…³é”®è¯ï¼Œæˆ–æ¨æµ‹ä¾æ®ä¸è¶³
        æ—¥å¸¸å¯’æš„ã€æƒ…ç»ªè¡¨è¾¾ã€éæŠ€æœ¯æ€§è®¡åˆ’è®¨è®º
        è¾“å‡ºè¦æ±‚ï¼š

        ä¸¥æ ¼è¾“å‡ºï¼š{{"is": true}} æˆ– {{"is": false}}
        å¿…é¡»æ˜¯åˆæ³• JSONï¼Œä¸åŒ…è£¹åœ¨ Markdownã€åå¼•å·ã€ä»£ç å—æˆ–ä»»ä½•é¢å¤–å­—ç¬¦ä¸­
        """
        return prompt

    @staticmethod
    def validate_response(response: str) -> Tuple[bool, Optional[dict]]:
        try:
            match = re.search(r'\{\s*"is"\s*:\s*(true|false)\s*\}', response, re.IGNORECASE)
            if match:
                is_true = match.group(1).lower() == 'true'
                print(f"[æ™ºèƒ½åˆ†æ] åˆ¤å®šç»“æœ: {is_true}")
                return True, {'is': is_true}
        except Exception as exc:
            print(f"[æ™ºèƒ½åˆ†æ] å“åº”è§£æå‡ºé”™: {exc}")
        return False, None

    async def analyze(self, messages: List[Dict], speaker_name: str) -> Dict:
        model_name = self.config.get('model_name') or self.config.get('model') or 'æœªçŸ¥æ¨¡å‹'
        try:
            prompt = self.build_analysis_prompt(messages, speaker_name)
            print(f"[æ™ºèƒ½åˆ†æ] å¼€å§‹åˆ†æï¼Œä¸»äººå…¬: {speaker_name}")
            if self.model_type == 'local':
                chat_messages = [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ ¼å¼åŒ–è¾“å‡ºå·¥å…·ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯æ¥æ”¶å¯¹è¯åˆ†ææŒ‡ä»¤å¹¶è¾“å‡ºJSONã€‚ä¸¥ç¦è¾“å‡ºä»»ä½•å…¶ä»–è§£é‡Šæ€§æ–‡å­—ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ]
            else:
                chat_messages = [{"role": "user", "content": prompt}]
            print(chat_messages)
            response_text = await self._run_chat(chat_messages)
            
            # å»é™¤ <think> æ ‡ç­¾å†…å®¹
            response_text = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)
            
            clean_text = response_text.replace("```json", "").replace("```", "").strip()
            is_valid, result = self.validate_response(clean_text)
            if is_valid and result:
                is_needed = result['is']
                reason = "æ£€æµ‹åˆ°éœ€è¦AIå¸®åŠ©åˆ†æ" if is_needed else "æ™®é€šå¯¹è¯ï¼Œæ— éœ€ AI ä»‹å…¥"
                return {
                    'is': is_needed,
                    'reason': reason,
                    'raw_response': response_text,
                    'model_name': model_name
                }
            return {
                'is': False,
                'reason': 'æ¨¡å‹å“åº”æ— æ•ˆ',
                'raw_response': response_text,
                'model_name': model_name
            }
        except RuntimeError as exc:
            return {
                'is': False,
                'reason': f'åˆ†æå¤±è´¥: {str(exc)}',
                'raw_response': '',
                'model_name': model_name
            }
        except Exception as exc:
            print(f"[æ™ºèƒ½åˆ†æ] åˆ†æè¿‡ç¨‹å‡ºé”™: {exc}")
            return {
                'is': False,
                'reason': f'åˆ†æå¤±è´¥: {str(exc)}',
                'raw_response': '',
                'model_name': model_name
            }

    def process_message(self, message: Dict, conversation_history: List[Dict]) -> Tuple[bool, Optional[str]]:
        text = message.get('text', '').strip()
        if len(text) < 3:
            print(f"[æ™ºèƒ½åˆ†æ] æ¶ˆæ¯é•¿åº¦ä¸è¶³3å­—ç¬¦ï¼Œå¿½ç•¥: {len(text)} å­—ç¬¦")
            return False, None

        current_time = time.time()
        self.last_message_time = current_time

        speaker_info = message.get('speaker', '')
        speaker_name = speaker_info.split(' (')[0] if '(' in speaker_info else speaker_info
        print(f"[æ™ºèƒ½åˆ†æ] å¤„ç†æ¶ˆæ¯: {speaker_name} - {text[:20]}... (é•¿åº¦: {len(text)})")

        if self.current_speaker is None:
            self.current_speaker = speaker_name
            self.accumulated_text = text
        elif self.current_speaker == speaker_name:
            self.accumulated_text += text
            print(f"[æ™ºèƒ½åˆ†æ] åŒä¸€è¯´è¯äººç´¯ç§¯ï¼Œé•¿åº¦: {len(self.accumulated_text)}")
        else:
            print(f"[æ™ºèƒ½åˆ†æ] è¯´è¯äººå˜æ›´: {self.current_speaker} -> {speaker_name}")
            self.current_speaker = speaker_name
            self.accumulated_text = text

        if len(self.accumulated_text) < self.threshold:
            print(f"[æ™ºèƒ½åˆ†æ] ç´¯ç§¯å­—ç¬¦ä¸è¶³: {len(self.accumulated_text)}/{self.threshold}")
            return False, None

        if not self.silence_detection_started:
            self.silence_detection_started = True
            if self.silence_timer:
                self.silence_timer.cancel()
            self.silence_timer = asyncio.create_task(self._monitor_silence())
            print(f"[æ™ºèƒ½åˆ†æ] å·²å¯åŠ¨é™éŸ³æ£€æµ‹ï¼Œé˜ˆå€¼: {self.silence_seconds}ç§’")
            return False, None
        else:
            triggered = self._check_trigger_conditions(text)
            if triggered:
                enriched_message = copy.deepcopy(message)
                accumulated = self.accumulated_text.strip()
                if accumulated:
                    enriched_message['text'] = accumulated
                    if 'content' in enriched_message:
                        enriched_message['content'] = accumulated
                self._pending_trigger_message = enriched_message
                self.accumulated_text = ""
                self.current_speaker = None
                self.silence_detection_started = False
                if self.silence_timer:
                    self.silence_timer.cancel()
                    self.silence_timer = None
            return triggered, "æ»¡è¶³è§¦å‘æ¡ä»¶"

    def prepare_analysis_messages(self, messages: List[Dict]) -> List[Dict]:
        if not self._pending_trigger_message:
            return messages
        
        # ç¡®ä¿pendingæ¶ˆæ¯è¢«åŒ…å«
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ¯”è¾ƒå¼•ç”¨ï¼Œå¦‚æœéœ€è¦åœ¨å†…å®¹ä¸Šåˆ¤é‡å¯èƒ½éœ€è¦è°ƒæ•´
        if self._pending_trigger_message not in messages:
            messages = [*messages, self._pending_trigger_message]
            print(f"[æ™ºèƒ½åˆ†æ] å·²è¿½åŠ è§¦å‘æ¶ˆæ¯: {self._pending_trigger_message.get('text', '')[:20]}...")
        
        self._pending_trigger_message = None
        return messages

    async def _monitor_silence(self):
        try:
            await asyncio.sleep(self.silence_seconds)
            print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³è¶…æ—¶ï¼Œè§¦å‘åˆ†æ")
            self.silence_detection_started = False
            self.silence_timer = None
        except asyncio.CancelledError:
            print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³æ£€æµ‹è¢«å–æ¶ˆ")

    def _check_trigger_conditions(self, current_text: str) -> bool:
        current_time = time.time()
        silence_duration = current_time - self.last_message_time
        print(f"[æ™ºèƒ½åˆ†æ] é™éŸ³æ—¶é•¿: {silence_duration:.2f}ç§’")

        if silence_duration >= self.silence_seconds:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶1æ»¡è¶³ï¼šé™éŸ³ â‰¥ {self.silence_seconds}")
            return True

        current_length = len(self.accumulated_text)
        if current_length >= self.force_trigger_threshold:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶2æ»¡è¶³ï¼šç´¯ç§¯ â‰¥ 3å€é˜ˆå€¼ ({current_length})")
            return True

        double_threshold = self.silence_seconds * 2
        if silence_duration >= double_threshold:
            print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶3æ»¡è¶³ï¼šé™éŸ³ â‰¥ {double_threshold}")
            return True

        print(f"[æ™ºèƒ½åˆ†æ] æ¡ä»¶ä¸è¶³ï¼Œç»§ç»­ç­‰å¾…")
        return False

    def reset_state(self):
        self.last_message_time = 0
        self.current_speaker = None
        self.accumulated_text = ""
        self.silence_detection_started = False
        if self.silence_timer:
            self.silence_timer.cancel()
            self.silence_timer = None
        self.last_analysis_time = 0
        self._pending_trigger_message = None
        print(f"[æ™ºèƒ½åˆ†æ] çŠ¶æ€å·²é‡ç½®")


class IntentRecognitionAgent(BaseLLMAgent):
    """è´Ÿè´£é˜¶æ®µ2æ„å›¾è¯†åˆ«çš„Agent"""

    def __init__(self, config: dict):
        super().__init__("æ„å›¾è¯†åˆ«", config)

    def build_prompt(self, messages: List[Dict], speaker_name: str) -> str:
        dialogue = format_messages_compact(messages)
        return f'''
        ä½ æ˜¯ä¸€åé¢è¯•åœºæ™¯ä¸“ç”¨çš„æŠ€æœ¯æ„å›¾åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™å¤„ç†è¾“å…¥ï¼š

        ğŸ”¹ å¤„ç†é€»è¾‘ï¼š
        1. ä»å¯¹è¯æœ«å°¾å‘å‰æ‰«ææ•´ä¸ªå¯¹è¯ï¼Œæå–â€œæœ€åå‡ºç°çš„æŠ€æœ¯é—®é¢˜å¥å­â€ï¼ˆå³ï¼šæ‰¾åˆ°è·ç¦»å¯¹è¯å°¾éƒ¨æœ€è¿‘ã€ä¸”å±äºç¼–ç¨‹/è°ƒè¯•/æ¶æ„/è¿ç»´/å·¥å…·/ç®—æ³•ç­‰é¢†åŸŸçš„æ˜ç¡®æŠ€æœ¯æé—®ï¼›ä¸å†é™åˆ¶äºè¿ç»­æ¶ˆæ¯å—ï¼‰ã€‚
        - è‹¥å¯¹è¯ä¸­åéƒ¨å‡ºç°çš„æ¶ˆæ¯ä¸ºé—²èŠã€ç”Ÿæ´»å†…å®¹ã€æƒ…ç»ªè¡¨è¾¾ç­‰ï¼Œåˆ™ç»§ç»­å‘å‰æ‰«æï¼Œç›´åˆ°æ‰¾åˆ°æœ€è¿‘çš„ä¸€æ¡æŠ€æœ¯ç±»é—®é¢˜ã€‚
        - è‹¥å¯¹è¯å…¨ç¨‹å‡æ— æŠ€æœ¯é—®é¢˜ï¼Œåˆ™è§†ä¸ºâ€œæ— æŠ€æœ¯é—®é¢˜â€ã€‚

        2. åˆ¤æ–­è¯¥æŠ€æœ¯é—®é¢˜æ˜¯å¦å­˜åœ¨ï¼š
        - è‹¥æ‰¾åˆ°æŠ€æœ¯é—®é¢˜ â†’ è¿›å…¥æ­¥éª¤ 4
        - è‹¥æœªæ‰¾åˆ°ä»»ä½•æŠ€æœ¯é—®é¢˜ â†’ è¾“å‡ºå›ºå®šç»“æ„ï¼š
            <leader_analysis><summary>æœªæ£€æµ‹åˆ°æŠ€æœ¯é—®é¢˜</summary><true_question></true_question><steps></steps></leader_analysis>

        3. å¦‚æ— æŠ€æœ¯é—®é¢˜ â†’ æŒ‰ä¸Šæ–¹å›ºå®šç»“æ„è¾“å‡ºï¼ˆä¿æŒä¸å˜ï¼‰

        4. å¦‚æœ‰æŠ€æœ¯é—®é¢˜ â†’ è¾“å‡ºç»“æ„å¦‚ä¸‹ï¼Œä¸”å¿…é¡»æ»¡è¶³ï¼š
        - <summary>ï¼šç”¨15~30å­—ç²¾å‡†æ¦‚æ‹¬æ„å›¾ï¼Œç¦æ­¢è¶…å­—æ•°

        ğŸ”¹ ç¤ºä¾‹ï¼ˆä¾›ä½ ç†è§£é£æ ¼ï¼Œä¸è¦å¤åˆ¶ï¼‰ï¼š
        è¾“å…¥å¯¹è¯ï¼š
        å€™é€‰äººAï¼šè£…é¥°å™¨é‚£ä¸ª@ç¬¦å·æˆ‘è€æ˜¯æä¸æ‡‚ä»€ä¹ˆæ—¶å€™åŠ æ‹¬å·ã€‚
        é¢è¯•å®˜ï¼šä½ æ˜¯è¯´å¸¦å‚æ•°å’Œä¸å¸¦å‚æ•°çš„åŒºåˆ«ï¼Ÿ
        å€™é€‰äººAï¼šå¯¹ï¼Œè¿˜æœ‰å®ƒæ€ä¹ˆå½±å“åŸå‡½æ•°çš„ã€‚

        åº”è¾“å‡ºï¼š
        <leader_analysis>
            <summary>å›°æƒ‘è£…é¥°å™¨è¯­æ³•ä¸å‚æ•°ä¼ é€’æœºåˆ¶</summary>
        </leader_analysis>

        ğŸ”¹ è¾“å‡ºè§„èŒƒï¼š
        - ä»…è¾“å‡º XMLï¼Œå‰åæ— ä»»ä½•å­—ç¬¦ã€ç©ºè¡Œã€æ³¨é‡Š
        - ä¸ä½¿ç”¨ Markdownã€ä¸ç¼–å·ã€ä¸åŠ ç²—
        - æ‰€æœ‰å­—æ®µå¿…é¡»é—­åˆï¼Œå³ä½¿ä¸ºç©º
            
        # ç°åœ¨æ˜¯å®é™…å†…å®¹ï¼š
        ğŸ”¹ è¾“å…¥å¯¹è¯ï¼š
        {dialogue}
        '''
    @staticmethod
    def _extract_xml(text: str) -> str:
        match = re.search(r'<leader_analysis[\s\S]*?</leader_analysis>', text, re.IGNORECASE)
        if match:
            return match.group(0).strip()
        import html
        content = text.strip() or "æœªæ£€æµ‹åˆ°æŠ€æœ¯é—®é¢˜"
        escaped = html.escape(content)
        return (
            "<leader_analysis>"
            f"<summary>{escaped}</summary>"
            "<true_question></true_question>"
            "<steps></steps>"
            "</leader_analysis>"
        )

    async def analyze(self, messages: List[Dict], speaker_name: str) -> Dict:
        prompt = self.build_prompt(messages, speaker_name)
        try:
            print(
                f"[æ„å›¾è¯†åˆ«] å¼€å§‹åˆ†æï¼Œä¸»äººå…¬: {speaker_name}, æ¶ˆæ¯æ•°: {len(messages)}, "
                f"æ¨¡å‹ç±»å‹: {self.model_type}, æ¨¡å‹: {self.config.get('model_name') or self.config.get('model')}"
            )
            print("[æ„å›¾è¯†åˆ«][è°ƒè¯•] å®Œæ•´ Prompt å†…å®¹:")
            print("=" * 80)
            print(prompt)
            print("=" * 80)
            if self.model_type == 'local':
                chat_messages = [
                    {"role": "system", "content": "ä½ æ˜¯æ„å›¾è¯†åˆ«Agentï¼Œåªèƒ½è¾“å‡ºä¸¥æ ¼çš„XMLåˆ†æç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ]
            else:
                chat_messages = [{"role": "user", "content": prompt}]

            response_text = await self._run_chat(chat_messages)
            
            # å»é™¤ <think> æ ‡ç­¾å†…å®¹
            response_text = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)
            
            xml_content = self._extract_xml(response_text)
            print("[æ„å›¾è¯†åˆ«] XMLç»“æœ: ")
            print(xml_content)
            return {
                'success': True,
                'summary_xml': xml_content,
                'raw_response': response_text,
                'model_name': self.config.get('model_name') or self.config.get('model')
            }
        except RuntimeError as exc:
            return {'success': False, 'error': str(exc)}
        except Exception as exc:
            print(f"[æ„å›¾è¯†åˆ«] åˆ†æå¤±è´¥: {exc}")
            return {'success': False, 'error': str(exc)}

def format_intent_analysis(intent_result: Optional[Dict]) -> str:
    if not intent_result:
        return ""
    if intent_result.get("success") and intent_result.get("summary_xml"):
        return intent_result["summary_xml"]
    error = intent_result.get("error")
    if error:
        safe_error = escape(str(error))
        return (
            "<leader_analysis>"
            f"<summary>{safe_error}</summary>"
            "<true_question></true_question>"
            "<steps></steps>"
            "</leader_analysis>"
        )
    return ""


class ThinkTankAgent:
    """è´Ÿè´£é˜¶æ®µ3æ™ºå›Šå›¢åˆ†å‘é€»è¾‘çš„Agent"""

    def __init__(self, agent_config_path: str = "api_config.json", role_config_path: str = "data/agent.json"):
        self.agent_config_path = agent_config_path
        self.role_config_path = role_config_path

    def _safe_load_json(self, path: str) -> dict:
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as exc:
            print(f"[æ™ºå›Šå›¢] åŠ è½½ {path} å¤±è´¥: {exc}")
            return {}

    def get_system_prompt(self, use_intent: bool = False, use_resume: bool = False) -> str:
        """
        è·å–å½“å‰åœºæ™¯å¯¹åº”çš„system prompt
        
        Args:
            use_intent: æ˜¯å¦å¯ç”¨æ„å›¾è¯†åˆ«
            use_resume: æ˜¯å¦å¯ç”¨ç®€å†ä¸ªæ€§åŒ–
        
        Returns:
            å¯¹åº”åœºæ™¯çš„system prompt
        """
        return get_sub_agent_system(
            agent_config_path=self.role_config_path,
            use_intent=use_intent,
            use_resume=use_resume
        )

    def prepare_distribution(
        self,
        messages: List[Dict],
        phase1_result: Optional[Dict],
        intent_result: Optional[Dict] = None,
        force: bool = False,
        use_intent: bool = False,
        use_resume: bool = False
    ) -> Dict:
        phase1_is_positive = bool(phase1_result and phase1_result.get('is'))
        if not force and not phase1_is_positive:
            print(f"[æ™ºå›Šå›¢] é˜¶æ®µ1åˆ¤å®šæ— éœ€AIï¼Œç›´æ¥è¿”å›é»˜è®¤æ¨¡å¼")
            return {
                'mode': 'default',
                'targets': [],
                'intent': intent_result,
                'system_prompt': self.get_system_prompt(use_intent, use_resume)
            }

        role_data = self._safe_load_json(self.role_config_path)
        raw_roles = role_data.get('think_tank_roles', [])
        roles = []
        for role in raw_roles:
            sanitized = sanitize_role_definition(role)
            if sanitized:
                roles.append(sanitized)

        config_data = self._safe_load_json(self.agent_config_path)
        active_names = set(config_data.get('multi_llm_active_names', []))
        configs = config_data.get('configs', [])
        config_tag_map = {
            c['name']: [
                normalize_identity_identifier(tag)
                for tag in c.get('tags', []) if tag
            ]
            for c in configs
        }

        role_targets = {}
        for role in roles:
            if not role.get("enabled", True):
                continue
            role_id = role.get('id')
            matching_configs = [
                name for name, tags in config_tag_map.items()
                if name in active_names and role_id in tags
            ]
            if matching_configs:
                role_targets[role_id] = matching_configs[0]

        # è·å–å½“å‰åœºæ™¯å¯¹åº”çš„system prompt
        system_prompt = self.get_system_prompt(use_intent, use_resume)

        if role_targets:
            print(f"[æ™ºå›Šå›¢] åŒ¹é…åˆ° {len(role_targets)} ä¸ªè§’è‰²ç›®æ ‡")
            return {
                'mode': 'think_tank',
                'targets': role_targets,
                'intent': intent_result,
                'system_prompt': system_prompt
            }

        default_targets = list(active_names)[:1]
        print(f"[æ™ºå›Šå›¢] æœªåŒ¹é…åˆ°è§’è‰²ï¼Œä½¿ç”¨é»˜è®¤ç›®æ ‡: {default_targets}")
        return {
            'mode': 'default',
            'targets': default_targets,
            'intent': intent_result,
            'system_prompt': system_prompt
        }


class AgentManager:
    """ç»„åˆä¸‰ä¸ªAgentå¹¶æä¾›ç»Ÿä¸€æ¥å£"""

    def __init__(self):
        self.agents: Dict[str, SmartAnalysisAgent] = {}
        self.intent_agent: Optional[IntentRecognitionAgent] = None
        self.think_tank_agent = ThinkTankAgent()
        self.enabled = False
        self.auto_trigger = True
        print("[æ™ºèƒ½åˆ†æ] Agent ç®¡ç†å™¨å·²åˆå§‹åŒ–")

    def _build_llm_runtime_config(self, overrides: dict, model_config: Optional[dict], fallback_model_name: str) -> dict:
        runtime = {
            'model_type': overrides.get('model_type') or (model_config.get('model_type') if model_config else 'api'),
            'model_name': overrides.get('model_name') or (model_config.get('model_name') if model_config else fallback_model_name),
            'api_key': overrides.get('api_key', ''),
            'base_url': overrides.get('base_url', ''),
            'model': overrides.get('model', ''),
            'generation_params': overrides.get('generation_params', {})
        }
        
        # å°† enable_thinking æ”¾å…¥ generation_params
        if overrides.get('enable_thinking'):
             runtime['generation_params']['enable_thinking'] = True
        elif model_config and model_config.get('enable_thinking'):
             runtime['generation_params']['enable_thinking'] = True
             
        if runtime['model_type'] == 'api' and model_config:
            runtime['api_key'] = model_config.get('api_key', runtime['api_key'])
            runtime['base_url'] = model_config.get('base_url', runtime['base_url'])
            runtime['model'] = model_config.get('model', runtime['model'])
            runtime['generation_params'] = model_config.get('generation_params', runtime['generation_params'])
        return runtime

    def load_agent(self, config: dict, model_config: dict) -> bool:
        try:
            model_name = model_config.get('model_name', config.get('model_name', 'Qwen/Qwen2-0.5B-Instruct'))
            overrides = {
                'model_type': model_config.get('model_type', config.get('model_type', 'api')),
                'model_name': model_name,
                'enable_thinking': config.get('enable_thinking', False)
            }
            agent_config = self._build_llm_runtime_config(overrides, model_config, model_name)
            agent_config.update({
                'threshold': config.get('min_characters', 10),
                'silence_seconds': config.get('silence_threshold', 2)
            })
            agent = SmartAnalysisAgent(agent_config)
            self.agents[model_name] = agent
            self.enabled = config.get('enabled', False)
            self.auto_trigger = config.get('auto_trigger', True)
            print(f"[æ™ºèƒ½åˆ†æ] å·²åŠ è½½ Agent: {model_name}, å¯ç”¨: {self.enabled}")
            return True
        except Exception as exc:
            print(f"[æ™ºèƒ½åˆ†æ] åŠ è½½ Agent å¤±è´¥: {exc}")
            return False

    def configure_intent_agent(self, config: dict, model_config: Optional[dict]) -> bool:
        try:
            fallback = config.get('model_name', 'Qwen3-0.6B')
            overrides = {
                'model_type': config.get('model_type', 'local'),
                'model_name': fallback,
                'enable_thinking': config.get('intent_enable_thinking', False)
            }
            agent_config = self._build_llm_runtime_config(overrides, model_config, fallback)
            self.intent_agent = IntentRecognitionAgent(agent_config)
            print(f"[æ„å›¾è¯†åˆ«] å·²é…ç½®: {agent_config.get('model_name')}")
            return True
        except Exception as exc:
            print(f"[æ„å›¾è¯†åˆ«] é…ç½®å¤±è´¥: {exc}")
            self.intent_agent = None
            return False

    def _get_primary_agent(self) -> Optional[SmartAnalysisAgent]:
        return next(iter(self.agents.values()), None)

    async def analyze_conversation(
        self,
        messages: List[Dict],
        speaker_name: str,
        agent_name: Optional[str] = None,
        bypass_enabled: bool = False
    ) -> Dict:
        if not self.enabled and not bypass_enabled:
            return {'is': False, 'reason': 'æ™ºèƒ½åˆ†æå·²å…³é—­'}

        agent = None
        if agent_name and agent_name in self.agents:
            agent = self.agents[agent_name]
        elif self.agents:
            agent = self._get_primary_agent()

        if not agent:
            return {'is': False, 'reason': 'æœªé…ç½®æ™ºèƒ½ Agent'}

        prepared_messages = agent.prepare_analysis_messages(messages)
        return await agent.analyze(prepared_messages, speaker_name)

    async def run_pipeline(
        self,
        messages: List[Dict],
        speaker_name: str,
        *,
        use_analysis: bool = True,
        use_intent: bool = False,
        use_resume: bool = False,
        use_think_tank: bool = True,
        bypass_enabled: bool = False,
        force_modules: bool = False
    ) -> Dict:
        print(
            "[æ™ºèƒ½åˆ†æ] run_pipeline -> "
            f"analysis={use_analysis}, intent={use_intent}, resume={use_resume}, "
            f"think_tank={use_think_tank}, bypass={bypass_enabled}, force={force_modules}"
        )
        if use_analysis:
            phase1_result = await self.analyze_conversation(
                messages,
                speaker_name,
                bypass_enabled=bypass_enabled
            )
        else:
            phase1_result = {
                'is': False,
                'reason': 'æ™ºèƒ½åˆ†ææ¨¡å—æœªå¯ç”¨',
                'confidence': 0.0,
                'raw_response': '',
                'skipped': True
            }

        phase1_success = bool(phase1_result.get('is')) if isinstance(phase1_result, dict) else False
        should_halt = use_analysis and not force_modules and not phase1_success

        intent_result = None
        if use_intent and not should_halt:
            print("[æ„å›¾è¯†åˆ«] æ¨¡å—å¯ç”¨ï¼Œå³å°†è¿è¡Œ IntentRecognitionAgent")
            intent_result = await self.run_intent_recognition(messages, speaker_name)

        distribution_result = None
        if use_think_tank:
            if should_halt:
                distribution_result = {
                    'mode': 'default',
                    'targets': [],
                    'intent': intent_result,
                    'system_prompt': self.think_tank_agent.get_system_prompt(use_intent, use_resume)
                }
            else:
                distribution_result = self.think_tank_agent.prepare_distribution(
                    messages,
                    phase1_result,
                    intent_result,
                    force=force_modules or not use_analysis,
                    use_intent=use_intent,
                    use_resume=use_resume
                )
        else:
            distribution_result = {
                'mode': 'skipped',
                'targets': [],
                'intent': intent_result,
                'system_prompt': self.think_tank_agent.get_system_prompt(use_intent, use_resume)
            }

        return {
            'phase1': phase1_result,
            'phase2': intent_result,
            'distribution': distribution_result
        }

    async def run_intelligent_analysis(
        self,
        messages: List[Dict],
        speaker_name: str,
        intent_recognition: bool = False,
        resume_personalization: bool = False
    ) -> Dict:
        print(f"[æ™ºèƒ½åˆ†æ] å¼€å§‹ä¸‰é˜¶æ®µåˆ†æï¼Œæ„å›¾è¯†åˆ«: {intent_recognition}, ç®€å†ä¸ªæ€§åŒ–: {resume_personalization}")
        result = await self.run_pipeline(
            messages,
            speaker_name,
            use_analysis=True,
            use_intent=intent_recognition,
            use_resume=resume_personalization,
            use_think_tank=True,
            bypass_enabled=True,
            force_modules=False
        )
        phase2_result = result.get('phase2')
        if phase2_result is not None:
            print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ2å®Œæˆ: {phase2_result.get('success', False)}")
        distribution_result = result.get('distribution', {})
        print(f"[æ™ºèƒ½åˆ†æ] é˜¶æ®µ3å®Œæˆ: mode={distribution_result.get('mode')}")
        print(f"[æ™ºèƒ½åˆ†æ] ä½¿ç”¨çš„ç³»ç»Ÿæç¤ºè¯æ¨¡å¼: {distribution_result.get('system_prompt', 'æœªè®¾ç½®')[:50]}...")
        return result

    async def run_intent_recognition(self, messages: List[Dict], speaker_name: str) -> Dict:
        agent = self.intent_agent
        if not agent:
            primary = self._get_primary_agent()
            if primary:
                print("[æ„å›¾è¯†åˆ«] æœªå•ç‹¬é…ç½®ï¼Œå¤ç”¨ä¸»Agentæ¨¡å‹å‚æ•°")
                agent_config = dict(primary.config)
                agent = IntentRecognitionAgent(agent_config)
            else:
                return {'success': False, 'error': 'æ— å¯ç”¨çš„æ„å›¾è¯†åˆ«æ¨¡å‹'}

        return await agent.analyze(messages, speaker_name)

    async def should_analyze(self, message: Dict, conversation_history: List[Dict]) -> Tuple[bool, Optional[str]]:
        if not self.enabled:
            return False, "æ™ºèƒ½åˆ†æå·²å…³é—­"
        if not self.auto_trigger:
            return False, "è‡ªåŠ¨è§¦å‘å·²å…³é—­"
        if not self.agents:
            return False, "æœªé…ç½®Agent"
        agent = self._get_primary_agent()
        return agent.process_message(message, conversation_history)


# å…¨å±€Agentç®¡ç†å™¨å®ä¾‹
agent_manager = AgentManager()
