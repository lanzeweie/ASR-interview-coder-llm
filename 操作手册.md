# 系统操作与配置手册

本手册基于系统源码 (`server.py`, `intelligent_agent.py`, `trigger_manager.py`) 编写，详细说明了系统的配置逻辑。

配置分为两部分：
1.  **后端配置文件**：`api_config.json` 和 `data/agent.json`。
2.  **配置方式**：✅ **前端可视化生成** 与 ✍️ **必须手动填写**。

---

## 1. 配置文件概览

系统主要依赖以下两个文件进行持久化存储：

| 文件路径 | 用途 | 关键内容 |
| :--- | :--- | :--- |
| `api_config.json` | 核心连接配置 | LLM 服务商信息、API Key、智能分析阈值、主人公设定。 |
| `data/agent.json` | 智能体角色与逻辑 | **智囊团角色** (Frontend Configurable) 和 **内部子智能体逻辑** (Manual Input Only)。 |

---

## 2. 详细配置参数表

以下表格详细列出了所有关键参数及其配置方式。


### 核心 LLM 连接与全局设置 (`api_config.json`)

| 参数字段 | 说明 | 来源 | 前端可配置? | 备注 |
| :--- | :--- | :--- | :--- | :--- |
| `name` | 配置别名 | 用户定义 | ✅ 是 | 如 "DeepSeek", "GPT-4" |
| `api_key` | 鉴权密钥 | 服务商 | ✍️ **需手动输入粘贴** | 必填，敏感信息 |
| `base_url` | 接口地址 | 服务商 | ✍️ **需手动输入粘贴** | 必填，如 `https://api.openai.com/v1` |
| `model` | 模型ID | 服务商 | ✍️ **需手动输入粘贴** | 必填，如 `gpt-4o`, `deepseek-chat` |
| `tags` | 身份标签 | 系统/用户 | ✅ 是 | 用于绑定智囊团角色 |
| `system_prompt`| 系统提示词 | 用户定义 | ✅ 是 | 可选，为该模型覆盖默认提示词 |
| `model_local` | **本地模型列表** | **用户定义** | ✍️ **必须手动修改文件** | **定义前端“本地模型”下拉框的选项** |

### 智能分析与意图识别 (`agent_config` in `api_config.json`)

| 参数字段 | 说明 | 前端可配置? | 默认值 | 备注 |
| :--- | :--- | :--- | :--- | :--- |
| `enabled` | 总开关 | ✅ 是 | `true` | |
| `model_type` | 分析模型类型 | ✅ 是 | - | `local` 或 `api` |
| `model_name` | 分析模型名称 | ✅ 是 | `Qwen3-0.6B` | **从 `model_local` 列表选择** |
| `min_characters`| 触发字数阈值 | ✅ 是 | 10 | 少于此字数不分析 |
| `silence_threshold`| 静音等待(秒) | ✅ 是 | 2.0 | 说话后停顿多久触发 |
| `max_messages` | 上下文长度 | ✅ 是 | 50 | 分析时回溯的消息数 |
| `intent_recognition_enabled` | 意图识别开关 | ✅ 是 | `false` | |
| `intent_model_name` | 意图模型名称 | ✅ 是 | - | 建议使用能力较强的模型 |

### 角色与逻辑 (`data/agent.json`)

此文件包含两部分，**其中一部分必须手动修改文件**。

| 模块 | 字段 | 说明 | 前端可配置? | 备注 |
| :--- | :--- | :--- | :--- | :--- |
| **Think Tank** | `think_tank_roles` | 智囊团角色定义 | ✅ 是 | 可在设置中添加/修改角色Prompt |
| **Sub Agents** | `sub_agents` | **内部核心逻辑提示词** | ❌ **否 (必须手动修改)** | 定义了“意图识别模式”、“简历模式”下的基底 System Prompt |

---

## 3. 手动配置示范 (Manual Configuration)

虽然大部分配置可在前端完成，但以下关键信息必须由你手动输入或直接修改文件。

### 3.1 必须手动填写的 API 信息 (在前端设置页)

你必须从模型供应商处获取以下信息并填入：

```json
// 示例：DeepSeek 配置
{
  "api_key": "sk-8f92xxxxxxxxxxxxxxxxxxxxxxxxxxxx",  // ✍️ 必须手动复制粘贴
  "base_url": "https://api.deepseek.com/v1",       // ✍️ 必须准确填写，注意 /v1 后缀
  "model": "deepseek-chat"                         // ✍️ 必须准确填写模型ID
}
```

### 3.2 必须手动修改的文件配置 (`data/agent.json`)

**注意：** 系统的“模式逻辑”（例如：当启用简历分析时，系统改用什么语气？当启用意图识别时，系统如何行动？）存储在 `sub_agents` 字段中。**这部分目前前端无法修改，如果需要定制，请直接编辑 `data/agent.json` 文件。**

**`data/agent.json` 完整示范：**

```json
{
  "sub_agents": {
    "direct_chat": {
      "name": "直接对话模式",
      "system": "听从用户的指示"  // ✍️ 手动修改此处定义普通模式性格
    },
    "with_intent": {
      "name": "意图识别模式",
      "system": "以意图识别内容为准，围绕核心技术问题进行回答。" // ✍️ 手动修改此处定义意图模式逻辑
    },
    "with_resume": {
      "name": "简历个性化模式",
      "system": "你的回答需结合候选人简历中的项目背景..." // ✍️ 手动修改此处定义简历模式逻辑
    },
    "full_featured": {
      "name": "全功能模式",
      "system": "结合简历背景，并优先解决意图识别出的技术问题。" // ✍️ 手动修改此处定义混合模式逻辑
    }
  },
  "think_tank_roles": [
    // ✅ 这部分可以在前端“身份管理”中配置生成
    {
      "id": "tech_assistant",
      "name": "专业精炼者",
      "prompt": "你是一名资深 Python 技术专家...",
      "enabled": true
    }
  ]
}
```

### 3.3 后端核心配置示范 (`api_config.json`)

通常由前端自动生成，但如果需要手动修复，请参考：

```json
{
  "configs": [
    {
      "name": "Local Qwen",
      "api_key": "EMPTY",
      "base_url": "http://localhost:8000/v1",
      "model": "Qwen3-0.6B",
      "tags": [],
      "system_prompt": ""
    }
  ],
  "current_config": "Local Qwen",
  "protagonist": "面试者",  // ✅ 前端可配
  "config_version": "1.0",
  
  // ✍️ [重要] 本地模型列表：定义了前端“本地模型”下拉框中会出现哪些选项
  // 来源：https://www.modelscope.cn/ (目前仅验证过 Qwen2.5 和 Qwen3 系列)
  "model_local": [
    "Qwen3-0.6B",
    "Qwen2.5-0.5B-Instruct",
    "DeepSeek-R1-Distill-Qwen-1.5B"
  ],

  "agent_config": {
    "enabled": true,                // ✅ 前端可配
    "model_type": "local",          // ✅ 前端可配
    "model_name": "Qwen3-0.6B",     // ✅ 前端可配 (从上面的 model_local 中选)
    "min_characters": 10,           // ✅ 前端可配
    "silence_threshold": 2.0,       // ✅ 前端可配
    "max_messages": 50,             // ✅ 前端可配
    "intent_recognition_enabled": true, // ✅ 前端可配
    "intent_model_type": "api",     // ✅ 前端可配
    "intent_model_name": "gpt-4o"
  }
}
```


---

## 4. 全量配置参考 (Total Configuration Example)

以下是一个**包含所有可能字段**的完整 `api_config.json` 示例。
如果你需要重置配置文件，或者想一次性手动配置所有功能（含简历分析、智囊团、本地模型列表等），可以直接复制以下内容覆盖你的 `api_config.json`。

```json
{
  "config_version": "1.0",
  
  // ==========================================
  // 1. LLM 连接配置池 (Frontend Configurable)
  // ==========================================
  "configs": [
    {
      "name": "Local Qwen Base",
      "api_key": "EMPTY",
      "base_url": "http://localhost:8000/v1",
      "model": "Qwen3-0.6B",
      "tags": [],
      "system_prompt": "你是一个有帮助的助手。"
    },
    {
      "name": "DeepSeek V3 (Cloud)",
      "api_key": "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
      "base_url": "https://api.deepseek.com/v1",
      "model": "deepseek-chat",
      "tags": ["tech_assistant", "code_expert"], // 绑定了技术专家身份
      "system_prompt": "你是一个资深代码审计专家。"
    },
    {
      "name": "OpenAI GPT-4o",
      "api_key": "sk-proj-xxxxxxxxxxxxxxxxxxxxxxxx",
      "base_url": "https://api.openai.com/v1",
      "model": "gpt-4o",
      "tags": ["guide"],
      "system_prompt": ""
    }
  ],

  // ==========================================
  // 2. 当前选中配置 (Frontend Configurable)
  // ==========================================
  "current_config": "DeepSeek V3 (Cloud)",
  "protagonist": "及格的候选人",
  
  // ==========================================
  // 3. 智囊团激活列表 (Frontend Configurable)
  // ==========================================
  // 在“智囊团模式”下，这些模型会同时响应
  "multi_llm_active_names": [
    "DeepSeek V3 (Cloud)",
    "OpenAI GPT-4o"
  ],

  // ==========================================
  // 4. 本地模型列表 (Manual Input Only)
  // ==========================================
  // ✍️ 定义了前端【智能分析 -> 本地模型】下拉框的可选项
  // 来源：https://www.modelscope.cn/ (目前仅验证过 Qwen2.5 和 Qwen3 系列)
  "model_local": [
    "Qwen3-0.6B",
    "Qwen2.5-0.5B-Instruct",
    "Qwen2.5-1.5B-Instruct",
    "DeepSeek-R1-Distill-Qwen-1.5B"
  ],

  // ==========================================
  // 5. 智能分析 Agent 配置 (Frontend Configurable)
  // ==========================================
  "agent_config": {
    "enabled": true,                  // 总开关
    "auto_trigger": true,             // 是否自动监听静音触发
    
    // --- 阶段1：智能分析 (判断是否需要AI) ---
    "model_type": "local",            // local 或 api
    "model_name": "Qwen3-0.6B",       // 必须在 model_local 中存在
    "min_characters": 15,             // 最小触发字数
    "silence_threshold": 2.0,         // 静音触发阈值(秒)
    "max_messages": 30,               // 分析参考的消息条数
    
    // --- 阶段2：意图识别 (提取技术问题) ---
    "intent_recognition_enabled": true,
    "intent_model_type": "api",       // 建议用 api 模型以保证准确率
    "intent_model_name": "DeepSeek V3 (Cloud)", // 引用 configs 中的 name
    "intent_enable_thinking": false   // 是否显示思考过程(仅部分模型支持)
  },

  // ==========================================
  // 6. 简历分析配置 (Frontend Configurable)
  // ==========================================
  "resume_config": {
    "model_type": "api",
    "model_name": "DeepSeek V3 (Cloud)" // 读取和分析简历所用的模型
  }
}
```

---

## 总结

1.  **绝大部分配置**（模型切换、阈值调整、角色Prompt）请直接使用 **前端设置面板**。
2.  **关键凭证**（API Key, Base URL, Model ID）需要你 **手动填入** 前端输入框。
3.  **核心逻辑 Prompt**（`sub_agents`）需要你 **手动修改 `data/agent.json` 文件**。
4.  **本地模型列表**（`model_local`）需要你 **手动修改 `api_config.json` 文件**。


为前端 LLM对话框的 上传简历的简历模态框，再添加一个列。显示岗位分析
你首先需要在@resume_manager.py 添加功能。根据用户提供的岗位JD分析出这个岗位。

流程是 用户在前端提供  岗位/可以直接是岗位+岗位JD  （输入）（就在LLM对话框的 上传简历的 简历模态框框架内）

然后传给后端，后端进行Agent。分析完后存储在本地（返回的是markdown）。唯一性，因为不考虑多用户。前端也需要显示，因为是markdown，前端支持markdown就可以了，这个方法是本来已经实现了的。

关于这个信息不需要做任何的按钮设置。直接拼装在简历分析的提示词中靠前部分。跟随简历个性化启动

输入：
岗位名（必填）
岗位JD（可选）
