import os
import time
import wave
import threading
import pyaudio
import webrtcvad
import numpy as np
import librosa
import soundfile as sf
import re
from funasr import AutoModel
from modelscope.pipelines import pipeline

# --- é…ç½® HuggingFace å›½å†…é•œåƒ (å¯é€‰) ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class RealTimeASR_SV:
    def __init__(self, on_message_callback=None):
        # --- å‚æ•°é…ç½® ---
        self.AUDIO_RATE = 16000
        self.AUDIO_CHANNELS = 1
        self.CHUNK = 1024
        self.VAD_MODE = 3  # 0-3ï¼Œ3æœ€æ•æ„Ÿ
        self.OUTPUT_DIR = "./output"
        self.VOICEPRINT_DIR = "./voiceprints"
        self.SV_THRESHOLD = 0.35  # å£°çº¹è¯†åˆ«é˜ˆå€¼
        self.on_message_callback = on_message_callback
        
        # åˆå§‹åŒ–ç›®å½•
        os.makedirs(self.OUTPUT_DIR, exist_ok=True)
        os.makedirs(self.VOICEPRINT_DIR, exist_ok=True)

        # --- åŠ è½½æ¨¡å‹ ---
        print("æ­£åœ¨åŠ è½½ SenseVoice æ¨¡å‹ (ASR)...")
        # å»ºè®®ä½¿ç”¨æœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œä¾‹å¦‚: r"G:\Code\ASR\SenseVoiceSmall"

        self.model_asr = AutoModel(
            model="SenseVoiceSmall", 
            trust_remote_code=True,
            device="cuda" 
        )

        print("æ­£åœ¨åŠ è½½ CAM++ æ¨¡å‹ (å£°çº¹è¯†åˆ«)...")
        # ä½¿ç”¨ä½ æ‰¾åˆ°çš„æ­£ç¡® SV æ¨¡å‹ ID
        self.sv_pipeline = pipeline(
            task='speaker-verification',
            model='speech_campplus_sv_zh-cn_16k-common',
            model_revision='v1.0.0'
        )

        # --- åŠ è½½å£°çº¹åº“ ---
        self.speakers = {} 
        self.load_voiceprints()

        # --- VAD åˆå§‹åŒ– ---
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(self.VAD_MODE)
        
        self.running = True

    def check_and_convert_audio(self, file_path):
        """
        æ£€æŸ¥éŸ³é¢‘é‡‡æ ·ç‡ï¼Œå¦‚æœä¸æ˜¯ 16000Hz åˆ™è‡ªåŠ¨è½¬æ¢å¹¶è¦†ç›–ä¿å­˜ã€‚
        è§£å†³ Windows ä¸‹ torchaudio sox_effects æŠ¥é”™é—®é¢˜ã€‚
        """
        try:
            # å¿«é€Ÿè¯»å–å…ƒæ•°æ®
            info = sf.info(file_path)
            if info.samplerate != self.AUDIO_RATE:
                print(f"ğŸ”„ æ£€æµ‹åˆ°é‡‡æ ·ç‡ä¸åŒ¹é… ({info.samplerate}Hz)ï¼Œæ­£åœ¨è½¬æ¢ä¸º {self.AUDIO_RATE}Hz: {os.path.basename(file_path)}")
                # åŠ è½½å¹¶é‡é‡‡æ ·
                y, sr = librosa.load(file_path, sr=self.AUDIO_RATE)
                # è¦†ç›–ä¿å­˜
                sf.write(file_path, y, self.AUDIO_RATE)
                print(f"âœ… è½¬æ¢å®Œæˆ: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘æ£€æŸ¥å¤±è´¥: {file_path}, é”™è¯¯: {e}")

    def extract_embedding(self, audio_path):
        """
        ä»éŸ³é¢‘æ–‡ä»¶ä¸­æå–å£°çº¹åµŒå…¥å‘é‡
        ä½¿ç”¨ ModelScope pipeline çš„åº•å±‚æ¥å£è·å–åµŒå…¥
        """
        try:
            # åŠ è½½éŸ³é¢‘
            waveform, sample_rate = librosa.load(audio_path, sr=self.AUDIO_RATE)
            # è½¬æ¢ä¸º numpy æ•°ç»„
            waveform_np = waveform.astype(np.float32)

            # ä½¿ç”¨ pipeline è·å–åµŒå…¥ï¼ˆå¦‚æœ pipeline æœ‰ embed æ–¹æ³•ï¼‰
            # å°è¯•å¤šç§æ–¹å¼è·å–åµŒå…¥
            if hasattr(self.sv_pipeline, 'embeddings'):
                # å¦‚æœæœ‰ embeddings å±æ€§
                embeddings = self.sv_pipeline.embeddings(waveform_np, sample_rate)
                return embeddings
            elif hasattr(self.sv_pipeline, 'model'):
                # å¦‚æœæœ‰ model å±æ€§ï¼Œå°è¯•ä½¿ç”¨æ¨¡å‹ç›´æ¥æ¨ç†
                import torch
                waveform_tensor = torch.from_numpy(waveform_np).unsqueeze(0)
                if hasattr(self.sv_pipeline.model, 'cpu'):
                    waveform_tensor = waveform_tensor.cpu()
                with torch.no_grad():
                    embedding = self.sv_pipeline.model(waveform_tensor)
                    if isinstance(embedding, tuple):
                        embedding = embedding[0]
                    return embedding.cpu().numpy().flatten()
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ pipeline çš„ generate æ–¹æ³•è·å–åˆ†æ•°ï¼Œç„¶åæå–åµŒå…¥
                # è¿™å¯èƒ½ä¸å‡†ç¡®ï¼Œä½†ä½œä¸ºåå¤‡æ–¹æ¡ˆ
                print(f"âš ï¸ æ— æ³•ç›´æ¥æå–åµŒå…¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {audio_path}")
                # åˆ›å»ºä¸€ä¸ªå¾ˆçŸ­çš„å‚è€ƒéŸ³é¢‘ç”¨äºæ¯”å¯¹
                dummy_ref = audio_path  # ä½¿ç”¨è‡ªå·±ä½œä¸ºå‚è€ƒ
                result = self.sv_pipeline([dummy_ref, audio_path])
                # å°è¯•ä»ç»“æœä¸­æå– embedding ä¿¡æ¯
                # æ³¨æ„ï¼šè¿™æ˜¯åå¤‡æ–¹æ¡ˆï¼Œå¯èƒ½ä¸å‡†ç¡®
                return None

        except Exception as e:
            print(f"âš ï¸ åµŒå…¥æå–å¤±è´¥: {e}")
            return None

    def cosine_similarity(self, a, b):
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯ numpy æ•°ç»„
            if not isinstance(a, np.ndarray):
                a = np.array(a)
            if not isinstance(b, np.ndarray):
                b = np.array(b)

            # å±•å¹³å‘é‡
            a = a.flatten()
            b = b.flatten()

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(a, b)
            norm_a = np.linalg.norm(a)
            norm_b = np.linalg.norm(b)

            if norm_a == 0 or norm_b == 0:
                return 0.0

            return dot_product / (norm_a * norm_b)
        except Exception as e:
            print(f"âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def load_voiceprints(self):
        """åŠ è½½ voiceprints æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å£°çº¹åµŒå…¥æ•°æ®"""
        self.speakers = {}  # å­˜å‚¨ {name: {'embedding': array, 'path': str}}
        print(f"æ­£åœ¨æ‰«æå£°çº¹åº“: {self.VOICEPRINT_DIR} ...")
        if not os.path.exists(self.VOICEPRINT_DIR):
            return

        wav_files = [f for f in os.listdir(self.VOICEPRINT_DIR) if f.lower().endswith('.wav')]
        if not wav_files:
            print("  [è­¦å‘Š] å£°çº¹åº“ä¸ºç©ºï¼Œæ‰€æœ‰äººéƒ½å°†è¢«è¯†åˆ«ä¸º 'æœªçŸ¥ç”¨æˆ·'")
            return

        for wav_filename in wav_files:
            name = os.path.splitext(wav_filename)[0]
            wav_path = os.path.join(self.VOICEPRINT_DIR, wav_filename)
            npy_path = os.path.join(self.VOICEPRINT_DIR, f"{name}.npy")

            # æ£€æŸ¥å¹¶è½¬æ¢éŸ³é¢‘æ ¼å¼
            self.check_and_convert_audio(wav_path)

            # å°è¯•åŠ è½½é¢„è®¡ç®—çš„åµŒå…¥
            if os.path.exists(npy_path):
                try:
                    embedding = np.load(npy_path)
                    self.speakers[name] = {
                        'embedding': embedding,
                        'path': wav_path
                    }
                    print(f"  - å·²åŠ è½½å£°çº¹: {name} (åµŒå…¥æ•°æ®)")
                    continue
                except Exception as e:
                    print(f"  âš ï¸ åŠ è½½åµŒå…¥å¤±è´¥ {name}: {e}")

            # å¦‚æœåµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è®¡ç®—å¹¶ä¿å­˜
            print(f"  ğŸ”„ è®¡ç®—å¹¶ä¿å­˜åµŒå…¥: {name}")
            try:
                embedding = self.extract_embedding(wav_path)
                if embedding is not None:
                    np.save(npy_path, embedding)
                    self.speakers[name] = {
                        'embedding': embedding,
                        'path': wav_path
                    }
                    print(f"  âœ… å·²ä¿å­˜å£°çº¹: {name}")
                else:
                    print(f"  âŒ åµŒå…¥æå–å¤±è´¥: {name}")
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥ {name}: {e}")

        if not self.speakers:
            print("  [è­¦å‘Š] å£°çº¹åº“ä¸ºç©ºï¼Œæ‰€æœ‰äººéƒ½å°†è¢«è¯†åˆ«ä¸º 'æœªçŸ¥ç”¨æˆ·'")

    def identify_speaker(self, audio_path):
        """å°†éŸ³é¢‘ä¸å£°çº¹åº“æ¯”å¯¹ - ä½¿ç”¨é¢„è®¡ç®—çš„åµŒå…¥æ•°æ®"""
        if not self.speakers:
            return "æœªçŸ¥ç”¨æˆ· (åº“ç©º)"

        best_score = -1.0
        best_speaker = "æœªçŸ¥ç”¨æˆ·"

        # æå–æŸ¥è¯¢éŸ³é¢‘çš„åµŒå…¥
        query_embedding = self.extract_embedding(audio_path)
        if query_embedding is None:
            print("âš ï¸ æ— æ³•æå–æŸ¥è¯¢éŸ³é¢‘çš„åµŒå…¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå›é€€åˆ°åŸå§‹çš„ pipeline æ¯”å¯¹
            return self._identify_speaker_fallback(audio_path)

        # ä¸å£°çº¹åº“ä¸­çš„æ‰€æœ‰åµŒå…¥è¿›è¡Œå¯¹æ¯”
        for name, speaker_data in self.speakers.items():
            enroll_embedding = speaker_data['embedding']
            try:
                score = self.cosine_similarity(enroll_embedding, query_embedding)
                # print(f"  >>> æ¯”å¯¹ {name}: {score:.4f}")

                if score > best_score:
                    best_score = score
                    best_speaker = name
            except Exception as e:
                print(f"å£°çº¹æ¯”å¯¹å‡ºé”™ ({name}): {e}")

        if best_score >= self.SV_THRESHOLD:
            return f"{best_speaker} (ç½®ä¿¡åº¦:{best_score:.2f})"
        else:
            return "æœªçŸ¥ç”¨æˆ·"

    def _identify_speaker_fallback(self, audio_path):
        """å¤‡ç”¨æ¯”å¯¹æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹çš„ pipeline æ¯”å¯¹"""
        if not self.speakers:
            return "æœªçŸ¥ç”¨æˆ· (åº“ç©º)"

        best_score = -1.0
        best_speaker = "æœªçŸ¥ç”¨æˆ·"

        # ä½¿ç”¨ä¿å­˜çš„è·¯å¾„è¿›è¡Œæ¯”å¯¹
        for name, speaker_data in self.speakers.items():
            try:
                enroll_path = speaker_data['path']
                result = self.sv_pipeline([enroll_path, audio_path])
                score = result.get('score', 0)

                if score > best_score:
                    best_score = score
                    best_speaker = name
            except Exception as e:
                print(f"å¤‡ç”¨æ¯”å¯¹å‡ºé”™ ({name}): {e}")

        if best_score >= self.SV_THRESHOLD:
            return f"{best_speaker} (ç½®ä¿¡åº¦:{best_score:.2f})"
        else:
            return "æœªçŸ¥ç”¨æˆ·"

    def transcribe(self, audio_path):
        """ä½¿ç”¨ SenseVoice è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—"""
        try:
            res = self.model_asr.generate(
                input=audio_path,
                cache={},
                language="auto",
                use_itn=False,
            )
            text = res[0]['text']
            clean_text = text.split(">")[-1].strip()
            return clean_text
        except Exception as e:
            print(f"ASR å‡ºé”™: {e}")
            return ""

    def process_audio(self, audio_file):
        """å¤„ç†éŸ³é¢‘ç‰‡æ®µ"""
        print("-" * 30)
        speaker_info = self.identify_speaker(audio_file)
        text = self.transcribe(audio_file)
        
        # Filter empty or short messages
        if not text:
            return

        # Check for Chinese characters
        is_chinese = bool(re.search(r'[\u4e00-\u9fff]', text))
        
        if is_chinese:
            if len(text) < 3:
                print(f"âš ï¸ å¿½ç•¥è¿‡çŸ­ä¸­æ–‡: {text}")
                return
        else:
            if len(text) < 2:
                print(f"âš ï¸ å¿½ç•¥è¿‡çŸ­æ–‡æœ¬: {text}")
                return
        
        current_time = time.strftime("%H:%M:%S", time.localtime())
        print(f"[{current_time}] ğŸ—£ï¸  {speaker_info}: {text}")
        print("-" * 30)

        if self.on_message_callback:
            self.on_message_callback({
                "time": current_time,
                "speaker": speaker_info,
                "text": text
            })

    def run(self):
        """ä¸»å¾ªç¯ï¼šå½•éŸ³ + VAD æ£€æµ‹"""
        p = pyaudio.PyAudio()
        stream = p.open(format=pyaudio.paInt16,
                        channels=self.AUDIO_CHANNELS,
                        rate=self.AUDIO_RATE,
                        input=True,
                        frames_per_buffer=self.CHUNK)

        print("\n=== ç³»ç»Ÿå·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬... (æŒ‰ Ctrl+C åœæ­¢) ===\n")
        
        audio_buffer = []
        is_speaking = False
        silence_counter = 0
        silence_threshold = int(1.0 * self.AUDIO_RATE / self.CHUNK) 

        try:
            while self.running:
                data = stream.read(self.CHUNK, exception_on_overflow=False)
                
                is_active = self.check_vad(data)

                if is_active:
                    if not is_speaking:
                        print("Detected speech...", end="\r")
                        is_speaking = True
                    silence_counter = 0
                    audio_buffer.append(data)
                else:
                    if is_speaking:
                        silence_counter += 1
                        audio_buffer.append(data)
                        
                        if silence_counter > silence_threshold:
                            temp_file = os.path.join(self.OUTPUT_DIR, "temp_speech.wav")
                            self.save_wav(audio_buffer, temp_file)
                            
                            t = threading.Thread(target=self.process_audio, args=(temp_file,))
                            t.start()

                            is_speaking = False
                            silence_counter = 0
                            audio_buffer = []
                            print("Waiting for speech...   ", end="\r")

        except KeyboardInterrupt:
            print("\nåœæ­¢å½•åˆ¶...")
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

    def check_vad(self, chunk_data):
        """VAD æ£€æµ‹"""
        step = 480 * 2 
        active_frames = 0
        total_frames = 0
        
        for i in range(0, len(chunk_data) - step, step):
            frame = chunk_data[i:i+step]
            if self.vad.is_speech(frame, self.AUDIO_RATE):
                active_frames += 1
            total_frames += 1
        
        if total_frames == 0: return False
        return (active_frames / total_frames) > 0.3

    def save_wav(self, frames, filename):
        wf = wave.open(filename, 'wb')
        wf.setnchannels(self.AUDIO_CHANNELS)
        wf.setsampwidth(2)
        wf.setframerate(self.AUDIO_RATE)
        wf.writeframes(b''.join(frames))
        wf.close()

if __name__ == "__main__":
    app = RealTimeASR_SV()
    app.run()