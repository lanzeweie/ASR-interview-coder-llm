import os
import time
import wave
import threading
import pyaudio
import webrtcvad
import numpy as np
import librosa
import soundfile as sf
import re
from funasr import AutoModel
from modelscope.pipelines import pipeline

# --- é…ç½® HuggingFace å›½å†…é•œåƒ (å¯é€‰) ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class RealTimeASR_SV:
    def __init__(self, on_message_callback=None):
        # --- å‚æ•°é…ç½® ---
        self.AUDIO_RATE = 16000
        self.AUDIO_CHANNELS = 1
        self.CHUNK = 1024
        self.VAD_MODE = 3  # 0-3ï¼Œ3æœ€æ•æ„Ÿ
        self.OUTPUT_DIR = "./output"
        self.VOICEPRINT_DIR = "./voiceprints"
        self.SV_THRESHOLD = 0.35  # å£°çº¹è¯†åˆ«é˜ˆå€¼
        self.on_message_callback = on_message_callback
        
        # åˆå§‹åŒ–ç›®å½•
        os.makedirs(self.OUTPUT_DIR, exist_ok=True)
        os.makedirs(self.VOICEPRINT_DIR, exist_ok=True)

        # --- åŠ è½½æ¨¡å‹ ---
        print("æ­£åœ¨åŠ è½½ SenseVoice æ¨¡å‹ (ASR)...")
        # å»ºè®®ä½¿ç”¨æœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œä¾‹å¦‚: r"G:\Code\ASR\SenseVoiceSmall"
        self.model_asr = AutoModel(
            model="SenseVoiceSmall", 
            trust_remote_code=True,
            device="cuda" 
        )

        print("æ­£åœ¨åŠ è½½ CAM++ æ¨¡å‹ (å£°çº¹è¯†åˆ«)...")
        # ä½¿ç”¨ä½ æ‰¾åˆ°çš„æ­£ç¡® SV æ¨¡å‹ ID
        self.sv_pipeline = pipeline(
            task='speaker-verification',
            model='speech_campplus_sv_zh-cn_16k-common',
            model_revision='v1.0.0'
        )

        # --- åŠ è½½å£°çº¹åº“ ---
        self.speakers = {} 
        self.load_voiceprints()

        # --- VAD åˆå§‹åŒ– ---
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(self.VAD_MODE)
        
        self.running = True

    def check_and_convert_audio(self, file_path):
        """
        æ£€æŸ¥éŸ³é¢‘é‡‡æ ·ç‡ï¼Œå¦‚æœä¸æ˜¯ 16000Hz åˆ™è‡ªåŠ¨è½¬æ¢å¹¶è¦†ç›–ä¿å­˜ã€‚
        è§£å†³ Windows ä¸‹ torchaudio sox_effects æŠ¥é”™é—®é¢˜ã€‚
        """
        try:
            # å¿«é€Ÿè¯»å–å…ƒæ•°æ®
            info = sf.info(file_path)
            if info.samplerate != self.AUDIO_RATE:
                print(f"ğŸ”„ æ£€æµ‹åˆ°é‡‡æ ·ç‡ä¸åŒ¹é… ({info.samplerate}Hz)ï¼Œæ­£åœ¨è½¬æ¢ä¸º {self.AUDIO_RATE}Hz: {os.path.basename(file_path)}")
                # åŠ è½½å¹¶é‡é‡‡æ ·
                y, sr = librosa.load(file_path, sr=self.AUDIO_RATE)
                # è¦†ç›–ä¿å­˜
                sf.write(file_path, y, self.AUDIO_RATE)
                print(f"âœ… è½¬æ¢å®Œæˆ: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘æ£€æŸ¥å¤±è´¥: {file_path}, é”™è¯¯: {e}")

    def load_voiceprints(self):
        """åŠ è½½ voiceprints æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ wav æ–‡ä»¶"""
        self.speakers = {}
        print(f"æ­£åœ¨æ‰«æå£°çº¹åº“: {self.VOICEPRINT_DIR} ...")
        if not os.path.exists(self.VOICEPRINT_DIR):
            return

        for filename in os.listdir(self.VOICEPRINT_DIR):
            if filename.lower().endswith('.wav'):
                path = os.path.join(self.VOICEPRINT_DIR, filename)
                
                # --- æ–°å¢æ­¥éª¤ï¼šåŠ è½½å‰å…ˆè‡ªåŠ¨ä¿®å¤éŸ³é¢‘æ ¼å¼ ---
                self.check_and_convert_audio(path)
                
                name = os.path.splitext(filename)[0]
                self.speakers[name] = path
                print(f"  - å·²åŠ è½½è¯´è¯äºº: {name}")
        
        if not self.speakers:
            print("  [è­¦å‘Š] å£°çº¹åº“ä¸ºç©ºï¼Œæ‰€æœ‰äººéƒ½å°†è¢«è¯†åˆ«ä¸º 'æœªçŸ¥ç”¨æˆ·'")

    def identify_speaker(self, audio_path):
        """å°†éŸ³é¢‘ä¸å£°çº¹åº“æ¯”å¯¹"""
        if not self.speakers:
            return "æœªçŸ¥ç”¨æˆ· (åº“ç©º)"

        best_score = -1.0
        best_speaker = "æœªçŸ¥ç”¨æˆ·"

        # ç¡®ä¿å½•åˆ¶çš„ä¸´æ—¶æ–‡ä»¶ä¹Ÿæ˜¯ 16k (è™½ç„¶éº¦å…‹é£å½•åˆ¶é€šå¸¸è®¾ç½®äº†ï¼Œä½†åŒä¿é™©)
        # self.check_and_convert_audio(audio_path) 

        for name, enroll_path in self.speakers.items():
            try:
                result = self.sv_pipeline([enroll_path, audio_path])
                score = result.get('score', 0)
                
                # print(f"  >>> æ¯”å¯¹ {name}: {score:.4f}") 

                if score > best_score:
                    best_score = score
                    best_speaker = name
            except Exception as e:
                print(f"å£°çº¹æ¯”å¯¹å‡ºé”™ ({name}): {e}")

        if best_score >= self.SV_THRESHOLD:
            return f"{best_speaker} (ç½®ä¿¡åº¦:{best_score:.2f})"
        else:
            return "æœªçŸ¥ç”¨æˆ·"

    def transcribe(self, audio_path):
        """ä½¿ç”¨ SenseVoice è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—"""
        try:
            res = self.model_asr.generate(
                input=audio_path,
                cache={},
                language="auto",
                use_itn=False,
            )
            text = res[0]['text']
            clean_text = text.split(">")[-1].strip()
            return clean_text
        except Exception as e:
            print(f"ASR å‡ºé”™: {e}")
            return ""

    def process_audio(self, audio_file):
        """å¤„ç†éŸ³é¢‘ç‰‡æ®µ"""
        print("-" * 30)
        speaker_info = self.identify_speaker(audio_file)
        text = self.transcribe(audio_file)
        
        # Filter empty or short messages
        if not text:
            return

        # Check for Chinese characters
        is_chinese = bool(re.search(r'[\u4e00-\u9fff]', text))
        
        if is_chinese:
            if len(text) < 4:
                print(f"âš ï¸ å¿½ç•¥è¿‡çŸ­ä¸­æ–‡: {text}")
                return
        else:
            if len(text) < 2:
                print(f"âš ï¸ å¿½ç•¥è¿‡çŸ­æ–‡æœ¬: {text}")
                return
        
        current_time = time.strftime("%H:%M:%S", time.localtime())
        print(f"[{current_time}] ğŸ—£ï¸  {speaker_info}: {text}")
        print("-" * 30)

        if self.on_message_callback:
            self.on_message_callback({
                "time": current_time,
                "speaker": speaker_info,
                "text": text
            })

    def run(self):
        """ä¸»å¾ªç¯ï¼šå½•éŸ³ + VAD æ£€æµ‹"""
        p = pyaudio.PyAudio()
        stream = p.open(format=pyaudio.paInt16,
                        channels=self.AUDIO_CHANNELS,
                        rate=self.AUDIO_RATE,
                        input=True,
                        frames_per_buffer=self.CHUNK)

        print("\n=== ç³»ç»Ÿå·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬... (æŒ‰ Ctrl+C åœæ­¢) ===\n")
        
        audio_buffer = []
        is_speaking = False
        silence_counter = 0
        silence_threshold = int(1.0 * self.AUDIO_RATE / self.CHUNK) 

        try:
            while self.running:
                data = stream.read(self.CHUNK, exception_on_overflow=False)
                
                is_active = self.check_vad(data)

                if is_active:
                    if not is_speaking:
                        print("Detected speech...", end="\r")
                        is_speaking = True
                    silence_counter = 0
                    audio_buffer.append(data)
                else:
                    if is_speaking:
                        silence_counter += 1
                        audio_buffer.append(data)
                        
                        if silence_counter > silence_threshold:
                            temp_file = os.path.join(self.OUTPUT_DIR, "temp_speech.wav")
                            self.save_wav(audio_buffer, temp_file)
                            
                            t = threading.Thread(target=self.process_audio, args=(temp_file,))
                            t.start()

                            is_speaking = False
                            silence_counter = 0
                            audio_buffer = []
                            print("Waiting for speech...   ", end="\r")

        except KeyboardInterrupt:
            print("\nåœæ­¢å½•åˆ¶...")
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

    def check_vad(self, chunk_data):
        """VAD æ£€æµ‹"""
        step = 480 * 2 
        active_frames = 0
        total_frames = 0
        
        for i in range(0, len(chunk_data) - step, step):
            frame = chunk_data[i:i+step]
            if self.vad.is_speech(frame, self.AUDIO_RATE):
                active_frames += 1
            total_frames += 1
        
        if total_frames == 0: return False
        return (active_frames / total_frames) > 0.3

    def save_wav(self, frames, filename):
        wf = wave.open(filename, 'wb')
        wf.setnchannels(self.AUDIO_CHANNELS)
        wf.setsampwidth(2)
        wf.setframerate(self.AUDIO_RATE)
        wf.writeframes(b''.join(frames))
        wf.close()

if __name__ == "__main__":
    app = RealTimeASR_SV()
    app.run()