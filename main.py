import os
import time
import wave
import threading
import pyaudio
import webrtcvad
import numpy as np
import librosa
import soundfile as sf
import re
from funasr import AutoModel
from modelscope.pipelines import pipeline

# --- é…ç½® HuggingFace å›½å†…é•œåƒ (å¯é€‰) ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class RealTimeASR_SV:
    def __init__(self, on_message_callback=None):
        # --- å‚æ•°é…ç½® ---
        self.AUDIO_RATE = 16000
        self.AUDIO_CHANNELS = 1
        self.CHUNK = 1024
        self.VAD_MODE = 3  # 0-3ï¼Œ3æœ€æ•æ„Ÿ
        self.OUTPUT_DIR = "./output"
        self.VOICEPRINT_DIR = "./voiceprints"
        self.SV_THRESHOLD = 0.35  # å£°çº¹è¯†åˆ«é˜ˆå€¼
        self.on_message_callback = on_message_callback
        
        # åˆå§‹åŒ–ç›®å½•
        os.makedirs(self.OUTPUT_DIR, exist_ok=True)
        os.makedirs(self.VOICEPRINT_DIR, exist_ok=True)

        # --- åŠ è½½æ¨¡å‹ ---
        print("æ­£åœ¨åŠ è½½ SenseVoice æ¨¡å‹ (ASR)...")
        # å»ºè®®ä½¿ç”¨æœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œä¾‹å¦‚: r"G:\Code\ASR\SenseVoiceSmall"
        self.model_asr = AutoModel(
            model="SenseVoiceSmall", 
            trust_remote_code=True,
            device="cuda" 
        )

        print("æ­£åœ¨åŠ è½½ CAM++ æ¨¡å‹ (å£°çº¹è¯†åˆ«)...")
        # ä½¿ç”¨ä½ æ‰¾åˆ°çš„æ­£ç¡® SV æ¨¡å‹ ID
        self.sv_pipeline = pipeline(
            task='speaker-verification',
            model='speech_campplus_sv_zh-cn_16k-common',
            model_revision='v1.0.0'
        )

        # --- åŠ è½½æœ¬åœ°æ™ºèƒ½åˆ†ææ¨¡å‹ ---
        print("æ­£åœ¨åŠ è½½æœ¬åœ°æ™ºèƒ½åˆ†ææ¨¡å‹ (Qwen2.5-1.5B-Instruct)...")
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            # åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆå¼€å‘é˜¶æ®µå¯èƒ½éœ€è¦ --no-asr å‚æ•°è·³è¿‡ ASRï¼‰
            model_name = "Qwen/Qwen2.5-1.5B-Instruct"
            self.local_model_name = model_name
            self.local_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.local_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.local_model.eval()
            print("âœ… æœ¬åœ°æ™ºèƒ½åˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")
        except ImportError:
            print("âš ï¸ æœªå®‰è£… transformersï¼Œæœ¬åœ°æ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")
            self.local_tokenizer = None
            self.local_model = None
            self.local_model_name = None
        except Exception as e:
            print(f"âš ï¸ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.local_tokenizer = None
            self.local_model = None
            self.local_model_name = None

        # --- åŠ è½½å£°çº¹åº“ ---
        self.speakers = {} 
        self.load_voiceprints()

        # --- VAD åˆå§‹åŒ– ---
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(self.VAD_MODE)
        
        self.running = True

    def check_and_convert_audio(self, file_path):
        """
        æ£€æŸ¥éŸ³é¢‘é‡‡æ ·ç‡ï¼Œå¦‚æœä¸æ˜¯ 16000Hz åˆ™è‡ªåŠ¨è½¬æ¢å¹¶è¦†ç›–ä¿å­˜ã€‚
        è§£å†³ Windows ä¸‹ torchaudio sox_effects æŠ¥é”™é—®é¢˜ã€‚
        """
        try:
            # å¿«é€Ÿè¯»å–å…ƒæ•°æ®
            info = sf.info(file_path)
            if info.samplerate != self.AUDIO_RATE:
                print(f"ğŸ”„ æ£€æµ‹åˆ°é‡‡æ ·ç‡ä¸åŒ¹é… ({info.samplerate}Hz)ï¼Œæ­£åœ¨è½¬æ¢ä¸º {self.AUDIO_RATE}Hz: {os.path.basename(file_path)}")
                # åŠ è½½å¹¶é‡é‡‡æ ·
                y, sr = librosa.load(file_path, sr=self.AUDIO_RATE)
                # è¦†ç›–ä¿å­˜
                sf.write(file_path, y, self.AUDIO_RATE)
                print(f"âœ… è½¬æ¢å®Œæˆ: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘æ£€æŸ¥å¤±è´¥: {file_path}, é”™è¯¯: {e}")

    def load_voiceprints(self):
        """åŠ è½½ voiceprints æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ wav æ–‡ä»¶"""
        self.speakers = {}
        print(f"æ­£åœ¨æ‰«æå£°çº¹åº“: {self.VOICEPRINT_DIR} ...")
        if not os.path.exists(self.VOICEPRINT_DIR):
            return

        for filename in os.listdir(self.VOICEPRINT_DIR):
            if filename.lower().endswith('.wav'):
                path = os.path.join(self.VOICEPRINT_DIR, filename)
                
                # --- æ–°å¢æ­¥éª¤ï¼šåŠ è½½å‰å…ˆè‡ªåŠ¨ä¿®å¤éŸ³é¢‘æ ¼å¼ ---
                self.check_and_convert_audio(path)
                
                name = os.path.splitext(filename)[0]
                self.speakers[name] = path
                print(f"  - å·²åŠ è½½è¯´è¯äºº: {name}")
        
        if not self.speakers:
            print("  [è­¦å‘Š] å£°çº¹åº“ä¸ºç©ºï¼Œæ‰€æœ‰äººéƒ½å°†è¢«è¯†åˆ«ä¸º 'æœªçŸ¥ç”¨æˆ·'")

    def identify_speaker(self, audio_path):
        """å°†éŸ³é¢‘ä¸å£°çº¹åº“æ¯”å¯¹"""
        if not self.speakers:
            return "æœªçŸ¥ç”¨æˆ· (åº“ç©º)"

        best_score = -1.0
        best_speaker = "æœªçŸ¥ç”¨æˆ·"

        # ç¡®ä¿å½•åˆ¶çš„ä¸´æ—¶æ–‡ä»¶ä¹Ÿæ˜¯ 16k (è™½ç„¶éº¦å…‹é£å½•åˆ¶é€šå¸¸è®¾ç½®äº†ï¼Œä½†åŒä¿é™©)
        # self.check_and_convert_audio(audio_path) 

        for name, enroll_path in self.speakers.items():
            try:
                result = self.sv_pipeline([enroll_path, audio_path])
                score = result.get('score', 0)
                
                # print(f"  >>> æ¯”å¯¹ {name}: {score:.4f}") 

                if score > best_score:
                    best_score = score
                    best_speaker = name
            except Exception as e:
                print(f"å£°çº¹æ¯”å¯¹å‡ºé”™ ({name}): {e}")

        if best_score >= self.SV_THRESHOLD:
            return f"{best_speaker} (ç½®ä¿¡åº¦:{best_score:.2f})"
        else:
            return "æœªçŸ¥ç”¨æˆ·"

    def transcribe(self, audio_path):
        """ä½¿ç”¨ SenseVoice è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—"""
        try:
            res = self.model_asr.generate(
                input=audio_path,
                cache={},
                language="auto",
                use_itn=False,
            )
            text = res[0]['text']
            clean_text = text.split(">")[-1].strip()
            return clean_text
        except Exception as e:
            print(f"ASR å‡ºé”™: {e}")
            return ""

    def process_audio(self, audio_file):
        """å¤„ç†éŸ³é¢‘ç‰‡æ®µ"""
        print("-" * 30)
        speaker_info = self.identify_speaker(audio_file)
        text = self.transcribe(audio_file)
        
        # Filter empty or short messages
        if not text:
            return

        # Check for Chinese characters
        is_chinese = bool(re.search(r'[\u4e00-\u9fff]', text))
        
        if is_chinese:
            if len(text) < 4:
                print(f"âš ï¸ å¿½ç•¥è¿‡çŸ­ä¸­æ–‡: {text}")
                return
        else:
            if len(text) < 2:
                print(f"âš ï¸ å¿½ç•¥è¿‡çŸ­æ–‡æœ¬: {text}")
                return
        
        current_time = time.strftime("%H:%M:%S", time.localtime())
        print(f"[{current_time}] ğŸ—£ï¸  {speaker_info}: {text}")
        print("-" * 30)

        if self.on_message_callback:
            self.on_message_callback({
                "time": current_time,
                "speaker": speaker_info,
                "text": text
            })

    def run(self):
        """ä¸»å¾ªç¯ï¼šå½•éŸ³ + VAD æ£€æµ‹"""
        p = pyaudio.PyAudio()
        stream = p.open(format=pyaudio.paInt16,
                        channels=self.AUDIO_CHANNELS,
                        rate=self.AUDIO_RATE,
                        input=True,
                        frames_per_buffer=self.CHUNK)

        print("\n=== ç³»ç»Ÿå·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬... (æŒ‰ Ctrl+C åœæ­¢) ===\n")
        
        audio_buffer = []
        is_speaking = False
        silence_counter = 0
        silence_threshold = int(1.0 * self.AUDIO_RATE / self.CHUNK) 

        try:
            while self.running:
                data = stream.read(self.CHUNK, exception_on_overflow=False)
                
                is_active = self.check_vad(data)

                if is_active:
                    if not is_speaking:
                        print("Detected speech...", end="\r")
                        is_speaking = True
                    silence_counter = 0
                    audio_buffer.append(data)
                else:
                    if is_speaking:
                        silence_counter += 1
                        audio_buffer.append(data)
                        
                        if silence_counter > silence_threshold:
                            temp_file = os.path.join(self.OUTPUT_DIR, "temp_speech.wav")
                            self.save_wav(audio_buffer, temp_file)
                            
                            t = threading.Thread(target=self.process_audio, args=(temp_file,))
                            t.start()

                            is_speaking = False
                            silence_counter = 0
                            audio_buffer = []
                            print("Waiting for speech...   ", end="\r")

        except KeyboardInterrupt:
            print("\nåœæ­¢å½•åˆ¶...")
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

    def check_vad(self, chunk_data):
        """VAD æ£€æµ‹"""
        step = 480 * 2 
        active_frames = 0
        total_frames = 0
        
        for i in range(0, len(chunk_data) - step, step):
            frame = chunk_data[i:i+step]
            if self.vad.is_speech(frame, self.AUDIO_RATE):
                active_frames += 1
            total_frames += 1
        
        if total_frames == 0: return False
        return (active_frames / total_frames) > 0.3

    def save_wav(self, frames, filename):
        wf = wave.open(filename, 'wb')
        wf.setnchannels(self.AUDIO_CHANNELS)
        wf.setsampwidth(2)
        wf.setframerate(self.AUDIO_RATE)
        wf.writeframes(b''.join(frames))
        wf.close()

    async def analyze_with_local_model(self, messages, speaker_name):
        """
        ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æåˆ¤å®š

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            speaker_name: ä¸»äººå…¬å§“å

        Returns:
            dict: {'is': bool, 'reason': str, 'confidence': float}
        """
        if not self.local_model or not self.local_tokenizer:
            return {
                'is': False,
                'confidence': 0.0,
                'reason': 'æœ¬åœ°æ¨¡å‹æœªåŠ è½½',
                'raw_response': ''
            }

        try:
            # æ„å»ºåˆ†æ Prompt
            message_texts = []
            for msg in messages:
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')
                speaker = msg.get('speaker', '')
                message_texts.append(f"[{speaker}] {content}")

            dialogue = "\n".join(message_texts)

            prompt = f"""è¯·ä½ åˆ†æä»¥ä¸‹å¯¹è¯ï¼š

{dialogue}

æ³¨æ„ï¼š{speaker_name} æ˜¯ä¸»äººå…¬ã€‚

è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š
1. æ˜¯å¦åŒ…å«æŠ€æœ¯é—®é¢˜æˆ–ä¸“ä¸šè®¨è®ºï¼Ÿ
2. æ˜¯å¦éœ€è¦ä¸“ä¸šå»ºè®®æˆ–è§£å†³æ–¹æ¡ˆï¼Ÿ
3. æ˜¯å¦æ¶‰åŠå¤æ‚å†³ç­–æˆ–éœ€è¦å¤šæ–¹é¢æ€è€ƒï¼Ÿ
4. æ’é™¤é—®å€™è¯­ã€å®¶ä¹¡ã€å§“åç­‰æ—¥å¸¸å¯¹è¯

è¯·è¿”å›ä¸¥æ ¼çš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š
{{"is": true/false}}

- true: éœ€è¦å¯åŠ¨å¤šæ¨¡å‹å…±è¯ï¼Œä¸»äººå…¬å¯ä»¥ä»å¤šä¸ªè§’åº¦è·å¾—å»ºè®®
- false: æ™®é€šå¯¹è¯ï¼Œæ— éœ€ AI ä»‹å…¥"""

            # å‡†å¤‡è¾“å…¥
            inputs = self.local_tokenizer(prompt, return_tensors="pt").to(self.local_model.device)

            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.local_model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.1,
                    do_sample=False,
                    pad_token_id=self.local_tokenizer.eos_token_id
                )

            response = self.local_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()

            # è§£æå“åº”
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    if 'is' in result and isinstance(result['is'], bool):
                        reason = "æ£€æµ‹åˆ°æŠ€æœ¯è®¨è®ºï¼Œå»ºè®®å¯åŠ¨å¤šæ¨¡å‹å…±è¯" if result['is'] else "æ™®é€šå¯¹è¯ï¼Œæ— éœ€ AI ä»‹å…¥"
                        return {
                            'is': result['is'],
                            'confidence': 0.95,
                            'reason': reason,
                            'raw_response': response
                        }
                except json.JSONDecodeError:
                    pass

            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            return {
                'is': False,
                'confidence': 0.0,
                'reason': 'å“åº”æ ¼å¼æ— æ•ˆ',
                'raw_response': response
            }

        except Exception as e:
            print(f"[æœ¬åœ°æ¨¡å‹åˆ†æ] å‡ºé”™: {e}")
            return {
                'is': False,
                'confidence': 0.0,
                'reason': f'åˆ†æå¤±è´¥: {str(e)}',
                'raw_response': ''
            }

if __name__ == "__main__":
    app = RealTimeASR_SV()
    app.run()