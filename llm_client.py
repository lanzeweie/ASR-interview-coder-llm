import os
from openai import AsyncOpenAI
import json
import traceback
import asyncio
import httpx

class LLMClient:
    def __init__(self, api_key, base_url, model):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.client = None
        self.init_client()

    def init_client(self):
        if self.api_key and self.base_url:
            try:
                clean_url = self.base_url.strip().rstrip('/')
                if not clean_url.endswith("/v1"):
                    self.base_url = clean_url + "/v1"
                else:
                    self.base_url = clean_url
                # åˆ›å»ºè‡ªå®šä¹‰çš„ httpx å®¢æˆ·ç«¯ï¼Œé¿å…ä»£ç†é—®é¢˜
                timeout = httpx.Timeout(60.0, connect=10.0)
                http_client = httpx.AsyncClient(
                    timeout=timeout,
                    follow_redirects=True
                )

                # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ http_client
                self.client = AsyncOpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url,
                    http_client=http_client
                )
                print(f"[ç³»ç»Ÿ] LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
            except Exception as e:
                print(f"[é”™è¯¯] åˆå§‹åŒ– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")
                print(f"[é”™è¯¯ç±»å‹] {type(e).__name__}")
                # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
                traceback.print_exc()
                self.client = None
        else:
            print("[é”™è¯¯] LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–: ç¼ºå°‘ API Key æˆ– Base URL")
            self.client = None

    def update_config(self, api_key, base_url, model):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.init_client()

    async def chat_stream(self, messages):
            """
            æµå¼è¯·æ±‚ LLM å“åº” (Async - æ·±åº¦è°ƒè¯•ç‰ˆ)
            """
            if not self.client:
                yield "é”™è¯¯: LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚"
                return

            try:
                print(f"[è°ƒè¯•] æ­£åœ¨å‘é€è¯·æ±‚åˆ°æ¨¡å‹: {self.model}...")
                
                # å‘èµ·è¯·æ±‚
                stream = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    stream=True,
                    # æŸäº›ä¸­è½¬å•†å¦‚æœé‡åˆ°ä¸æ”¯æŒçš„å‚æ•°ä¼šæŠ¥é”™ï¼Œè¿™é‡Œä¿æŒæœ€ç®€å‚æ•°
                    temperature=0.7 
                )
                print("[è°ƒè¯•] è¯·æ±‚è¿æ¥å»ºç«‹æˆåŠŸï¼Œå‡†å¤‡æ¥æ”¶æµå¼æ•°æ®...")
                
                chunk_count = 0
                async for chunk in stream:
                    chunk_count += 1
                    
                    # --- ğŸ” æ·±åº¦è°ƒè¯•ï¼šæ‰“å°å‰3ä¸ªåŒ…çš„åŸå§‹æ•°æ®ï¼Œçœ‹çœ‹æœåŠ¡å™¨åˆ°åº•å›äº†ä»€ä¹ˆ ---
                    if chunk_count <= 3:
                        print(f"\n[åº•å±‚æ•°æ® Chunk {chunk_count}] {chunk.model_dump_json()}")
                    # -----------------------------------------------------------

                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        
                        # æ£€æŸ¥ delta é‡Œåˆ°åº•æœ‰ä»€ä¹ˆ
                        if chunk_count == 1 and not delta.content:
                            print(f"[è°ƒè¯•] ç¬¬ä¸€ä¸ªåŒ…å†…å®¹ä¸ºç©ºï¼ŒRole: {getattr(delta, 'role', 'Unknown')}")

                        if hasattr(delta, 'content') and delta.content is not None:
                            content = delta.content
                            if content: 
                                yield content
                            else:
                                # è¿™æ˜¯ä¸€ä¸ªç©ºå­—ç¬¦ä¸² ""ï¼Œæœ‰äº›æ¨¡å‹ä¼šå‘ç©ºå­—ç¬¦ä¸²ä¿æ´»
                                pass 
                
                if chunk_count == 0:
                    yield "\n[è­¦å‘Š] è¿æ¥å»ºç«‹æˆåŠŸï¼Œä½†æµæ˜¯ç©ºçš„ (Stream Empty)ã€‚\nå¯èƒ½åŸå› ï¼šAPI Keyé¢åº¦ä¸è¶³ã€æ¨¡å‹åç§°æ‹¼å†™é”™è¯¯ (å°è¯•æ”¹ä¸º gpt-3.5-turbo æˆ– deepseek-chat æµ‹è¯•)ã€‚"
                
                print(f"\n[è°ƒè¯•] æµæ¥æ”¶å®Œæ¯•ï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®åŒ…ã€‚")

            except Exception as e:
                print(f"\n[ä¸¥é‡é”™è¯¯] è¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸:")
                traceback.print_exc()
                yield f"è¯·æ±‚é”™è¯¯: {str(e)}"
    def __repr__(self):
        status = "å·²è¿æ¥" if self.client is not None else "æœªè¿æ¥"
        return f"LLMClient(æ¨¡å‹='{self.model}', åœ°å€='{self.base_url}', çŠ¶æ€={status})"

async def main():
    # --- æµ‹è¯•éƒ¨åˆ† ---
    CONFIG_FILE = "api_config.json"
    
    print("--- å¼€å§‹æµ‹è¯• LLM å®¢æˆ·ç«¯ (Async) ---")
    
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            config_data = json.load(f)
            
        current_config_name = config_data.get("current_config")
        current_config = next((c for c in config_data.get("configs", []) if c["name"] == current_config_name), None)
        
        if current_config:
            print(f"æ­£åœ¨åŠ è½½é…ç½®: {current_config_name}")
            
            raw_key = current_config.get("api_key", "")
            masked_key = raw_key[:6] + "******" + raw_key[-4:] if len(raw_key) > 10 else "******"
            print(f"API Key (è„±æ•): {masked_key}")

            client = LLMClient(
                api_key=raw_key,
                base_url=current_config.get("base_url"),
                model=current_config.get("model")
            )
            
            print(f"å®¢æˆ·ç«¯çŠ¶æ€: {client}")
            
            test_messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "ä½ å¥½ï¼å¦‚æœèƒ½æ”¶åˆ°æ¶ˆæ¯è¯·å›å¤'æµ‹è¯•æˆåŠŸ'ã€‚"}
            ]
            
            print("\n[æ“ä½œ] å‘é€æµ‹è¯•æ¶ˆæ¯ä¸­...")
            print("-" * 30)
            
            received_content = False
            async for chunk in client.chat_stream(test_messages):
                print(chunk, end="", flush=True)
                received_content = True
            
            print("\n" + "-" * 30)
            
            if not received_content:
                print("\n[ç»“æœ] æœªæ”¶åˆ°ä»»ä½•å›å¤å†…å®¹ã€‚")
            else:
                print("\n[ç»“æœ] æµ‹è¯•ç»“æŸã€‚")
            
        else:
            print(f"[é”™è¯¯] åœ¨ {CONFIG_FILE} ä¸­æœªæ‰¾åˆ°é…ç½® '{current_config_name}'")
    else:
        print(f"[é”™è¯¯] æ‰¾ä¸åˆ°æ–‡ä»¶ {CONFIG_FILE}ï¼Œè¯·ç¡®ä¿å®ƒåœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")

if __name__ == "__main__":
    asyncio.run(main())