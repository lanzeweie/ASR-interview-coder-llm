import os
from openai import AsyncOpenAI
import json
import traceback
import asyncio
import httpx
from logger_config import setup_logger

logger = setup_logger(__name__)

class LLMClient:
    def __init__(self, api_key, base_url, model):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.client = None
        self.init_client()

    def init_client(self):
        if self.api_key and self.base_url:
            try:
                clean_url = self.base_url.strip().rstrip('/')
                if not clean_url.endswith("/v1"):
                    self.base_url = clean_url + "/v1"
                else:
                    self.base_url = clean_url
                # åˆ›å»ºè‡ªå®šä¹‰çš„ httpx å®¢æˆ·ç«¯ï¼Œé¿å…ä»£ç†é—®é¢˜
                timeout = httpx.Timeout(60.0, connect=10.0)
                http_client = httpx.AsyncClient(
                    timeout=timeout,
                    follow_redirects=True
                )

                # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ http_client
                self.client = AsyncOpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url,
                    http_client=http_client
                )
                logger.info(f"[ç³»ç»Ÿ] LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
            except Exception as e:
                logger.error(f"[é”™è¯¯] åˆå§‹åŒ– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")
                logger.error(f"[é”™è¯¯ç±»å‹] {type(e).__name__}")
                # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
                logger.exception("åˆå§‹åŒ–å¼‚å¸¸è¯¦æƒ…:")
                self.client = None
        else:
            logger.error("[é”™è¯¯] LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–: ç¼ºå°‘ API Key æˆ– Base URL")
            self.client = None

    def update_config(self, api_key, base_url, model):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.init_client()

    async def chat_stream(self, messages, stream=True):
            """
            è¯·æ±‚ LLM å“åº” (Async - æ”¯æŒæµå¼å’Œéæµå¼)
            """
            if not self.client:
                yield "é”™è¯¯: LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚"
                return

            try:
                logger.debug(f"[è°ƒè¯•] æ­£åœ¨å‘é€è¯·æ±‚åˆ°æ¨¡å‹: {self.model} (Stream={stream})...")
                
                # å‘èµ·è¯·æ±‚
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    stream=stream,
                    # æŸäº›ä¸­è½¬å•†å¦‚æœé‡åˆ°ä¸æ”¯æŒçš„å‚æ•°ä¼šæŠ¥é”™ï¼Œè¿™é‡Œä¿æŒæœ€ç®€å‚æ•°
                    temperature=0.7 
                )
                logger.debug("[è°ƒè¯•] è¯·æ±‚è¿æ¥å»ºç«‹æˆåŠŸ...")
                
                if stream:
                    chunk_count = 0
                    async for chunk in response:
                        chunk_count += 1
                        
                        # --- ğŸ” æ·±åº¦è°ƒè¯•ï¼šæ‰“å°å‰3ä¸ªåŒ…çš„åŸå§‹æ•°æ®ï¼Œçœ‹çœ‹æœåŠ¡å™¨åˆ°åº•å›äº†ä»€ä¹ˆ ---
                        if chunk_count <= 3:
                            logger.debug(f"[åº•å±‚æ•°æ® Chunk {chunk_count}] {chunk.model_dump_json()}")
                        # -----------------------------------------------------------

                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            
                            # æ£€æŸ¥ delta é‡Œåˆ°åº•æœ‰ä»€ä¹ˆ
                            if chunk_count == 1 and not delta.content:
                                logger.debug(f"[è°ƒè¯•] ç¬¬ä¸€ä¸ªåŒ…å†…å®¹ä¸ºç©ºï¼ŒRole: {getattr(delta, 'role', 'Unknown')}")

                            if hasattr(delta, 'content') and delta.content is not None:
                                content = delta.content
                                if content: 
                                    yield content
                                else:
                                    # è¿™æ˜¯ä¸€ä¸ªç©ºå­—ç¬¦ä¸² ""ï¼Œæœ‰äº›æ¨¡å‹ä¼šå‘ç©ºå­—ç¬¦ä¸²ä¿æ´»
                                    pass 
                    
                    if chunk_count == 0:
                        yield "\n[è­¦å‘Š] è¿æ¥å»ºç«‹æˆåŠŸï¼Œä½†æµæ˜¯ç©ºçš„ (Stream Empty)ã€‚\nå¯èƒ½åŸå› ï¼šAPI Keyé¢åº¦ä¸è¶³ã€æ¨¡å‹åç§°æ‹¼å†™é”™è¯¯ (å°è¯•æ”¹ä¸º gpt-3.5-turbo æˆ– deepseek-chat æµ‹è¯•)ã€‚"
                    
                    logger.debug(f"[è°ƒè¯•] æµæ¥æ”¶å®Œæ¯•ï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®åŒ…ã€‚")
                else:
                    # éæµå¼å¤„ç†
                    if response.choices and len(response.choices) > 0:
                        content = response.choices[0].message.content
                        yield content
                    else:
                         yield "\n[è­¦å‘Š] æœªæ”¶åˆ°æœ‰æ•ˆå“åº”å†…å®¹ã€‚"

            except Exception as e:
                logger.error(f"[ä¸¥é‡é”™è¯¯] è¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸:")
                logger.exception("è¯·æ±‚å¼‚å¸¸è¯¦æƒ…:")
                yield f"è¯·æ±‚é”™è¯¯: {str(e)}"

    async def test_connection(self):
        """
        æµ‹è¯•è¿æ¥æ˜¯å¦æœ‰æ•ˆ
        """
        if not self.client:
            return False, "å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        
        try:
            # å°è¯•å‘é€ä¸€ä¸ªæç®€çš„è¯·æ±‚
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "ä½ å¥½"}],
                max_tokens=1
            )
            return True, "è¿æ¥æˆåŠŸ"
        except Exception as e:
            return False, f"è¿æ¥å¤±è´¥: {str(e)}"
    def __repr__(self):
        status = "å·²è¿æ¥" if self.client is not None else "æœªè¿æ¥"
        return f"LLMClient(æ¨¡å‹='{self.model}', åœ°å€='{self.base_url}', çŠ¶æ€={status})"

async def main():
    # --- æµ‹è¯•éƒ¨åˆ† ---
    CONFIG_FILE = "api_config.json"
    
    logger.info("--- å¼€å§‹æµ‹è¯• LLM å®¢æˆ·ç«¯ (Async) ---")
    
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            config_data = json.load(f)
            
        current_config_name = config_data.get("current_config")
        current_config = next((c for c in config_data.get("configs", []) if c["name"] == current_config_name), None)
        
        if current_config:
            logger.info(f"æ­£åœ¨åŠ è½½é…ç½®: {current_config_name}")
            
            raw_key = current_config.get("api_key", "")
            masked_key = raw_key[:6] + "******" + raw_key[-4:] if len(raw_key) > 10 else "******"
            logger.info(f"API Key (è„±æ•): {masked_key}")

            client = LLMClient(
                api_key=raw_key,
                base_url=current_config.get("base_url"),
                model=current_config.get("model")
            )
            
            logger.info(f"å®¢æˆ·ç«¯çŠ¶æ€: {client}")
            
            test_messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè¶…çº§ç²¾ç®€æµ‹è¯•ä½“ï¼Œä½ åªèƒ½å›å¤æå°‘é‡æ–‡å­—è¡¨ç¤ºä½ é€šè¿‡æµ‹è¯•äº†ã€‚"},
                {"role": "user", "content": "ä½ å¥½ï¼å¦‚æœèƒ½æ”¶åˆ°æ¶ˆæ¯è¯·å›å¤'æµ‹è¯•æˆåŠŸ'ã€‚"}
            ]
            
            logger.info("\n[æ“ä½œ] å‘é€æµ‹è¯•æ¶ˆæ¯ä¸­...")
            logger.info("-" * 30)
            
            received_content = False
            async for chunk in client.chat_stream(test_messages):
                print(chunk, end="", flush=True) # Keep print for accurate streaming visualization in CLI test
                received_content = True
            
            logger.info("\n" + "-" * 30)
            
            if not received_content:
                logger.warning("\n[ç»“æœ] æœªæ”¶åˆ°ä»»ä½•å›å¤å†…å®¹ã€‚")
            else:
                logger.info("\n[ç»“æœ] æµ‹è¯•ç»“æŸã€‚")
            
        else:
            logger.error(f"[é”™è¯¯] åœ¨ {CONFIG_FILE} ä¸­æœªæ‰¾åˆ°é…ç½® '{current_config_name}'")
    else:
        logger.error(f"[é”™è¯¯] æ‰¾ä¸åˆ°æ–‡ä»¶ {CONFIG_FILE}ï¼Œè¯·ç¡®ä¿å®ƒåœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")

if __name__ == "__main__":
    asyncio.run(main())